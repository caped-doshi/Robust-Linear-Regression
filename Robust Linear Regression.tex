
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath,amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{mathtools}

\newenvironment{proofsketch}{%
	\renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage{hyperref}

\usepackage{cleveref}

\usepackage{tikz,pgfplots}
\usetikzlibrary {datavisualization.formats.functions}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\declaretheorem[name=Theorem,numberwithin=section]{thm}
\declaretheorem[name=Lemma,numberwithin=thm]{lemma}
\declaretheorem[name=Definition,numberwithin=section]{definition}
\declaretheorem[name=Assumption,numberwithin=section]{assumption}

\usepackage{xcolor}
\usepackage[linesnumbered,ruled]{algorithm2e}
\DontPrintSemicolon
% Set algorithm keyword formatting
\newcommand{\Comment}{\tcc*[r]}
\SetKwComment{tcc}{$\triangleright$~}{}
\SetCommentSty{normalfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
% Set algorithm line numbers
\SetNlSty{}{}{:}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\usepackage{url}

\usepackage{appendix}
\usepackage{titletoc}

\title{Robust Linear Regression by Super-Quantile Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Arvind Rathnashyam \thanks{ Use footnote for providing further information
		about author (webpage, alternative address)---\emph{not} for acknowledging
		funding agencies.  Funding acknowledgements go at the end of the paper.} \\
	Department of Computer Science\\
	Rensselaer Polytechnic University\\
	Troy, NY 12180, USA \\
	\texttt{\{rathna\}@rpi.edu} \\
	\And
	Ji Q. Ren \& Yevgeny LeNet \\
	Department of Computational Neuroscience \\
	University of the Witwatersrand \\
	Joburg, South Africa \\
	\texttt{\{robot,net\}@wits.ac.za} \\
	\AND
	Coauthor \\
	Affiliation \\
	Address \\
	\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
	
	
	\maketitle
	
	\begin{abstract}
		Robust Linear Regression is the problem of fitting data to a distribution, $\displaystyle \mathbb{P}$ when there exists contaminated samples, $\displaystyle \mathbb{Q}$. We model this as $\displaystyle \hat{\mathbb{P}} = (1-\epsilon)\mathbb{P} + \epsilon\mathbb{Q}$. Traditional Least Squares Methods fit the empirical risk model to all training data in $\displaystyle \hat{\mathbb{P}}$. In this paper we show theoretical and experimental results of sub-quantile optimization, where we optimize with respect to the $p$-quantile of the empirical loss.
	\end{abstract}
	
	\section{Introduction}
	Linear Regression is one of the most widely used statistical estimators throughout Science. Although robustness is only a somewhat recent topic in machine learning, it has been a topic in statistics for many decades. Several popular methods have been very popular due to their simplicity and high effectiveness including quantile regression \cite{quantile-regression}, Theil-Sen Estimator \cite{thiel-sen}, and Huber Regression \cite{Huber2009}.
	
	Our goal is to provide a theoretic analysis and convergence conditions for sub-quantile optimization and offer practioners a method for robust linear regression.

	In this section we quantify the effect of corruption on the desired model. To introduce notation, let $\mP$ represent the data from distribution $\mathbb{P}$ and let $\mQ$ represent the training data for $\mathbb{Q}$. Let $\vy_P$ represent the target data for $\mathbb{P}$ and let $\vy_Q$ represent the target data for $\mathbb{Q}$.\\
	It is know the least squares optimal solution for $\mX$ is equal to $(\mX^T\mX)^{-1}\mX^T\vy$\\
	Note $\displaystyle \mX = \begin{pmatrix}\mP \\ \mQ \end{pmatrix}$ so $\mX^T = \begin{pmatrix}\mP^T & \mQ^T \end{pmatrix}$
	\begin{align}
		\mX^T\mX &= \begin{pmatrix}\mP^T & \mQ^T \end{pmatrix} \begin{pmatrix}\mP \\ \mQ \end{pmatrix} &&\\
		&= \mP^T\mP + \mQ^T \mQ &&\\
		(\mX^T\mX)^{-1}\mX^T &= (\mP^T\mP+ \mQ^T\mQ)^{-1}\begin{pmatrix}\mP^T &\mQ^T \end{pmatrix}&&\\
		&= \begin{pmatrix}(\mP^T\mP + \mQ^T\mQ)^{-1} \mP^T & (\mP^T\mP  + \mQ^T\mQ)^{-1} \mQ^T \end{pmatrix} &&\\
		\mX^{\dagger}\vy &= \begin{pmatrix}(\mP^T\mP + \mQ^T\mQ)^{-1} \mP^T & (\mP^T\mP  + \mQ^T\mQ)^{-1} \mQ^T \end{pmatrix} \begin{pmatrix}\vy_P \\ \vy_Q \end{pmatrix} &&\\
			&= (\mP^T\mP + \mQ^T\mQ)^{-1} \mP^T \vy_P + (\mP^T\mP + \mQ^T \mQ)^{-1} \mQ^T \vy_Q &&
	\end{align}

	Note the optimal solution for a linear regression model on $\mathbb{P}$ is $\displaystyle (\mP^T\mP)^{-1}\mP^T \vy_P$\\
	Often times in the case of corrupted data we have $\mP$ and $\mQ$ are sampled similarly however $\vy_P$ and $\vy_Q$ are very different. Thus $\displaystyle (\mP^T\mP + \mQ^T\mQ)^{-1}\mQ^T\vy_Q$ could have a large effect on the optimal solution. This is why we propose Sub-Quantile Optimization, we seek to reduce the impact of $\mQ^T\mQ$ and $\vy_Q$ by reducing the number of rows in $\mQ$. Thus we reduce the condition number of $\mQ^T \mQ$ and the overall effect on the optimal solution for $\mP$.
	
	In this paper we will show how Sub-Quantile Optimization can address the shortcomings of ERM in the case of corrupted data or imbalanced data, where there exists a majority class and a minority class.
	
		
	\section{Related Work}
	Least Trimmed Squares (LTS) \cite{LTS}.
	
	Tilted Empirical Risk Minimization (TERM) \cite{li2020tilted} is a framework built to similarly handle the shortcomings of ERM with respect to robustness. The TERM framework instead minimizes the following quantity, where $t$ is a hyperparameter
	\begin{equation}
		\tilde{R}(t;\theta) \coloneqq \frac{1}{t} \log\left(\frac{1}{N}\sum_{i \in \left[N\right]}e^{tf(x_i;\theta)} \right)
	\end{equation}
	
	SMART \cite{https://doi.org/10.48550/arxiv.2206.04777}
	
	SEVER \cite{DiakonikolasKKLSS19}
	
	Gradient Filtering Approaches (Need Source)
	
	Super-Quantile Optimization \cite{ROCKAFELLAR2014140}
			
	\section{Sub-quantile Optimization}
	\label{headings}
	
	The two-step optimization for Sub-Quantile optimization is given as follows 
	\begin{equation}
		\label{eqn:t-update}
		t_{k+1} = \argmax_t g(t,\vtheta_k) 
	\end{equation}
	\begin{equation}
		\label{eqn:theta-update}
		\vtheta_{k+1} = \vtheta_k + \alpha \nabla_{\vtheta_k} g(t,\vtheta_k)
	\end{equation}
		
	This algorithm is adopted from \cite{Razaviyayn}
		
	\begin{restatable}{thm}{Convergence}
		\label{thm:convergence}
		Sub-Quantile Optimization Converges Almost Surely
	\end{restatable}
	
	\begin{restatable}{lemma}{gfermat}\label{lem:gfermat}
		$g(t,\vtheta)$ is maximized when $t = Q_p(U)$
	\end{restatable}
		\begin{proof}
		Since $\displaystyle g(t,\vtheta)$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
		Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T\mX - \vy)^2\right)$ and note $\displaystyle 0 < p < 1$
		\begin{align}
			\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) &= \partial{\left(-t + \frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)} &&\\
			&= -1 + \frac{1}{np}\sum_{i=1}^{n}
			\left\{
			\begin{array}{lr}
				1, & \text{if } t > \hat{\vnu}_i\\
				0, & \text{if } t < \hat{\vnu}_i \\
				\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
			\end{array}
			\right\}&&\\
			&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
		\end{align}
		This is the $p$-quantile of $\displaystyle \vnu$. Not necesarily the $p$-quantile of $Q_p(U)$
	\end{proof}
	
	\begin{restatable}{lemma}{gthetaderiv}\label{lem:gthetaderiv}
		Let $t = \hat{\vnu}_{np}$. The second step in the optimization is the derivative with respect to the first $np$ elements in the sorted squared losses, $\hat{\vnu}$. The derivative of $g(t,\vtheta)$ w.r.t $\displaystyle \nabla_{\vtheta}g(t_{k+1},\vtheta_k) = \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)$
	\end{restatable}
	We provide a proof in Appendix \ref{app:gthetaderiv}. By our choice of $t_{k+1}$ what we find is the terms cancel out and we are left minimizing the terms within the $np$ lowest squared losses.

	\section{Theoretical Analysis}
	\subsection{Robustness}
	\begin{assumption}\label{asm:sym-0}
		The parameters of $\vtheta$ are sampled from a symmetrical continuous distribution around 0. Inspired by \cite{Lu_2020}. 
	\end{assumption}
	By Lemma \ref{lem:gthetaderiv}, $\vtheta$ is updated only on the $np$ points with the smallest squared loss. To quantify how ''Robust'' our linear regressor is, we want to know how many data points from $\mathbb{Q}$ are within the lowest $np$ squared losses as the number of iterations, $k \to \infty$. To do this, we will model the data sampled from $\mathbb{P}$ and $\mathbb{Q}$ as random variables. Let $\vp_1,\vp_2,\ldots,\vp_{(1-\eps)n}$ be the $(1-\epsilon)n$ points sampled i.i.d from $\mathbb{P}$. Let $P_1,P_2,\dots,P_m$ be random variables that represent the data sampled from $\mathbb{P}$,  $\vp_1,\vp_2,\dots,\vp_{1-(\eps)n}$ such that
	
	\begin{equation}
		P_i = \begin{cases}
					1 & \text{ if } (\vtheta_k^T\vp_i - y_{i})^2 \leq \hat{\vnu}_{np} \\
					0 & \text{ if } (\vtheta_k^T\vp_i - y_{i})^2 > \hat{\vnu}_{np} \\
				\end{cases}
	\end{equation}
	
	Let $\vq_1,\vq_2,\ldots,\vq_{\eps n}$ be the $\eps n$ points sampled i.i.d from $\mathbb{Q}$. Let $Q_1,Q_2,\dots,Q_m$ be random variables that represent the data sampled from $\mathbb{Q}$,  $\vq_1,\vq_2,\dots,\vq_{\eps n}$ such that
	
	\begin{equation}
		Q_i = \begin{cases}
			1 & \text{ if } (\vtheta_k^T\vq_i - y_{i})^2 \leq \hat{\vnu}_{np} \\
			0 & \text{ if } (\vtheta_k^T\vq_i - y_{i})^2 > \hat{\vnu}_{np} \\
		\end{cases}
	\end{equation}

	It is clear that $\displaystyle \mathbb{P}\left[Q_i = 1\right] = 1 - \mathbb{P}\left[P_i = 1\right]$ So it is only necesary to calculate $\mathbb{P}\left[P_i = 1\right]$
	
	Furthermore, we will define another random variable to determine the number of corrupted samples within the $np$ lowest squared losses after optimization iteration $k$.
	\begin{equation}
		Q^+_k = \sum_{i=1}^{n\epsilon} Q_i \text{ and } P^+_k = \sum_{i=1}^{n(1-\epsilon)} P_i
	\end{equation}
	Let $\vp_1,\vp_2,\dots,\vp_m$ represent the points sampled from $\mathbb{P}$ within the lowest $np$ squared losses and let $\vq_1,\vq_2,\dots,\vq_l$ represent the points sampled from $\mathbb{Q}$ within the lowest $np$ squared losses, where $m = \mathbb{E}\left[P_0^+\right]$ and $l = \mathbb{E}\left[Q_0^+\right]$. We will first note the following
	
	From here we can calculate the expected update rule
	\begin{equation}
		\vtheta_1 = \vtheta_0 + \alpha\sum_{i=1}^m2\vp_i(\vtheta^T_0\vp_i -y_i) + \alpha\sum_{i=1}^l2\vq_i(\vtheta^T_0\vq_i - y_i)
	\end{equation}

	 
	
	\begin{restatable}{lemma}{filipschitz}
		\label{lem:filipschitz}
		$f_i(\vtheta)$ is a Lipschitz Continuous with parameter $\displaystyle L = || \vx_i || _2^2$
	\end{restatable}
	\begin{proof}
		The proof is quite simple in optimization theory, we provide the full proof in Appendix \ref{app:filipschitz}
	\end{proof}
	Let us define two functions for the empirical loss on $\mathbb{P}$ and $\mathbb{Q}$
	\begin{equation}
		\phi(\vtheta) = \frac{1}{np}\sum_{i=1}^m(\vtheta^T\vp_i - y_i)^2
	\end{equation}
	\begin{equation}
		\psi(\vtheta) = \frac{1}{np}\sum_{i=1}^l(\vtheta^T\vq_i - y_i)^2
	\end{equation}
	These two functions hold nice properties.
	\begin{equation}
		\nabla_{\vtheta}\phi(\vtheta) = \frac{1}{np} \sum_{i=1}^m2\vp_i (\vtheta^T\vp_i - y_i)
	\end{equation}
	\begin{equation}
		\nabla_{\vtheta}\psi(\vtheta) = \frac{1}{np} \sum_{i=1}^l2\vq_i(\vtheta^T\vq_i - y_i)
	\end{equation}
	Here we note that the summation of these derivatives is equal to the theta update
	\begin{equation}
		\nabla_{\vtheta_k}(t_{k+1},\vtheta_k) = \nabla_{\vtheta_k}\phi(\vtheta) + \nabla_{\vtheta_k}\psi(\vtheta)
	\end{equation}
	
	\begin{restatable}{assumption}{derivphi-greater-derivpsi}
		\label{asm:derivphi-greater-derivpsi}
		Since we randomly choose the parameters of $\vtheta_0$ and we assume since $m > l$ by expectation,
		\begin{equation}
			\nabla_{\vtheta_0}\phi(\vtheta_0) > \nabla_{\vtheta_0}\psi(\vtheta_0)
		\end{equation}
	\end{restatable}
	
	

	\subsection{Convergence}	
	\begin{restatable}{lemma}{gthetaconvexlemma}
		\label{lem:gthetaconvex}
		$g(t_{k+1},\vtheta_k)$ is convex with respect to $\vtheta_k$.
	\end{restatable}
	
	Lemma \ref{lem:gthetaconvex} tells us we are solving a min-max concave-convex optimization problem. In \cite{Jin_2019}, the researchers examined the problem where they are given a max-oracle which is correct up to some value $\eps$. In our case, we can set $\eps = 0$.
	
	
	\begin{lemma} $g(t,\vtheta)$ is Lipschitz Continuous with respect to $t$
		
	\end{lemma}
	
	\begin{restatable}{lemma}{g-lsmooth}
		\label{lem:g-lsmooth}
		$g(t,\vtheta)$ is $L$-smooth with respect to $\vtheta$ with $\displaystyle L = \norm{\frac{2}{np}\sum_{i=1}^{np}\norm{\vx_i}^2}$ 
	\end{restatable}

	\begin{restatable}{lemma}{g-monotonic}
		\label{lem:g-monotonic}
		Since $g(t,\vtheta)$ is $L$-smooth by Lemma \ref{lem:g-lsmooth} $g(t,\vtheta)$ is a monotonically decreasing function. 
	\end{restatable}
	\begin{proofsketch}
		We are looking to prove $g(t_{k+1},\vtheta_{k+1}) \leq g(t_k,\vtheta_k)$. This is equivalent to proving 
		\begin{equation}
			g(t_{k+1},\vtheta_k) - g(t_{k+1},\vtheta_{k+1}) \geq g(t_{k+1},\vtheta_k) - g(t_k,\vtheta_k)
		\end{equation}
		This is due to the ordering of our two-step optimization.
	\end{proofsketch}

	\begin{restatable}{lemma}{g-bounded}
		\label{lem:g-bounded}
		$g(t,\vtheta)$ is bounded above by $\displaystyle \sum_{i=1}^{np}\vnu_i$ and below by $0$.
	\end{restatable}

	\begin{restatable}{thm}{g-converges}
		\label{thm:g-converges}
		By Lemma \ref{lem:g-monotonic} and \ref{lem:g-bounded}, $g(t,\vtheta)$ converges to a local minimum.
	\end{restatable}
	\begin{proofsketch}
		Note after the $t$-update and $\vtheta$-update as described in equations \ref{eqn:t-update} and \ref{eqn:theta-update}, respectively, lemma \ref{lem:g-monotonic} tells us $g(t_{k+1},\vtheta_{k+1}) \leq g(t_k,\vtheta_k)$
	\end{proofsketch}
	
	\subsubsection{Point Change Conditions}
	
	In this section we mathematically reason the conditions for a point \textit{initially} outside the lowest $np$ squared losses to come within the lowest $np$ squared losses.
	
	Let us take two data points $\vx$ and $\vx'$ such that $\vx \leq t_0$ and $\vx' > t_0$
	\begin{restatable}{thm}{faster-loss-condition}
		\label{thm:faster-loss-condition}
		The rate of decrease of $\vx'$ is greater than $\vx$ iff \begin{equation}|| \vx' || |\vtheta_0^T\vx' - y| \cos(\omega') > ||\vx|| |\vtheta_0^T\vx' - y| \cos(\omega) \end{equation}
	\end{restatable}
	Theorem \ref{thm:faster-loss-condition} reveals to us the importance of $\cos(\omega)$ which represents the angle between $\nabla f_\vx(\vtheta_0)$ and $\nabla g(\vtheta_0)$. By our initial assumption $|\vtheta_0^T\vx' - y'| > |\vtheta_0^T\vx - y|$. Let us look at the example where $|| \vx || = || \vx' ||$, in this case, while $\cos(\omega') > \cos(\omega)$, the rate of decrease of $\vx'$ will be more than the rate of decrease of $\vx$. What this means is there will be an iteration step where $(\vtheta_k^T\vx' - y')^2 < (\vtheta_k^T\vx - y)^2$. Thus $\vx'$ will come within the lowest $np$ squared losses and $\vx$ will no longer be in the $np$ lowest square losses. We can now formulate our convergence conditions. \\
	For all points outside the $np$ lowest squared losses. There exists no point within the $np$ lowest squared losses such that for any $k \in \mathbb{N}$, the points within the $np$ lowest squared losses do not change. 
			

	
	\section{Optimizing for the Sub-Quantile}

	The first experiment we will run will display the difference of the following two $t$ updates \begin{equation} \label{eq:t_update1} t_{k+1} = \hat{\vnu}_{np}\end{equation}  \begin{equation} \label{eq:t_update2} t_{k+1} = \frac{1}{np}\sum_{i=1}^{np}\hat{\vnu}_i \end{equation}In general, if the $\hat{\vnu}_1, \hat{\vnu}_2, \dots, \hat{\vnu}_{np}$ are closely distributed, then $\displaystyle \frac{1}{np}\sum_{i=1}^{np}\hat{\vnu}_{i} \approx \hat{\vnu}_{np}$. In Algorithm \ref{alg:sqo1}, we display our training method for Sub-quantile Optimization with the $t$ update as described in equation \ref{eq:t_update1}. In Algorithm \ref{alg:sqo2}, we use the same training procedure but modify the $t$-update as described in equation \ref{eq:t_update2}. 
	
	\begin{algorithm}[H]
		\DontPrintSemicolon
		
		\KwInput{Training iterations $m$, Quantile $p$, Corruption Percentage $\eps$, Input Parameters $d$}
		\KwOutput{Trained Parameters, $\displaystyle \vtheta$}
		\KwData{Inliers: $\displaystyle y|x \sim \mathcal{N}(x^2-x+2,0.01)$, Outliers: $\displaystyle y|x \sim \mathcal{N}(-x^2+x+4,0.01)$}
		$\vtheta_1 \gets \mathcal{N}(0,\sigma)^d$\\
		\For{$k \in 1,2,\ldots,m$}
		{
			$\displaystyle \vnu = \left(\mX\vtheta_k - \vy\right)^2$\\
			$\hat{\vnu} = sorted(\vnu)$\\
			$\displaystyle t_{k+1} = \hat{\vnu}_{np}$\\
			$L \coloneqq \sum_{i=1}^{np} \vx_i^T\vx_i$\\
			$\alpha \coloneqq \frac{1}{2L}$\\
			$\displaystyle \vtheta_{k+1} = \vtheta_k - \alpha \nabla_{\vtheta_k}g(t_{k+1},\vtheta_k)$
		}
		\KwRet{$ \frac{1}{n} \sum_{i=1}^{n}(\vtheta_m^T\vx_i - y_i)^2$}
		\caption{Sub-Quantile Optimization where $t_{k+1} = \vnu_{np}$}
		\label{alg:sqo1}
	\end{algorithm}

	\begin{algorithm}[H]
		\DontPrintSemicolon
		
		\KwInput{Training iteration, $m$, Quantile $p$, Corruption Percentage $\eps$, Input Parameters $d$}
		\KwOutput{Trained Parameters, $\displaystyle \vtheta$}
		\KwData{Inliers: $\displaystyle y|x \sim \mathcal{N}(x^2-x+2,0.01)$, Outliers: $\displaystyle y|x \sim \mathcal{N}(-x^2+x+4,0.01)$}
		$\vtheta_1 \gets \mathcal{N}(0,\sigma)^d$\\
		\For{$k \in 1,2,\ldots,m$}
		{
			$\displaystyle \vnu = \left(\mX\vtheta_k - \vy\right)^2$\\
			$\hat{\vnu} = sorted(\vnu)$\\
			$t_{k+1} = \frac{1}{np}\sum_{i=1}^{np}\hat{\vnu_{i}}$\\
			$\displaystyle \vtheta_{k+1} = \vtheta_k - \alpha \nabla_{\vtheta_k}g(t_{k+1},\vtheta_k)$
		}
		\KwRet{$ \frac{1}{n} \sum_{i=1}^{n}(\vtheta_m^T\vx_i - y_i)^2$}
		\caption{Sub-Quantile Optimization where $t_{k+1} = \frac{1}{np}\sum_{i=1}^{np}\vnu_{i}$}
		\label{alg:sqo2}
	\end{algorithm}

	\subsection{Synthetic Data}
	\begin{figure}[hp]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				grid=both,
				xmin=-3,xmax=3,
				ymin=0,ymax=14,
				ylabel near ticks,
				xlabel=$x$,ylabel=$y$
				]
				\addplot[mark=*,color=orange,only marks] table [x=Px, y=Py, col sep=comma] {quadratic-regression-p.csv};
				\addplot[mark=*,color=cyan,only marks] table [x=Qx,y=Qy, col sep=comma]{quadratic-regression-q.csv};
				\addplot[
					domain=-3:3,
					samples=60,
					color=green,
					very thick
				] {1.204*x^2-1.036*x+1.688} ;
				\legend{$\mathbb{P}$,$\mathbb{Q}$,$\vtheta$}
			\end{axis}
		\end{tikzpicture}
		\label{fig:synthetic-quadratic}
		\caption{Quadratic Regression, $p = 0.9$}
	\end{figure}
	In our first synthetic experiment, we run Algorithm \ref{alg:sqo1} on synthetically generated quadratic data.

	\subsection{Real Data}
	
	We provide results on the \textit{Drug Discovery} Dataset in \cite{DiakonikolasKKLSS19}
	
%	\begin{table}[t]
%		\caption{Sample table title}
%		\label{sample-table}
%		\begin{center}
%			\begin{tabular}{ll}
%				\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
%				\\ \hline \\
%				Dendrite         &Input terminal \\
%				Axon             &Output terminal \\
%				Soma             &Cell body (contains cell nucleus) \\
%			\end{tabular}
%		\end{center}
%	\end{table}
	
	\subsubsection*{Author Contributions}
	
	\subsubsection*{Acknowledgments}	

	\bibliographystyle{iclr2023_conference}
	\bibliography{iclr2023_conference}
	
	\begin{appendices}
		
	\newpage
	\appendix
	\startcontents[sections]
	\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
	\newpage
	\section{Assumptions}
	\section{General Properties of Sub-Quantile Linear Regression}
	\subsection{Proof of Lemma~\ref{lem:gfermat}}
	\label{app:gfermat}
	\begin{proof}
		Since $\displaystyle g(t,\vtheta)$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
		Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T\mX - \vy)^2\right)$ and note $\displaystyle 0 < p < 1$
		\begin{align}
			\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) &= \partial{\left(-t + \frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)} &&\\
			&= \partial{(-t)} + \partial{\left(\frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)}&&\\
			&= -1 + \frac{1}{np}\sum_{i=1}^{n}\partial{(t-\hat{\vnu}_i)^+} &&\\
			&= -1 + \frac{1}{np}\sum_{i=1}^{n}
			\left\{
			\begin{array}{lr}
				1, & \text{if } t > \hat{\vnu}_i\\
				0, & \text{if } t < \hat{\vnu}_i \\
				\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
			\end{array}
			\right\}&&\\
			&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
		\end{align}
		This is the $p$-quantile of $\displaystyle \vnu$. Not necesarily the $p$-quantile of $Q_p(U)$
	\end{proof}

	\subsection{Proof of Lemma~\ref{lem:gthetaderiv}}
	\label{app:gthetaderiv}
	\begin{proof}
		Note that $t_k = \vnu_{np}$ which is equivalent to $(\vtheta_k^T\vx_{np} - y_{np})^2$
		\begin{align}
			\nabla_{\vtheta_k} g(t_{k+1},\vtheta_k) &= \nabla_{\vtheta_k}\left(\vnu_{np} - \frac{1}{np}\sum_{i=1}^n(\vnu_{np} - (\vtheta_k^T\vx_i - y_i)^2)^+\right) &&\\
			&= \nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+\right) &&\\
			&= \nabla_{\vtheta_k}(\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+ &&\\				
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{n}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) \notag\\ &\qquad-2\vx_i(\vtheta_k^T\vx_i - y_i) \left\{
			\begin{array}{lr}
				1, & \text{if } t > v_i\\
				0, & \text{if } t < v_i \\
				\left[0,1\right], & \text{if } t = v_i \\
			\end{array} \right\} &&\\
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{np}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\	
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) - 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) + \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)&&
		\end{align}
	This is the derivative of the $np$ samples with lowest error with respect to $\vtheta$.
	\end{proof}
	
	\subsection{Proof of Lemma~\ref{lem:P-iequals1}}
	\label{app:P-iequals1}
	\begin{proof}
		The probability a point from $\mathbb{P}$ is within the $np$ lowest squared points is equivalent to the probability of a point from $\mathbb{P}$ being within the $p$ quantile of the combined distribution of $\mathbb{P}$ and $\mathbb{Q}$. Let $\mu_P$ be the average loss over all points in $\mathbb{P}$ and $\mu_Q$ be the average loss over all points in $\mathbb{Q}$. Similarly let $\sigma_P^2$ and $\sigma_Q^2$ be the respective variances.
		\begin{equation}
			\mu_P = \frac{1}{n(1-\eps)}\sum_{i=1}^{n(1-\eps)}(\vtheta^T\vp_i - y_i)^2
		\end{equation}
		\begin{equation}
			\mu_Q = \frac{1}{n\eps}\sum_{i=1}^{n\eps}(\vtheta^T\vq_i - y_i)^2
		\end{equation}
		\begin{equation}
			\sigma_P^2 = \frac{1}{n(1-\eps) - 1}\sum_{i=1}^{n(1-\eps)}(\mu_P - (\vtheta^T\vp_i - y_i)^2)^2
		\end{equation}
		\begin{equation}
			\sigma_Q^2 = \frac{1}{n\eps - 1}\sum_{i=1}^{n\eps}(\mu_Q - (\vtheta^T\vq_i - y_i)^2)^2
		\end{equation}
		Let us now calculate the combined distribution, which by our problem statement is $\hat{\mathbb{P}}$.
		\begin{align}
			\mu_{\hat{P}} &= (1-\eps)\mu_P + \eps \mu_Q &&\\
			\sigma_{\hat{P}} &= (1-\eps)^2 \sigma_P^2 + \eps^2\sigma_Q^2 + \eps(1-\eps)Cov(P,Q) &&\\
			&= (1-\eps)^2\sigma_P^2 + \eps^2\sigma_Q^2 &&
		\end{align}
		Notice the Covariance is 0 because the samples are i.i.d. Now we will calculate the $p$-quantile of $\mathbb{Z}$. \\
		Let $\Phi \sim \mathcal{N}(0,1)$ 
		\begin{equation}
			Q_p(\hat{\mathbb{P}}) = \mu_{\hat{P}} + \Phi^{-1}(p)\sigma_{\hat{P}}
		\end{equation}
		$Q_p(\hat{\mathbb{P}})$ represents the $np$th squared loss. We know want to know what is the probability a point from $\mathbb{P}$ is below this.
		\begin{align}
			\mathbb{P}\left[P_i < Q_p(\hat{\mathbb{P}})\right] &= \Phi\left(\frac{Q_p(\hat{\mathbb{P}}) - \mu_{P}}{\sigma_P}\right) &&\\
			&= \Phi\left(\frac{(1-\eps)\mu_P + \eps\mu_Q + \Phi^{-1}(p)((1-\eps)^2\sigma^2_P + \eps^2\sigma^2_Q) - \mu_P}{\sigma_P}\right) &&\\
			&= \Phi\left(\frac{-\eps\mu_P + \eps\mu_Q + \Phi^{-1}(p)((1-\eps)^2\sigma^2_P + \eps^2\sigma^2_Q) }{\sigma_P}\right)
		\end{align}
	\end{proof}
		
	\subsection{Proof of Lemma~\ref{lem:filipschitz}}
	\label{app:filipschitz}
	\begin{proof}
   		Note we defined $\displaystyle g_i(\vtheta)  = (\vtheta^T\vx_i - y_i)^2 $, thus $\displaystyle \nabla g_i(\vtheta) = 2\vx_i(\vtheta^T\vx_i - y_i)^2$. We will prove there exists $\beta$ such that 
   		\begin{equation}
   			\displaystyle || \nabla g_i(\vtheta') - \nabla g_i(\vtheta) || \leq \beta|| \vtheta' - \vtheta ||
   		\end{equation}
   	The proof is as follows
   	\begin{align}
		|| \nabla g_i(\vtheta') - \nabla g_i(\vtheta) || &= || 2\vx_i (\vtheta'^T\vx_i - y_i) - 2\vx_i (\vtheta^T\vx_i - y_i) || &&\\
		&= || 2\vx_i(\vtheta'^T\vx_i) - 2\vx_iy_i - 2\vx_i(\vtheta^T\vx_i) + 2\vx_iy_i || &&\\
		&= || 2\vx_i(\vtheta'^T\vx_i - \vtheta^T\vx_i) || &&\\
		&= || 2\vx_i || \, || \vtheta'^T\vx_i - \vtheta^T\vx_i || &&\\		
		&= 2 || \vx_i ||^2 || \vtheta' - \vtheta || &&
  	\end{align}
  Thus $g_i(\vtheta)$ is $|| \vx_i ||^2$-smooth
   	\end{proof}
  
  \subsection{Proof of Theorem~\ref{thm:faster-loss-condition}}
  	\label{app:faster-loss-condition}
	\begin{proof}
		As given in the assumption, $f_{\vx}(\vtheta) < f_{\vx'}(\vtheta)$. So we are interested in the condition for $f_{\vx'}(\vtheta_1) - f_{\vx'}(\vtheta_0) < f_{\vx}(\vtheta_1) - f_{\vx}(\vtheta_0)$. We will calculate $f_{\vx}(\vtheta_1) - f_{\vx}(\vtheta_0)$ and generalize the results for $\vx'$.
		\begingroup
		\addtolength{\jot}{1em}
		\begin{align}
			f_\vx(\vtheta_1) - f_\vx(\vtheta_0) &= (\vtheta_1^T\vx - y)^2 - (\vtheta_0^T - y)^2 &&\\
			&= (\vtheta_1^T\vx)^2 - 2(\vtheta_1^T\vx)y - (\vtheta^T_0\vx)^2 + 2(\vtheta_0^T\vx)y &&\\
			&= ((\vtheta_0 - \alpha \nabla g(\vtheta_0))^T\vx)^2 - 2((\vtheta_0 - \alpha\nabla g(\vtheta_0))^T\vx)y \notag\\ &\qquad - (\vtheta^T_0\vx)^2 + 2(\vtheta_0^T\vx)y &&\\
			\intertext{Note $\vtheta_1 = \vtheta_0 - \alpha\nabla g(t_1,\vtheta)$ by Equation \ref{eqn:theta-update}}
			&= (\vtheta_0^T\vx - \alpha\nabla g(\vtheta_0)^T \vx)^2 - 2(\vtheta_0^T\vx)y + 2\alpha(\nabla g(\vtheta)^T\vx)y \notag\\ &\qquad- (\vtheta^T_0\vx)^2 + 2(\vtheta_0^T\vx)y &&\\
			&= (\vtheta_0^T\vx)^2 - 2\alpha(\vtheta_0^T)(\nabla g(\vtheta)^T\vx) + \alpha^2(\nabla g(\vtheta)^T\vx)^2 + 2\alpha(\nabla g(\vtheta)^T\vx)y \notag\\ &\qquad- (\vtheta^T_0\vx)^2&& \\
			&= - 2\alpha(\vtheta_0^T)(\nabla g(\vtheta)^T\vx) + \alpha^2(\nabla g(\vtheta)^T\vx)^2 + 2\alpha(\nabla g(\vtheta)^T\vx)y &&\\
			&= \alpha(\nabla g(\vtheta_0)^T\vx) (-2(\vtheta_0)^T\vx + \alpha \nabla g(\vtheta_0)^T\vx + 2y) &&\\
			&= \alpha(\nabla g(\vtheta_0)^T\vx) (-2(\vtheta_0^T\vx -y)) + \alpha \nabla g(\vtheta_0)^T\vx) &&\\
			\intertext{Note $\nabla_{\vtheta}f_{\vx}(\vtheta) = 2\vx(\vtheta^T\vx - y)$}
			&= -\alpha\nabla g(\vtheta_0)^T\nabla f_\vx(\vtheta_0) + \alpha^2 (\nabla g(\vtheta_0)^T\vx)^2 &&\\
			&= -\alpha\left(|| \nabla g(\vtheta_0) || || \nabla f_\vx(\vtheta_0) || \cos(\omega)- \alpha || \nabla g(\vtheta_0) ||^2 || \vx || ^2 \cos^2(\eta) \right) &&\\
			&= -\alpha || \nabla g(\vtheta_0) || \left(|| f_\vx(\vtheta_0) || \cos(\omega) - \alpha || \nabla g(\vtheta_0) || || \vx ||^2 \cos^2(\eta)\right) &&\\
			&= -\alpha || \nabla g(\vtheta_0) || \left(2|| \vx || | \vtheta_0^T\vx - y | \cos(\omega) - \alpha || \nabla g(\vtheta_0) || || \vx ||^2 \cos^2(\eta)\right) &&\\
			&= -\alpha || \nabla g(\vtheta_0) || || \vx || \left(2|| | \vtheta_0^T\vx - y | \cos(\omega) - \alpha || \nabla g(\vtheta_0) || || \vx || \cos^2(\eta)\right) &&
		\end{align}
		\endgroup
		Now we will generalize our results to the inequality $f_{\vx'}(\vtheta_1) - f_{\vx'}(\vtheta_0) < f_\vx(\vtheta_1) - f_{\vx'}(\vtheta_0)$
		\begin{equation}
			\label{eqn:unsimple-decrease}
			\begin{split}
				|| \vx' || (2 |\vtheta_0^T\vx'-y|\cos(\omega') - \alpha || & \nabla g(\vtheta) || || \vx'|| \cos^2(\eta'))\\
				& > || \vx || (2 |\vtheta_0^T\vx-y|\cos(\omega) - \alpha || \nabla g(\vtheta) || || \vx|| \cos^2(\eta))
			\end{split}
		\end{equation}
		Here we note that $\alpha$ is a very small term, $\alpha = \frac{1}{2L}$ where $L = || \mX^T \mX ||$
		So equation \ref{eqn:unsimple-decrease} can be approximately simplied. 
		\begin{equation}
			|| \vx' || |\vtheta_0^T\vx' - y'| \cos(\omega') > ||\vx|| |\vtheta_0^T\vx - y| \cos(\omega)
		\end{equation}
		This completes the proof.
	\end{proof}

	\section{Proofs for Convergence}
	\subsection{Proof of Lemma~\ref{lem:g-lsmooth}}
	This is a standard proof in Optimization Theory. The objective function 
	$\displaystyle g(\vtheta,t)$ is $L$-smooth w.r.t $\vtheta$ iff
	\begin{equation}
		||  \nabla_\vtheta g(\vtheta',t) - \nabla_\vtheta g(\vtheta,t) || \leq L|| \vtheta' - \vtheta || 
	\end{equation}
	\begin{align}
		\norm{ \nabla_{\vtheta} g(\vtheta^{'},t) - \nabla_{\vtheta} g(\vtheta,t) } &= \norm{ \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'T} \vx_i - y_i) - \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T \vx_i - y_i) } &&\\
		&= \norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'T}\vx_i - \vtheta_k^T\vx_i)}&&\\
		&= \norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i^T\vx_i(\vtheta_k^{'T} - \vtheta_k^T)} &&\\
		&\leq \norm{\frac{2}{np}\sum_{i=1}^{np}\norm{\vx_i}^2}\norm{\vtheta_k^{'T} - \vtheta_k^T} &&\\
		&= L\norm{\vtheta_k^{'T} - \vtheta_k^T} &&
	\end{align}
	where $\displaystyle L = \norm{\frac{2}{np}\sum_{i=1}^{np}\norm{\vx_i}^2}$\\
	This concludes the proof.
	\end{appendices}
\end{document}
