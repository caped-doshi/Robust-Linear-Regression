@incollection{Bengio+chapter2007,
	author = {Bengio, Yoshua and LeCun, Yann},
	booktitle = {Large Scale Kernel Machines},
	publisher = {MIT Press},
	title = {Scaling Learning Algorithms Towards {AI}},
	year = {2007}
}

@article{Hinton06,
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	journal = {Neural Computation},
	pages = {1527--1554},
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	volume = {18},
	year = {2006}
}

@book{goodfellow2016deep,
	title={Deep learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	volume={1},
	year={2016},
	publisher={MIT Press}
}

@article{Lu_2020,
	doi = {10.4208/cicp.oa-2020-0165},
	url = {https://doi.org/10.4208%2Fcicp.oa-2020-0165},
	year = {2020},
	publisher = {Global Science Press},
	volume = {28},
	number = {5},
	pages = {1671--1706},
	author = {Lu Lu},
	title = {Dying {ReLU} and Initialization: Theory and Numerical Examples},
	journal = {Communications in Computational Physics}
}

@misc{Jin_2019,
	doi = {10.48550/ARXIV.1902.00618},
	url = {https://arxiv.org/abs/1902.00618},
	author = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I.},
	title = {What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?},
	publisher = {arXiv},
	year = {2019},
}


@article{LTS,
	abstract = {The linear least trimmed squares (LTS) estimator is a statistical technique for fitting a linear model to a set of points. Given a set of n points in ℝdand given an integer trimming parameter h≤n, LTS involves computing the (d−1)-dimensional hyperplane that minimizes the sum of the smallest h squared residuals. LTS is a robust estimator with a 50 {\%}-breakdown point, which means that the estimator is insensitive to corruption due to outliers, provided that the outliers constitute less than 50 {\%} of the set. LTS is closely related to the well known LMS estimator, in which the objective is to minimize the median squared residual, and LTA, in which the objective is to minimize the sum of the smallest 50 {\%} absolute residuals. LTS has the advantage of being statistically more efficient than LMS. Unfortunately, the computational complexity of LTS is less understood than LMS. In this paper we present new algorithms, both exact and approximate, for computing the LTS estimator. We also present hardness results for exact and approximate LTS. A number of our results apply to the LTA estimator as well.},
	author = {Mount, David M. and Netanyahu, Nathan S. and Piatko, Christine D. and Silverman, Ruth and Wu, Angela Y.},
	date = {2014/05/01},
	date-added = {2023-03-10 22:48:14 -0500},
	date-modified = {2023-03-10 22:48:14 -0500},
	doi = {10.1007/s00453-012-9721-8},
	id = {Mount2014},
	isbn = {1432-0541},
	journal = {Algorithmica},
	number = {1},
	pages = {148--183},
	title = {On the Least Trimmed Squares Estimator},
	url = {https://doi.org/10.1007/s00453-012-9721-8},
	volume = {69},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00453-012-9721-8}}

@article{LAD,
	ISSN = {00063444, 14643510},
	URL = {http://www.jstor.org/stable/20441446},
	abstract = {We develop a unified L₁-based analysis-of-variance-type method for testing linear hypotheses. Like the classical L ₂ -based analysis of variance, the method is coordinate-free in the sense that it is invariant under any linear transformation of the covariates or regression parameters. Moreover, it allows singular design matrices and heterogeneous error terms. A simple approximation using stochastic perturbation is proposed to obtain cut-off values for the resulting test statistics. Both test statistics and distributional approximations can be computed using standard linear programming. An asymptotic theory is derived for the method. Special cases of one- and multi-way analysis of variance and analysis of covariance models are worked out in detail. The main results of this paper can be extended to general quantile regression. Extensive simulations show that the method works well in practical settings. The method is also applied to a dataset from General Social Surveys.},
	author = {Kani Chen and Zhiliang Ying and Hong Zhang and Lincheng Zhao},
	journal = {Biometrika},
	number = {1},
	pages = {107--122},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Analysis of Least Absolute Deviation},
	urldate = {2023-03-11},
	volume = {95},
	year = {2008}
}

@article{li2020tilted,
	author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
	journal={arXiv preprint arXiv:2007.01162},
	title={Tilted empirical risk minimization},
	year={2020}
}

@misc{https://doi.org/10.48550/arxiv.2206.04777,
	doi = {10.48550/ARXIV.2206.04777},
	url = {https://arxiv.org/abs/2206.04777},
	author = {Awasthi, Pranjal and Das, Abhimanyu and Kong, Weihao and Sen, Rajat},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized Linear Models},
	publisher = {arXiv},
	year = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{DiakonikolasKKLSS19,
	author        = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
	title         = {Sever: A Robust Meta-Algorithm for Stochastic Optimization},
	booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
	series        = {ICML '19},
	year          = {2019},
	pages         = {1596--1606},
	publisher     = {JMLR, Inc.}
}

@unknown{Razaviyayn,
	author = {Razaviyayn, Meisam and Huang, Tianjian and Lu, Songtao and Nouiehed, Maher and Sanjabi, Maziar and Hong, Mingyi},
	year = {2020},
	month = {06},
	pages = {},
	title = {Non-convex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances}
}

@article{ROCKAFELLAR2014140,
	title = {Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk},
	journal = {European Journal of Operational Research},
	volume = {234},
	number = {1},
	pages = {140-154},
	year = {2014},
	issn = {0377-2217},
	doi = {https://doi.org/10.1016/j.ejor.2013.10.046},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221713008692},
	author = {R.T. Rockafellar and J.O. Royset and S.I. Miranda},
	keywords = {Generalized regression, Superquantiles, Conditional value-at-risk, Uncertainty quantification, Buffered failure probability, Stochastic programming},
	abstract = {The paper presents a generalized regression technique centered on a superquantile (also called conditional value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of regression functions, discuss the stability of regression functions under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management.}
}

@article{quantile-regression,
	Author = {Koenker, Roger and Hallock, Kevin F.},
	Title = {Quantile Regression},
	Journal = {Journal of Economic Perspectives},
	Volume = {15},
	Number = {4},
	Year = {2001},
	Month = {December},
	Pages = {143-156},
	DOI = {10.1257/jep.15.4.143},
	URL = {https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143}}

@article{thiel-sen,
	author = { Pranab   Kumar   Sen },
	title = {Estimates of the Regression Coefficient Based on Kendall's Tau},
	journal = {Journal of the American Statistical Association},
	volume = {63},
	number = {324},
	pages = {1379-1389},
	year  = {1968},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.1968.10480934},
	URL = { 
	https://www.tandfonline.com/doi/abs/10.1080/01621459.1968.10480934
	},
	eprint = { https://www.tandfonline.com/doi/pdf/10.1080/01621459.1968.10480934
	}
}

@Book{Huber2009,
	author={Huber, Peter J.
	and Ronchetti, Elvezio.},
	title={Robust statistics},
	series={Wiley series in probability and statistics},
	year={2009},
	edition={2nd ed.},
	publisher={Wiley},
	address={Hoboken, N.J.},
	url={http://catdir.loc.gov/catdir/toc/ecip0824/2008033283.html},
	language={English}
}
