
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{mathtools}

\usepackage{geometry}

\usepackage{subcaption}

\usepackage{bbm}

\usepackage{color, colortbl}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{darkgray176}{RGB}{176,176,176}

\DeclareMathOperator{\proj}{proj}

\newcommand\cauchyschwarz{\stackrel{\mathclap{\normalfont\mbox{Cauchy-Schwarz}}}{\leq}}

\newcommand\woodbury{\stackrel{\mathclap{\normalfont\mbox{Woodbury}}}{=}}

\newenvironment{proofsketch}{%
	\renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\concat}{%
	\mathchoice%
	{\Big\Vert}%
	{\big\Vert}%
	{\Vert}%
	{\Vert}%
}

\usepackage{hyperref}

\usepackage{cleveref}

\usepackage{tikz,pgfplots,float}
\usetikzlibrary {datavisualization.formats.functions}
\usepgfplotslibrary{fillbetween}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{matrix}
\usepackage{xcolor}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\declaretheorem[name=Theorem]{thm}
\declaretheorem[name=Lemma,numberwithin=thm]{lemma}
\declaretheorem[name=Definition]{definition}
\declaretheorem[name=Assumption]{assumption}
\declaretheorem[name=Interpretation]{interpretation}
\declaretheorem[name=Proposition]{proposition}
\declaretheorem[name=Corollary,numberwithin=thm]{corollary}
\declaretheorem[name=Fact]{fact}

\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled]{algorithm2e}
\DontPrintSemicolon
% Set algorithm keyword formatting
\SetKwComment{tcc}{$\triangleright$~}{}
\SetCommentSty{normalfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
% Set algorithm line numbers
\SetNlSty{}{}{:}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\usepackage{url}

\usepackage{appendix}
\usepackage{titletoc}

\usepackage{booktabs,caption,dcolumn}
\newcolumntype{d}[1]{D{.}{.}{4}}% column type for figures with 4 decimals
\newcommand{\subhead}[1]{\multicolumn{1}{c}{#1}}% to format sub-headings of d-type columns

\title{Robust Linear Regression by Sub-Quantile Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Arvind Rathnashyam, Fatih Orhan, Joshua Myers, \& Jake Herman  \thanks{ Work done as a part of ML and Optimization Spring 2023 Group Project.} \\
	Department of Computer Science\\
	Rensselaer Polytechnic University\\
	Troy, NY 12180, USA \\
	\texttt{\{rathna, orhanf, myersj5, hermaj2\}@rpi.edu} \\
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Robust Linear Regression is the problem of fitting data to a distribution, $P$ when there exists contaminated samples, $Q$. We consider the Huber Contamination modeled as $\hat{P} = (1-\varepsilon)P + \varepsilon Q$ where $\varepsilon \in (0,0.5)$. Traditional Least Squares Methods fit the empirical risk model to all training data in $\displaystyle \hat{P}$. In this paper we show theoretical and experimental results of sub-quantile optimization, where we optimize with respect to the $p$-quantile of the empirical loss. Sub-Quantile Optimization theoretically and empirically works in the case of both oblivious and adversarial outliers. 
	\end{abstract}
	
	\section{Introduction}
	
	Linear Regression is one of the most widely used statistical estimators throughout science. Robustness Learning in High Dimensions on Huber Contamination Models, \cite{Huber2009}, has gained much attention in the last decade, \cite{Diakonikolas2019RecentAI}. The key motivating factor in investigating robust linear regression is the sheer vastness of probability distributions that are not drawn from a normal distribution schema. Given that outliers in data sets occur so frequent, the ability for a linear regression model to be robust is necessary to compensate for the various distributions being analyzed. 
	\subsection{Motivations}
	The failure of classical regression techniques being unable to model data highly corrupted by outliers can be conveyed clearly in numerous datasets, including those featuring data in the medical, economic, and meteorological fields. Ultimately, in many real data sets, the samples may not be collected from even or fair distributions; thus, classical analyses such as standard regression or least-squares may not represent the actual distribution of the data well. 
	
	The quantile is a statistical measure that is distribution-agnostic, this makes it very suitable for robust estimation in the Huber Contamination Model. 
	
	\subsection{Contributions}
	Our goal is to provide a theoretic analysis and convergence conditions for sub-quantile optimization and offer practitioners a method for robust linear regression.
	Several popular methods have been utilized due to their simplicity and high effectiveness including quantile regression \cite{quantile-regression}, Theil-Sen Estimator \cite{thiel-sen}, and Huber Regression \cite{Huber2009}. These methods, although rudimentary, serve to show the effectiveness of building resistance against outliers in data. By improving upon existing methods, namely least-squares estimation in these cases, models can be designed to better estimate data sets with considerably corruptive outliers.
	
	Sub-Quantile Optimization aims to address the shortcomings of ERM in applications such as noisy/corrupted data (\cite{khetan2018learning},\cite{jiang2018mentornet}), classification with imbalanced classes, (\cite{lin2017dense},\cite{he2009imbalanced}), as well as fair learning (\cite{Corbett2018fairness}).
	
	\begin{figure}[!t]
		\begin{tikzpicture}
			\begin{groupplot}[group style={group size= 2 by 4},height=5cm,width=6.4cm,xmax=4,xmin=-4,
				ymin= -10,ymax=10,
				xtick={-4,-3,-2,...,4},
				ytick={-10,-8,-6,...,10},]
				\nextgroupplot[title=Adversarial Noise,ylabel={$y$},xlabel={$x$}]
				\coordinate (c1) at (current axis.left of origin);
				\addplot[mark=*,color=cyan,only marks] coordinates{(0.5454756141119426,4.20207144062236) (2.6952740390873733,8.475671650669172) (-0.1612438784423731,2.688323570846449) (1.9998807196500725,7.011477368661404) (0.6573126643085468,4.432503359641061) (0.822541876079238,4.691099695987436) (-1.665736406528514,-0.2771610262088303) (0.2955820690953599,3.6958661148678047) (4.329988449821567,11.633259356797302) (-4.412313203501535,-5.77063957797815) (3.5629538644332275,10.091868592245838) (-0.6175885445775965,1.7473187369931718) (0.8108793561773843,4.772417668204566) (1.4214273053887914,5.818934958869865) (3.081315566792172,9.08926804908307) (-0.12089034451099817,2.6605946971138144) (0.4628376617792958,3.923002461330265) (0.37617729415550505,3.676581100428469) (0.977114787165429,4.918947422974899) (1.314754156260769,5.688804682122404) (-0.36235424331683597,2.2267827837116263) (-0.8007556950385752,1.4886373858967947) (-0.7015664550907283,1.7651694045148667) (-2.1753647987904734,-1.482231084924366) (-1.495574696589605,0.04857682714064265) (-1.0588112744961395,0.8183551891451499) (-3.827119610541674,-4.796523392389788) (-0.8559789159750391,1.1837704089349095) (-0.11761120022533014,2.7730611413376285) (3.3752350984949295,9.778804511993851) (2.4222770389407957,7.727148811092623) (-1.5410032016624133,-0.2633438113497304) (2.480017633230306,7.979402356067379) (0.5911583134769168,4.283748748593926) (-0.71501674964969,1.6324881095139794) (0.6516043112113717,4.294631666773893) (-3.079711134644328,-3.285226613281087) (-1.5088703031224757,-0.08081402676688078) (0.7827672445662932,4.570750601954812) (1.8939059587103262,6.858172766747038)};
				\addplot[mark=*,color=orange,only marks] coordinates{(-2.946629011640607,-8.793723527498958) (0.8607384957406246,-1.538989556510056) (-1.1903001477494382,-5.345751610044574) (1.0305878148541996,-0.8812658035679226) (2.7345343942849927,2.5442143953062244) (-1.6113746066635852,-6.284678207630399) (-0.8883952236652384,-4.5410943399901775) (0.8157297797465353,-1.2566730413348421) (0.9642301396642339,-0.9032576567416104) (-0.14448993411816005,-3.334946134068381)};
				\addplot[domain=-5:5,samples=120,color=black,very thick] {1.99804993*x+2.99972117} ;
				\addplot[domain=-5:5,samples=120,color=red,very thick] {0.55184722*x+0.77673256} ;
				\addplot[domain=-5:5,samples=120,color=green,very thick] {0.61291924*x+1.73788515} ;
				\nextgroupplot[title=Oblivious Noise,xlabel={$x$},legend to name={IntroLegend},legend style={legend columns=5}]
				\coordinate (c2) at (current axis.right of origin);% I moved this to the upper right corner
				\addplot[domain=-5:5,samples=120,color=black,very thick] {2.00041502*x+3.00091281} ;
				\addlegendentry{SubQuantile}
				\addplot[domain=-5:5,samples=120,color=red,very thick] {0.000166177232*x-1.89493121} ;
				\addlegendentry{TERM}
				\addplot[domain=-5:5,samples=120,color=green,very thick] {1.90359844*x+2.82276139} ;
				\addlegendentry{Sever}
				\addplot[mark=*,color=cyan,only marks] coordinates{(0.788762485297498,4.619409754983769) (-1.8444557076323929,-0.5824544981134812) (-3.8667186653234786,-4.754991283615981) (-1.0770702633013416,0.7722164021812354) (-0.35460032498749144,2.2768627851664966) (-6.0822151692288084,-9.06286764938399) (-1.3972518211012221,0.1885381821988659) (2.9097896507970673,8.788974171667162) (-0.3463437020806441,2.420626808504963) (-2.1062359514012012,-1.2085000641078332) (4.336773668705573,11.632780878244247) (-0.6106102670869219,1.8184898177512854) (2.964822961712252,8.905021515559394) (-1.5637145371389622,-0.07761865537709459) (1.0345144344033383,5.0204590940620575) (2.056356483528722,7.198480210723839) (-0.7446791767339869,1.5853174482562558) (0.9631985535487033,4.9300987804715914) (0.8591431516732644,4.6302171243067765) (2.136527593488855,7.225942702339085) (-0.22230647531299189,2.5877920816746207) (1.4255477338524918,5.833868092502212) (-1.4608390115429133,0.12305693654859831) (0.8455335501589681,4.769483861326801) (-2.1885026970448886,-1.4714423105566306) (0.31837311592194245,3.604291796660203) (-2.5244214783210617,-2.0353095069267404) (0.9463907944621844,4.957203119009067) (0.8146787696108556,4.669681514124957) (0.7171745412553174,4.525279797042462)};
				\addlegendentry{clean}
				\addplot[mark=*,color=orange,only marks] coordinates{(-3.2474795820658593,6.450035960386647) (1.8753826722797218,-0.41858598306318395) (1.4931353198669002,-1.2946095099790276) (-6.390816833283051,0.40645831126308823) (0.39048632994899735,2.323962397589362) (-0.08068388106259215,2.8091622671588747) (1.3134080902531131,-3.6866845075408796) (-0.5728261692279717,-1.6637925896534693) (-3.795620644314326,3.901941933326407) (-2.2805802625691634,2.1567188534369586) (-1.1331317978018236,-1.0620378556289347) (-4.057270733769615,6.22788036393642) (3.580927646622491,1.4134339560728868) (1.1473909216831844,0.7892374536150136) (4.60155271303633,1.1396713973323012) (0.777450139132804,-2.5904275309220073) (1.2193257645912936,0.9783812850138183) (-2.0137550195781966,0.08990758847097716) (-3.0052924293950043,0.9558207339897568) (-1.6843214424199917,3.0989236738078767)};
				\addlegendentry{noise}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
				{\pgfplotslegendfromname{IntroLegend}};
		\end{tikzpicture}
		\caption{Sub-Quantile Performance on Adapative Outliers}
		\label{fig:structure-unstructured-noise}
	\end{figure}
	
	As seen in the above comparison, current models fail to estimate data sets corrupted by structured noise, with some models even failing to estimate trends plagued with unstructured noise. Through this, sub-quantile optimization is shown to prevail at overcoming these challenges current models currently face. In Table 
	
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcc}
			\toprule 
			Paper & Adversary & Threshold\\   
			\midrule                
			Sever \cite{DiakonikolasKKLSS19} & Adaptive & Gradient of Loss \\
			\midrule
			CRR \cite{bhatia2017} & Oblivious & \\
			\midrule 
			This Paper & Adapative & Loss\\
			\bottomrule
		\end{tabular}
		\caption{A comparison of different iterative thresholding algorithms for Robust Least Squares Regression}
		\label{tab:related-work}
	\end{table}
	
	\section{Related Work}
	Least Trimmed Squares (LTS) \cite{LTS} is an estimator that relies on minimizing the sum of the smallest $h$ residuals given a $(d-1)$-dimension hyperplane calculated given $n$ data points in $\mathbf{R}^d$ and an integer trimming parameter $h$. Given that the outliers comprise less than half the data, this algorithm is more efficient than the more common LMS estimator. However, this algorithm unfortunately suffers from the curse of dimensionality; the computational cost of the algorithm grows exponentially with increasing dimensions of the data. Thus, the necessity to design a more computationally efficient algorithm is expressed.
	
	Tilted Empirical Risk Minimization (TERM) \cite{li2020tilted} is a framework built to similarly handle the shortcomings of emperical risk minimization (ERM) with respect to robustness. The TERM framework instead minimizes the following quantity, where $t$ is a hyperparameter known as tilt
	\begin{equation}
		\tilde{R}(t;\vtheta) \coloneqq \frac{1}{t} \log\left(\frac{1}{N}\sum_{i \in \left[N\right]}e^{tf(\vx_i;\vtheta)} \right)
	\end{equation}
	By using the tilt hyperparameter to change the individual impact of each specific loss, the model is more resistant to outliers found in the data.
	
	SMART \cite{https://doi.org/10.48550/arxiv.2206.04777} proposes the \textit{iterative trimmed maximum likelihood estimator} against adversarially corrupted samples in General Linear Models (GLM). The estimator is defined as follows, where $S = \{(\vx_i,y_i)\}_{i=1}^n$ represents the training data. \vspace{1em}
	\begin{equation}
		\hat{\vtheta}(S) = \min_{\vtheta} \min_{\hat{S} \subset S, |\hat{S}| = (1-\eps)n} \sum_{(\vx_i,y_i) \in S} -\log f(y_i|\vtheta^T\vx_i)
	\end{equation}
	This estimator is proven to return near-optimal risk on a variety of linear models, including Gaussian regression, Poisson regression, and binomial regression; these achievements can be demonstrated on label and covariate corruptions.
	
	
	SEVER \cite{DiakonikolasKKLSS19} is a gradient filtering algorithm which removes elements whose gradients have the furthest distance from the average gradient of all points
	\begin{equation}
		\tau_i = \left((\nabla f_i(\vw) - \hat{\nabla})\cdot \vv\right)^2
	\end{equation}
	This method is novel in that it is highly scalable, making it robust against high-dimension data with structured outliers. Similarly, SEVER is easily implemented with standard machine learning libraries and can be applied to many typical learning problems, including classification and regression. Despite this, the algorithm still falls short when features have high covariance or when features have low predictive power of the target. Moreover, SEVER requires approximate learners to be run after every iteration, making SEVER unfeasible for large-scale machine learning tasks. 
	
	Quantile Regression \cite{yu_quantile_2003} relies on splitting data into quantiles to better represent data that is not evenly distributed. The paper introduces various estimation methods for quantile regression and apply them to a multitude of datasets. In doing so, they prove quantile regression is suitable at estimating both linear and nonlinear response models.
	
	Super-Quantile Optimization \cite{ROCKAFELLAR2014140} aims to solve error minimization problems by building upon the aforementioned quantile regression by centering around a conditional value-at-risk, or a superquantile. For $\alpha \in [0,1)$, the $\alpha$-superquantile for a random variable $Y$ is defined as \begin{equation}
		\Bar{q}_\alpha (Y) := \frac{1}{1-\alpha} \int_\alpha^1 q_\beta (Y) d\beta
	\end{equation}
	In doing so, more conservatively fitted curves are produced. As with quantile regression, such curves do require the solution of a linear program.
	This concept of superquantile error provides insight into tail behavior for quantities of error and an overall unique approach to linear regression.
	
	Robust Risk Minimization \cite{RRM} is a method in which given an upper bound on the corrupted data fraction $\epsilon$, the risk function can be minimized as follows:
	\begin{equation}
		\hat{\mathbb{\theta}}_{RRM} = \operatorname*{argmin}_{\theta \in \Theta} \operatorname*{min}_{\pi \in \Pi: \mathbf{H}(\pi) \geq ln|(1-\tilde{\epsilon})n|} R(\mathbb{\theta,\pi})
	\end{equation}
	This method is popular as it does not require the removal of corrupted data points and does not rely on a specified corruption fraction.
		
	\section{Sub-quantile Optimization}
	\label{sec:sub-quantile-optimization}
	
	\begin{definition}
		Let $F_X$ represent the Cumulative Distribution Function (CDF) of the random variable $X$. The \textbf{$\mathbf{p}$-Quantile} of a Random Variable $X$ is defined as follows \vspace{1em}
		\begin{equation}
			Q_p(p) = \inf\{x\in\mathbb{R}: p \leq F(x)\} 
		\end{equation}
	\end{definition}
	
	\begin{definition}
		Let $\ell$ be the loss function. \textbf{Risk} is defined as follows
		\begin{equation}
			U = \mathbb{E}_{(\vx,y) \sim \mathbb{P}}\left[\ell \left( f(\vx;\vtheta,y)\right)\right]
		\end{equation}
	\end{definition}
	The $\mathbf{p}$-\textbf{Quantile} of the Empirical Risk is given
	\begin{equation}\label{eqn:sub-quantile}
		\mathbb{L}_p(U) = \frac{1}{p}\int_0^p \mathcal{Q}_q(U)\,dq = \mathbb{E}\left[U|U \leq \mathcal{Q}_p(U) \right] = \max_{t\in \mathbb{R}}\left\{t - \frac{1}{p}\mathbb{E}\left[(t-U)^+\right]\right\}
	\end{equation}
	In equation \ref{eqn:sub-quantile}, $t$ represents the $p$-quantile of $U$. We also show that we can calculate $t$ by a maximizing optimization function. 
	The Sub-Quantile Optimization problem is posed as follows
	\begin{equation}
		\vtheta_{SM} = \argmin_{\vtheta \in \mathbb{R}^d} \max_{t \in \mathbb{R}} \left\{t - \frac{1}{p}\mathbb{E}(t - \ell(f(\vx;\vtheta),y))^+\right\}
	\end{equation}
	
	For the linear regression case, this equation becomes 
	\begin{equation}
		\label{eqn:theta_sm}
		\vtheta_{SM} = \argmin_{\vtheta \in \mathbb{R}^d} \max_{t \in \mathbb{R}} \left\{t - \frac{1}{np}\sum_{i=1}^n(t-(\vtheta^T\vx_i - y_i)^2)^+\right\}
	\end{equation}
		
	The two-step optimization for Sub-Quantile optimization is given as follows \vspace{1em}
	\begin{equation}
		\label{eqn:t-update}
		t_{k+1} = \argmax_t g(t,\vtheta_k) 
	\end{equation}
	\begin{equation}
		\label{eqn:theta-update}
		\vtheta_{k+1} = \vtheta_k + \alpha \nabla_{\vtheta_k} g(t,\vtheta_k)
	\end{equation}
		
	This algorithm is adopted from \cite{Razaviyayn}. Theoretically, it has been proven to converge in research by \cite{Jin_2019}.\\
	\begin{minipage}{0.48\textwidth}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		
		\KwInput{Training iterations $T$, Quantile $p$, SubQuantile Update: $j$}
		\KwOutput{Trained Parameters, $\displaystyle \vtheta_{(T)}$}
		$\vtheta_{(0)} \gets (X^T X)^{-1}X^T y$\\
		\For{$k \in 1,2,\ldots,T$}
		{
			$\displaystyle \vnu \gets \left(X\vtheta_{(k)} - y\right)^2$\\
			\If{$k \% j = 0$}
			{
				$\hat{\vnu} \gets sorted(\vnu)$\\
				$\displaystyle t_{(k+1)} \gets \hat{\vnu}_{np}$\\
			}
			$S_{(k)} \gets \concat \vx_i\text{ if } (\vtheta_{(k)}^T \vx_i - y_i)^2 \leq t_{(k+1)}$\\
			$L_{(k)} \gets \frac{1}{np}\norm{S^T S}_2$\\
			$\alpha_{(k)} \gets \frac{1}{2L_{(k)}}$\\
			$\displaystyle \vtheta_{(k+1)} \gets \vtheta_k - \alpha_{(k)} \nabla_\vtheta g(t_{(k+1)},\vtheta_{(k)})$
		}
		\KwRet{$ \vtheta_T $}
		\caption{Sub-Quantile Minimization Gradient Descent}
		\label{alg:sqo1}
	\end{algorithm}
	\end{minipage}
	\hfill
	\begin{minipage}{0.48\textwidth}
		\begin{algorithm}[H]
			\KwInput{Training Iterations $T$,Quantile $p$}
			\KwOutput{Trained Parameters, $\vtheta_{(T)}$}
			$\vtheta_{(0)} \gets (X^T X + \lambda I)^{-1}X^T y$\\
			\For{$k \in \left\{1,2,\dots,T\right\}$}
			{
				$\vnu \gets (X \vtheta_{(k)} - y)^2$\\
				$t_{(k+1)} \gets \hat{\vnu}_{np}$\\
				$S_{(k)} \gets \concat \vx_i\text{ if } (\vtheta_{(k)}^T \vx_i - y_i)^2 \leq t_{(k+1)} \text{ where $\concat$ represents the concatenation operator}$\\
				$\vtheta_{(k+1)} \gets (S^T S + \lambda I)^{-1}S^T y_S$\\
			}
			\KwRet{$ \vtheta_T $}
			\caption{Sub-Quantile Minimization for Ridge Regression}
			\label{alg:sqo-ridge}
		\end{algorithm}
	\end{minipage}
		
	\subsection{Motivation}
	\begin{assumption}\label{asm:normal-corruption}
		To provide theoretical bounds on the effectiveness of Sub-Quantile Minimization, we make the General Linear Model Assumption that
		\begin{equation}\vspace{1em}
			\vy_P = P\vbeta_P  + \eps_P
		\end{equation}
		and similarly 
		\begin{equation}
			\vy_Q = Q \vbeta_Q + \eps_Q
		\end{equation}
		where $\vbeta_P$ and $\vbeta_Q$ the \textit{oracle} regressors for $\mathbb{P}$ and $\mathbb{Q}$ and $\eps_P$ and $\eps_Q$ are both Normally Distributed with mean $0$.  
	\end{assumption}
	Since we are interested in learning the optimal model for distributions, our goal is to learn the parameters $\vbeta_P$ from the distribution $\hat{P}$. We want to clarify the corruption is not adversarially chosen. \\
	In this section we quantify the effect of corruption on the desired model. To introduce notation, let $P$ represent the data from distribution $\mathbb{P}$ and let $Q$ represent the training data for $\mathbb{Q}$. Let $\vy_P$ represent the target data for $\mathbb{P}$ and let $\vy_Q$ represent the target data for $\mathbb{Q}$.\\
	\begin{assumption}\label{asm:p-q-sample}
		We assume the rows of $P$ and $Q$ are sampled from the same multivariate normal distribution. \vspace{1em}
		\begin{equation}
			P_i,Q_j \sim \mathcal{N}_p(\mathbf{0},\Sigma)
		\end{equation}
	\end{assumption}
	
	We will use our assumptions to quantify the effect of the corrupted data on an optimal least squares regression model. We are interested in $\displaystyle (X^T X)^{-1}X^T y - (P^T P)^{-1}P^T y$.
	It is know the least squares optimal solution for $X$ is equal to $(X^T X)^{-1}X^T y$\\
	Note $\displaystyle X = \begin{pmatrix}P \\ Q \end{pmatrix}$ and $\displaystyle y = \begin{pmatrix} \vy_P \\ \vy_Q \end{pmatrix} $so $X^T = \begin{pmatrix}P^T & Q^T \end{pmatrix}$\\
	
	\begin{restatable}{thm}{expected-value-corrupted}\label{thm:expected-value-corrupted}
		The expected optimal parameters of the corrupted model $\hat{\mathbb{P}}$\vspace{1em}
		\begin{equation}
			\label{eqn:corrupted-optimal}
			\mathbb{E}\left[X^{\dagger}y \right] = \vbeta_P + \eps(\vbeta_Q - \vbeta_P)
		\end{equation}
	\end{restatable}
	The proof is reliant on assumption \ref{asm:p-q-sample}, this allows us to utilize the Wishart Distribution, $\mathcal{W}$, and the inverse Wishart Distribution, $\mathcal{W}^{-1}$. Please refer to Appendix \ref{app:expected-value-corrupted}. By Theorem \ref{thm:expected-value-corrupted} we can see the level of corruption is dependent upon $\eps$, which represents the percentage of corrupted samples, and the distance between the optimal parameters for $\mathbb{P}$, which is $\vbeta_P$ and the optimal parameters for $\mathbb{Q}$, which is $\vbeta_Q$. 
	
		
	Here we utilize the idea of \textit{influence} from \cite{McWilliams2014}.
	
	Theorem \ref{thm:expected-value-corrupted} finds the optimal model when the corrupted distribution is sampled from the same distribution as the target distribution but has different optimal parameters. We will now look at the case of feature corruption. This is where the optimal parameters of the two distributions are the same but the data from $\mathbb{P}$ and $\mathbb{Q}$ are sampled differently. 
	
	In equation \ref{eqn:corrupted-optimal}, note as $\displaystyle \eps \to 0$ we are returned $\vbeta_P$. This is the intuition behind SubQuantile Minimization. By minimizing over the SubQuantile, we seek to reduce $\eps$, and thus our model will return a model which is by expectation closer to $\vbeta_P$. 	

	\section{Theory} 
	
	\subsection{Analysis of $g(t,\theta)$}
	
	In this section, we will explore the fundamental aspects of $g(t,\vtheta)$. This will motivate the convergence analysis in the next section.
	
	\begin{restatable}{lemma}{gtconcavelemma}
		\label{lem:gtcomcavelemma}
		$g(t_{k+1},\vtheta_k)$ is concave with respect to $t$.
	\end{restatable}
	\begin{proof}
		We provide a simple argument for concavity. Note $t$ is a concave and convex function. Also $(\cdot)^+$ is a convex strictly non-negative function. Therefore we have a concave function minus the non-negative multiple of a summation of an affine function composed with a convex function. Therefore this is a concave function with respect to $t$. 
	\end{proof}	
	
	\begin{restatable}{lemma}{gfermat}
		\label{lem:gfermat}
		The maximizing value of $t$ in $g(t,\vtheta)$ in $t$-update step of optimization as described by Equation \ref{eqn:t-update} is maximized when $t = Q_p(U)$
	\end{restatable}
		\begin{proof}
		Since $\displaystyle g(t,\vtheta)$ with respect to $t$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
		Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T X - y)^2\right)$ and note $\displaystyle 0 < p < 1$
		\begin{align}
			\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) 
			&= -1 + \frac{1}{np}\sum_{i=1}^{n}
			\left\{
			\begin{array}{lr}
				1, & \text{if } t > \hat{\vnu}_i\\
				0, & \text{if } t < \hat{\vnu}_i \\
				\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
			\end{array}
			\right\}&&\\
			&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
		\end{align}
		This is the $p$-quantile of $U$. A full proof is provided in Appendix \ref{app:gfermat}.
	\end{proof}
	
	\begin{restatable}{lemma}{gthetaderiv}\label{lem:gthetaderiv}
		Let $t = \hat{\vnu}_{np}$. The $\vtheta$-update step described in Equation \ref{eqn:theta_sm} is equivalent to minimizing the least squares loss of the $np$ elements with the lowest squared loss.
	   \begin{equation}
			\nabla_{\vtheta}g(t_{k+1},\vtheta_k) = \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)
		\end{equation}
	\end{restatable}
	We provide a proof in Appendix \ref{app:gthetaderiv}. However, this result is quite intuitive as it shows we are optimizing over the $p$ Sub-Quantile of the Risk.
	\begin{interpretation}
		\label{int:minimize-small}
		Sub-Quantile Minimization continously minimizes the risk over the $p$-quantile of the error. In each iteration, this means we reduce the error of the points within the lowest $np$ errors.
	\end{interpretation}

		
	\begin{restatable}{lemma}{gthetaconvexlemma}
		\label{lem:gthetaconvex}
		$g(t_{k+1},\vtheta_k)$ is convex with respect to $\vtheta_k$.
	\end{restatable}
	\begin{proof}
		We see by lemma \ref{lem:gfermat} and interpretation \ref{int:minimize-small}, we are optimizing by the $np$ points with the lowest squared error. Mathematically, 
		\begin{align*}
			g(t_{k+1},\vtheta_k) &= t_{k+1} - \frac{1}{np}\sum_{i=1}^{n}\left(t_{k+1} - (\vtheta^T\vx_i - y_i)^2\right)^+ &&\\
			&= t - t + \frac{1}{np}\sum_{i=1}^{np}(\vtheta^T\vx_i - y_i)^2 &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}(\vtheta^T\vx_i - y_i)^2 &&
		\end{align*}
		Now we can make a simple argument for convexity. We have a non-negative multiple of the sum of the composition of an affine function with a convex function. Thus $g(t,\vtheta)$ is convex with respect to $\vtheta$.
	\end{proof}
	
	\begin{restatable}{lemma}{g-lsmooth}
		\label{lem:g-lsmooth}
		$g(t,\vtheta)$ is $L$-smooth with respect to $\vtheta$ with $\displaystyle L = \norm{\frac{2}{np}\sum_{i=1}^{np}\norm{\vx_i}^2}$ 
	\end{restatable}
	
	Now we will state two properties regarding the effect of the $t$-update step and the $\vtheta$-update step as described in Equations \ref{eqn:t-update} and \ref{eqn:theta-update}, respectively. 
	\begin{restatable}{lemma}{g-change-wrt-t}
		\label{lem:g-change-wrt-t}
		If $t_{k+1} \leq t_k$ then $ g(t_{k+1},\vtheta_k) = g(t_{k}) + \frac{1}{np}\sum_{i=np}^n(t_k - \vnu_i)^+$. If $t_{k+1} > t_k$, then $ g(t_{k+1},\vtheta_k) = g(t_k) + \frac{1}{np}\sum_{i=n(p-\delta)}^{np}(t-\vnu_i)^+  - \delta t$. For a small $\delta$. 
	\end{restatable}
	\begin{proofsketch}
		When $t_{k+1} \leq t_k$ this result is quite intuitive, as we are simply removing the error of the elements outside elements within the lowest $np$ squared losses. We delegate the rest of the proof to Appendix \ref{app:g-change-wrt-t}
	\end{proofsketch}

	\subsection{Optimization}

	We are solving a min-max convex-concave problem, thus we are looking for a Nash Equilibrium Point. 
	
	\begin{restatable}{definition}{nash-equilibrium}
		\label{def:nash-equilibrium}
		$(t^*,\vtheta^*)$ is a \textbf{Nash Equilibrium} of $g$ if for any $(t,\vtheta) \in \mathbb{R}\times\mathbb{R}^d$\vspace{1em}
		\begin{equation}
			g(t^*,\vtheta) \leq g(t^*,\vtheta^*) \leq g(t,\vtheta^*)
		\end{equation}
	\end{restatable}	
	\begin{restatable}{definition}{local-nash-equilibrium}
		\label{def:local-nash-equilibrium}
		$(t^*,\vtheta^*)$ is a \textbf{Local Nash Equilibrium} of $g$ if there exists $\delta > 0$ such that for any $t,\vtheta$ $(t,\vtheta)$ satsifying $\norm{t - t^*} \leq \delta$ and $\norm{\vtheta -\vtheta^*} \leq \delta$ then: 
		\vspace{1em}
		\begin{equation}
			g(t^*,\vtheta) \leq g(t^*,\vtheta^*) \leq g(t,\vtheta^*)
		\end{equation}
	\end{restatable}
	\begin{proposition}\label{prop:first-order-nash}
		As $g$ is first-order differentiable,  any local Nash Equilibrium satisfies $\nabla_\vtheta g(t,\vtheta) = \mathbf{0}$ and $\nabla_t g(t,\vtheta) = 0$
	\end{proposition}
	
	We are now interested in what it means to be at a Local Nash Equilibrium. By Proposition \ref{prop:first-order-nash}, this means both first-order partial derivatives are equal to $0$. By lemma \ref{lem:gfermat}, we have shown $\nabla_tg(t,\vtheta) = 0$ when $\vnu_{np} \leq t < \vnu_{np+1}$. Furthermore, by lemma \ref{lem:gthetaderiv}, we have shown $\nabla_\vtheta(g,\vtheta) = 0$ when the least squares error is minimized for the $np$ points with lowest squared error. In other words:
	\begin{align*}
		\mathbb{E}\left[\nabla_\vtheta g(t_{k+1},\vtheta_k)\right] &= 0 &&\\
		2(\vmu\vmu^T + \Sigma)(\vtheta_k - (1-\varepsilon)\vbeta_P-\varepsilon\vbeta_Q) &= 0&&
	\end{align*}
	Since the first term is non-zero, the equality is satisfied when:
	\begin{equation*}
		(\vtheta_k - (1-\varepsilon)\vbeta_P-\varepsilon\vbeta_Q) = 0
	\end{equation*}	
	\begin{equation*}
		\vtheta_k = (1-\varepsilon)\vbeta_P + \varepsilon\vbeta_P 
	\end{equation*}
	Note this aligns with the results of Theorem \ref{thm:expected-value-corrupted}.
	This means that for a subset of $np$ points from $X$, the least squares error is minimized. What we are interested in is how many points within those $np$ points come from $\mathbb{P}$ and how many of those points from $\mathbb{Q}$. Our goal is to minimize the number of points within the $np$ lowest squared losses from $Q$, as they will introduce error to our predictions on points from $P$. 
	
	
	
	\begin{restatable}{thm}{convergence-guarantee}
		\label{thm:convergence-guarantee}
		Let $g(t,\vtheta)$ be differentiable and $g(t,\vtheta)$ be $L$-smooth in $\vtheta$. Let $t_{(k)}$ and $\vtheta_{(k)}$ be iterates from algorithm \ref{alg:sqo-ridge}. Then, $\lim_{k \to \infty} \mathbb{E}\left[\norm{\nabla_\vtheta g(t_{(k+1)},\vtheta_{(k)})}\right] = 0$.
	\end{restatable}
	\begin{proofsketch}
		From the intuitions we have gained on Subquantile Minimization. We can state the following:
		\begin{equation}
			\argmin_{\vtheta \in \mathbb{R}^d}\min_{S \in \Pi}\norm{\vtheta^T S - y_S}_2^2 \iff \argmin_{\vtheta \in \mathbb{R}^d}\max_{t \in \mathbb{R}}\norm{\vtheta^T S - y_S}_2^2
		\end{equation}
		where $\Pi$ represents the entire distribution of Subquantile matrices. 
	\end{proofsketch}
	

	\subsection{Converging to $\beta_P$}
	
	We will start by defining the two types of noise we are interesting in. 
	\begin{definition}
		\textbf{Oblivious Noise} is noise that is not dependent on the input data, i.e., $\displaystyle \mathbb{P}\left[y | X\right] = \mathbb{P}\left[y\right]$
	\end{definition}
	\begin{definition}
		\textbf{Adaptive Noise} is noise that is made from a linear combination of the input data, i.e. $\displaystyle y = \vbeta_QX + \veps$
	\end{definition}
	Also note we often consider Gaussian Noise as Oblivious Noise, but it can be modeled as Adaptive Noise where $\vbeta_Q = \vzero$. 

	\begin{restatable}{lemma}{effect-of-projection}
		\label{lem:effect-of-projection}
		The expected value of error on points in $\mathbb{P}$ will be lower than the expected value of error on points in $\mathbb{Q}$ if $\proj_{\vbeta_P}(\vtheta) - \vbeta_P < \proj_{\vbeta_Q}(\vtheta) - \vbeta_Q$
	\end{restatable}
	
	Lemma \ref{lem:effect-of-projection} gives us an intuitive result, the proof is in appendix \ref{app:effect-of-projection}. If in each optimization step, our projection on $\vbeta_P$ is closer than our projection on to $\vbeta_Q$, we know the number of steps from $\mathbb{Q}$ will increase from the previous iteration.
	\begin{restatable}{thm}{improvement-of-theta}
		\label{thm:improvement-of-theta}
		After an optimization step, 
		\begin{equation*}
			\left|\proj_{\vbeta_P}\vtheta_{(t+1)} -1 \right| - \left|\proj_{\vbeta_P}\vtheta_{(t)} -1 \right| <  \left|\proj_{\vbeta_Q}\vtheta_{(t+1)} - 1\right| - \left|\proj_{\vbeta_Q}\vtheta_{(t)} - 1\right|
		\end{equation*}
			  if the following holds
		\begin{equation}
			\norm{\left((1-\varepsilon^{(t)}) - \alpha_1\right)\vbeta_P} > \norm{\left(\varepsilon^{(t)} - \alpha_2\right)\vbeta_Q}
		\end{equation}
		where $\alpha_1$ and $\alpha_2$ represents the coefficients for the linear combination of $\vtheta$ in the basis defined as $\mB = \begin{bmatrix} \vbeta_P & \vbeta_Q & \mR\end{bmatrix}$
	\end{restatable}

	We are also interested in Algorithm \ref{alg:sqo-ridge}, so we provide theory for it.
	\begin{restatable}{corollary}{ridge-algorithm-convergence}
		\label{thm:ridge-algorithm-convergence}
		Theorem \ref{alg:sqo-ridge} by expectation converges to a good solution. 
	\end{restatable}
	\begin{proofsketch}
		Let us first note $\varepsilon^{(0)} < 0.5$, and by Theorem \ref{thm:expected-value-corrupted}, $\vtheta_{(0)} = (1-\varepsilon^{(0)})\vbeta_P + \varepsilon^{(0)}\vbeta_Q$. It thus follows:
		\begin{equation}
			\norm{\vtheta_{(0)} - \vbeta_P} = \norm{\varepsilon^{(0)}\vbeta_P + \varepsilon^{(0)}\vbeta_Q}
		\end{equation}
		and similarly 
		\begin{equation}
			\norm{\vtheta_{(0)} - \vbeta_Q} = \norm{(1 - \varepsilon^{(0)})\vbeta_P + (1 - \varepsilon^{(0)}) \vbeta_Q}
		\end{equation}
		Therefore in the linear case 
		\begin{equation}\label{eqn:theta-0-closer-to-p}
			\norm{\varepsilon^{(0)}\vbeta_P + \varepsilon^{(0)}\vbeta_Q} < \norm{(1 - \varepsilon^{(0)})\vbeta_P + (1 - \varepsilon^{(0)}) \vbeta_Q}
		\end{equation}
		Then if $\Var(\epsilon_P) \leq \Var(\epsilon_Q)$, it follows 
		\begin{equation*}
			\mathbb{E}\left[\left(\vtheta_{(0)}^T \vp - y_i\right)^2\right] \leq \mathbb{E}\left[\left(\vtheta_{(0)}^T \vq - y_i\right)^2\right]
		\end{equation*}	
		This concludes the proof.
	\end{proofsketch}
	
	Note theorem \ref{thm:ridge-algorithm-convergence} is enough to show the final subquantile matrix will have more elements from $P$. It thus follows, in the case of no feature noise, by expectation, $\vtheta$ will move towards $\vbeta_P$ faster than it moves to $\vbeta_Q$. We instead calculate the probability in theorem \ref{thm:probability-epsilon-decreasing}. 
	
	We are now interested in theoretical guarantees with no distributional assumptions. First we will consider some intuition on why this problem is not as hard as compared to when the corruption is linearly structured. Let us say the noise is of non-linear regression, in other words $\vy_Q \sim f(X,\vbeta_Q)$, where $f$ is a non-linear combination of features of $X$. In this case, it is not possible to model the non-linear regression by a linear combination of the features, thus, if we have elements from $\mathbb{Q}$ within the lowest $np$ losses, then training on these points will not generalize well to points from $\mathbb{Q}$, so their error will not decrease. 

	
	\section{Empirical Results}\label{sec:numerical-experiments}

	We also present a batch algorithm which improves training speed significantly. In accordance with Minibatch theory, if the subset $I$ of all data is representative of all the data, then this will have similar results to Algorithm \ref{alg:sqo1}.

	\subsection{Synthetic Data}
	
	\begin{figure}[!b]
		\centering
		\begin{tikzpicture}
			\begin{groupplot}[group style={
					group name=myplot,
					group size= 2 by 1},height=6cm,width=0.5\linewidth,
					tick align=outside,
					tick pos=left,
					legend pos=outer north east,
					x grid style={darkgray176},
					xmin=0.1, xmax=0.4,
					xtick style={color=darkgray176},
					y grid style={darkgray176},
					ymin=0, ymax=1000,
					ytick style={color=darkgray176},
					ytick={0,0.001,0.01,0.1,1,10,100,1000},
					ymajorgrids=true,
					grid style=dashed,
					ymode=log,
					log ticks with fixed point,
				]
			\nextgroupplot[ylabel=Test RMSE,title=\texttt{Adaptive Linear Regression}]
			\coordinate (c1) at (current axis.left of origin);
			\addplot [very thick, black,mark=square*]
			table {%
				0.1 0.0125554371625185
				0.13 0.0125358905643225
				0.16 0.0129381362348795
				0.19 0.0133792851120234
				0.22 0.0137144038453698
				0.25 0.0135425860062242
				0.28 0.0141213834285736
				0.31 0.014158477075398
				0.34 0.0144458776339889
				0.37 0.0152960792183876
				0.4 0.0154073610901833
			};
			\addplot [very thick, green,mark=square*]
			table {%
				0.1 0.0521650016307831
				0.13 0.0445362776517868
				0.16 0.0440022274851799
				0.19 0.0425776466727257
				0.22 0.0404041111469269
				0.25 0.0345044210553169
				0.28 0.0329911485314369
				0.31 0.0288031361997128
				0.34 0.0262199137359858
				0.37 0.0241425465792418
				0.4 0.0189416743814945
			};
			\addplot [very thick, red,mark=square*]
			table {%
				0.1 237.896408081055
				0.13 227.826049804688
				0.16 230.267791748047
				0.19 240.226058959961
				0.22 247.975341796875
				0.25 229.499435424805
				0.28 240.464080810547
				0.31 232.256820678711
				0.34 231.335601806641
				0.37 244.49983215332
				0.4 238.339447021484
			};
			\addplot [very thick, orange,mark=square*]
			table {%
				0.1 0.554122269153595
				0.13 0.700407743453979
				0.16 0.853803277015686
				0.19 1.00364184379578
				0.22 1.14672327041626
				0.25 1.29208374023438
				0.28 1.446732878685
				0.31 1.58860540390015
				0.34 1.74065947532654
				0.37 1.89163446426392
				0.4 2.03946781158447
			};
			\addplot [very thick, purple,mark=square*]
			table {%
				0.1 2.18698644638062
				0.13 2.05041885375977
				0.16 2.53638052940369
				0.19 1.44385433197021
				0.22 1.8708907365799
				0.25 1.2774612903595
				0.28 2.1888313293457
				0.31 1.95714855194092
				0.34 1.51524746417999
				0.37 1.80507040023804
				0.4 2.08419466018677
			};
			\addplot [very thick, cyan,mark=square*]
			table {%
				0.1 0.55413419008255
				0.13 0.70040762424469
				0.16 0.853901743888855
				0.19 1.00362634658813
				0.22 1.14677453041077
				0.25 1.29210960865021
				0.28 1.44673705101013
				0.31 1.58858036994934
				0.34 1.74076187610626
				0.37 1.8917053937912
				0.4 2.03950119018555
			};
			\addplot [very thick, blue,mark=square*]
			table {%
				0.1 0.381346136331558
				0.13 0.567987024784088
				0.16 0.730611085891724
				0.19 0.883088111877441
				0.22 1.02834987640381
				0.25 1.16971182823181
				0.28 1.31483685970306
				0.31 1.45577394962311
				0.34 1.60332000255585
				0.37 1.74965143203735
				0.4 1.91863870620728
			};
			\addplot [very thick, teal,mark=square*]
			table {%
				0.1 0.167886197566986
				0.13 0.279087960720062
				0.16 0.372507721185684
				0.19 0.495923638343811
				0.22 0.622189044952393
				0.25 0.781642854213715
				0.28 1.02761101722717
				0.31 1.26646912097931
				0.34 1.43443489074707
				0.37 1.68049740791321
				0.4 1.95596516132355
			};
			\nextgroupplot[legend to name={CommonLegend},legend style={legend columns=8},title=\texttt{Oblivious Linear Regression}]
			\coordinate (c2) at (current axis.right of origin);
			\addplot [very thick, black,mark=square*]
			table {%
				0.1 0.0128471776843071
				0.13 0.0132497362792492
				0.16 0.0133665129542351
				0.19 0.0132069261744618
				0.22 0.0136235542595387
				0.25 0.014018764719367
				0.28 0.0141585795208812
				0.31 0.0143229514360428
				0.34 0.0147281233221292
				0.37 0.0151354484260082
				0.4 0.0153331179171801
			};\addlegendentry{SubQuantile}
			\addplot [very thick, green,mark=square*]
			table {%
				0.1 0.0540679693222046
				0.13 0.0504136011004448
				0.16 0.0463055409491062
				0.19 0.0414693579077721
				0.22 0.0409594476222992
				0.25 0.0382022894918919
				0.28 0.0341422967612743
				0.31 0.0303579084575176
				0.34 0.0269672852009535
				0.37 0.0250413306057453
				0.4 0.022193968296051
			};\addlegendentry{Sever}
			\addplot [very thick, red,mark=square*]
			table {%
				0.1 246.295135498047
				0.13 246.269775390625
				0.16 247.859344482422
				0.19 237.750305175781
				0.22 242.284729003906
				0.25 251.897003173828
				0.28 242.425659179688
				0.31 237.389801025391
				0.34 237.131713867188
				0.37 234.046859741211
				0.4 232.261001586914
			};\addlegendentry{TERM}
			\addplot [very thick, orange,mark=square*]
			table {%
				0.1 63.5551452636719
				0.13 145.981842041016
				0.16 157.928924560547
				0.19 225.061798095703
				0.22 319.59423828125
				0.25 375.710998535156
				0.28 390.717224121094
				0.31 491.960021972656
				0.34 469.778259277344
				0.37 414.336395263672
				0.4 534.187805175781
			};\addlegendentry{Ransac}
			\addplot [very thick, purple,mark=square*]
			table {%
				0.1 1.46980595588684
				0.13 1.87412810325623
				0.16 3.35894680023193
				0.19 2.29600596427917
				0.22 2.45804905891418
				0.25 2.86910390853882
				0.28 1.73560845851898
				0.31 2.57050657272339
				0.34 2.66841530799866
				0.37 1.83763372898102
				0.4 29.6955299377441
			};\addlegendentry{Huber}
			\addplot [very thick, cyan,mark=square*]
			table {%
				0.1 28.1231517791748
				0.13 34.0072326660156
				0.16 41.1976280212402
				0.19 47.9778938293457
				0.22 56.1155090332031
				0.25 64.5382843017578
				0.28 69.4678497314453
				0.31 75.4956512451172
				0.34 83.6161499023438
				0.37 87.8404235839844
				0.4 95.7903823852539
			};\addlegendentry{ERM}
			\addplot [very thick, blue,mark=square*]
			table { 
				0.1 2.51470565795898
				0.13 5.41616868972778
				0.16 9.31512641906738
				0.19 13.0698957443237
				0.22 18.0407886505127
				0.25 23.9833374023438
				0.28 29.2465381622314
				0.31 35.9098510742188
				0.34 42.9741249084473
				0.37 48.0417175292969
				0.4 56.2337112426758
			};\addlegendentry{CRR}
			\addplot [very thick, teal,mark=square*]
			table { 
				0.1 0.531812369823456
				0.13 0.662591338157654
				0.16 0.542104721069336
				0.19 1.25191402435303
				0.22 2.22975873947144
				0.25 2.94694495201111
				0.28 3.24172282218933
				0.31 3.70417833328247
				0.34 226.69775390625
				0.37 225.584381103516
				0.4 225.356048583984
			};\addlegendentry{STIR}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
			{\pgfplotslegendfromname{CommonLegend}};
		\end{tikzpicture}
		\caption{\texttt{Structured Linear Regression} \& \texttt{Noisy Linear Regression} Datasets}
		\label{fig:synthetic-linear-regression}
	\end{figure}
	We now demonstrate SubQuantile Regression in the presence of Gaussian Random Noise. 
		
	In our first synthetic experiment, we run Algorithm \ref{alg:sqo1} on synthetically generated structured linear regression data, the noise is sampled from a linear distribution that is dependent on the vector of $X$. Our results show the near optimal performance of Sub-Quantile Minimization. The results and comparison with other methods can be seen in Table \ref{tab:quadratic-regression}. We see in Table \ref{tab:quadratic-regression}, Sub-Quantile Minimization produces State of the Art Results in the Quadratic Regression Case. Furthermore, it performs significantly better than baseline methods in the high-noise regimes $(\eps = 0.4)$, this is confirmed in both the small data and large data datasets. Please refer to Appendix \ref{app:experimental-details} for more details on the \texttt{Structured Linear Regression} Dataset. 

	\subsection{Real Data}
	
	We provide results on the \texttt{Drug Discovery} Dataset in \cite{DiakonikolasKKLSS19} utilizing the noise procedure described in \cite{li2020tilted}. For each algorithm, if possible we use Ridge Regression, else we use typical least squares. SubQuantile Minimization, ERM, RANSAC, SEVER, TERM, and SMART are all capable of Ridge Regression. 
	
		\begin{figure}
		\centering
		\pgfplotsset{scaled y ticks=false}
		\begin{tikzpicture}
			\begin{groupplot}[group style={
					group name=myplot,
					group size= 2 by 1},height=6cm,width=0.5\linewidth,
				tick align=outside,
				tick pos=left,
				legend pos=outer north east,
				x grid style={darkgray176},
				xmin=0.1, xmax=0.4,
				xtick style={color=darkgray176},
				y grid style={darkgray176},
				ymin=0.8, ymax=16,
				ytick style={color=darkgray176},
				%ytick={0.1,1,6},
				ymajorgrids=true,
				grid style=dashed,
				ymode=log,
				log basis y={2},
				log ticks with fixed point,
				]
				\nextgroupplot[ylabel=Test RMSE,title=Oblivious Noise]
				\coordinate (c1) at (current axis.left of origin);
				\addplot [very thick, black,mark=square*]
				table {%
					0.1 0.882
					0.13 0.940
					0.16 0.980
					0.19 0.963
					0.22 0.942
					0.25 1.03914260864258
					0.28 1.07516872882843
					0.31 1.05182552337646
					0.34 1.18973660469055
					0.37 1.22633171081543
					0.4 1.13422465324402
				};
				\addplot [very thick, green,mark=square*]
				table {%
					0.1 1.00317788124084
					0.13 1.05096316337585
					0.16 1.1015111207962
					0.19 1.0629894733429
					0.22 1.10910606384277
					0.25 1.08021092414856
					0.28 1.10074019432068
					0.31 1.11493849754333
					0.34 1.28342533111572
					0.37 1.76835131645203
					0.4 2.51373958587646
				};
				\addplot [very thick, red,mark=square*]
				table {%
					0.1 1.45994329452515
					0.13 1.37001824378967
					0.16 1.44806778430939
					0.19 1.45339107513428
					0.22 1.41311264038086
					0.25 1.38933956623077
					0.28 1.50759243965149
					0.31 1.51065766811371
					0.34 1.49574542045593
					0.37 1.60239005088806
					0.4 1.50713086128235
				};
				\addplot [very thick, orange,mark=square*]
				table {%
					0.1 529206607872
					0.13 248858066944
					0.16 2117881954304
					0.19 805079744512
					0.22 779128274944
					0.25 860870279168
					0.28 1958463799296
					0.31 815870509056
					0.34 997245255680
					0.37 2418093064192
					0.4 1562900692992
				};
				\addplot [very thick, purple,mark=square*]
				table {%
					0.1 1.25154936313629
					0.13 1.44858860969543
					0.16 1.52697730064392
					0.19 1.32283139228821
					0.22 1.59512722492218
					0.25 1.63998472690582
					0.28 2.10984826087952
					0.31 1.81169891357422
					0.34 1.65826773643494
					0.37 1.62222862243652
					0.4 2.32202768325806
				};
				\addplot [very thick, cyan,mark=square*]
				table {%
					0.1 1.17466223239899
					0.13 1.59103155136108
					0.16 1.75408041477203
					0.19 1.68247008323669
					0.22 1.57592701911926
					0.25 2.24019551277161
					0.28 2.96093344688416
					0.31 2.06391406059265
					0.34 1.72260689735413
					0.37 2.03535914421082
					0.4 3.04255032539368
				};
				\addplot [very thick, blue,mark=square*]
				table { 
					0.1 1.02377986907959
					0.13 1.08241760730743
					0.16 1.22590231895447
					0.19 1.29572296142578
					0.22 1.38240742683411
					0.25 1.79025042057037
					0.28 2.58377599716187
					0.31 1.94901299476624
					0.34 1.70375561714172
					0.37 2.07255697250366
					0.4 3.18292450904846
				};
				\nextgroupplot[legend to name={DrugLegend},legend style={legend columns=7},title=Adaptive Noise]
				\coordinate (c2) at (current axis.right of origin);% I moved this to the upper right corner
				\addplot [very thick, black,mark=square*]
				table {%
					0.1 1.29677128791809
					0.13 1.22180867195129
					0.16 1.35283875465393
					0.19 2.0823187828064
					0.22 1.9808212518692
					0.25 2.02461886405945
					0.28 1.68995881080627
					0.31 1.84686589241028
					0.34 2.73925709724426
					0.37 2.29721999168396
					0.4 3.76814651489258
				};\addlegendentry{SubQuantile}
				\addplot [very thick, green,mark=square*]
				table {%
					0.1 1.00960803031921
					0.13 1.03763794898987
					0.16 1.02492117881775
					0.19 1.13363671302795
					0.22 1.11420011520386
					0.25 0.988892674446106
					0.28 1.03024888038635
					0.31 1.09969282150269
					0.34 1.05457997322083
					0.37 1.09024500846863
					0.4 24.5828189849854
				};\addlegendentry{Sever}
				\addplot [very thick, red,mark=square*]
				table {%
					0.1 1.33121466636658
					0.13 1.36325013637543
					0.16 1.34999430179596
					0.19 1.3828319311142
					0.22 1.42280578613281
					0.25 1.32452011108398
					0.28 1.34932780265808
					0.31 1.39414024353027
					0.34 1.35189986228943
					0.37 1.38910329341888
					0.4 1.39197826385498
				};\addlegendentry{TERM}
				\addplot [very thick, orange,mark=square*]
				table {%
					0.1 2404382670848
					0.13 1237811331072
					0.16 1555973275648
					0.19 2838279225344
					0.22 2747414609920
					0.25 5128395423744
					0.28 1649884266496
					0.31 2477872644096
					0.34 3036363096064
					0.37 4817864359936
					0.4 3626952294400
				};\addlegendentry{Ransac}
				\addplot [very thick, purple,mark=square*]
				table {%
					0.1 3.70415687561035
					0.13 4.70572566986084
					0.16 9.21195125579834
					0.19 8.9310827255249
					0.22 9.00326251983643
					0.25 13.6958179473877
					0.28 18.9618034362793
					0.31 32.9235801696777
					0.34 48.9421997070312
					0.37 69.7418212890625
					0.4 64.1903228759766
				};\addlegendentry{Huber}
				\addplot [very thick, cyan,mark=square*]
				table {%
					0.1 27.9922885894775
					0.13 34.2989959716797
					0.16 41.1022644042969
					0.19 47.1282081604004
					0.22 54.4811897277832
					0.25 52.0908088684082
					0.28 58.3855781555176
					0.31 65.2018814086914
					0.34 66.0037002563477
					0.37 70.1102981567383
					0.4 80.9247589111328
				};\addlegendentry{ERM}
				\addplot [very thick, blue,mark=square*]
				table { 
					0.1 8.90076446533203
					0.13 9.91339015960693
					0.16 14.3634700775146
					0.19 20.5581798553467
					0.22 29.8026847839355
					0.25 34.1425895690918
					0.28 39.9472427368164
					0.31 51.5498962402344
					0.34 51.7053298950195
					0.37 58.5951766967773
					0.4 76.0169296264648
				};\addlegendentry{CRR}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
			{\pgfplotslegendfromname{DrugLegend}};
		\end{tikzpicture}
		\caption{\texttt{Drug Discovery} Dataset with Normal Noise and Structured Noise}
		\label{fig:drug-discovery}
	\end{figure}

	\begin{table}[!h]
		\centering
		\begin{tabular}{lcccc}
			\toprule 
			\textbf{Objectives}&\multicolumn{4}{c}{Test RMSE (\texttt{Drug Discovery})}\\                   
			\cmidrule(rl){2-5}
			&\subhead{$\eps = 0.1$}& \subhead{$\eps = 0.2$}& \subhead{$\eps = 0.3$}& \subhead{$\eps = 0.4$}\\ 
			\midrule
			ERM  &$1.303_{(0.0665)}$&$1.790_{(0.0849)}$&$2.198_{(0.0645)}$&$2.623_{(0.1010)}$\\
			CRR \cite{bhatia2017}  &$1.079_{(0.0899)}$&$1.125_{(0.0832)}$&$1.385_{(0.1372)}$&$1.725_{(0.1136)}$\\
			STIR \cite{pmlr-v89-mukhoty19a} &$1.087_{(0.1256)}$&$1.167_{(0.0750)}$&$1.403_{(0.0987)}$&$1.668_{(0.1142)}$\\
			Robust Risk \cite{RRM} &$1.176_{(0.1110)}
			$&$1.336_{(0.1882)}$&$1.437_{(0.1723)}$&$1.800_{(0.0820)}$\\
			SMART \cite{https://doi.org/10.48550/arxiv.2206.04777} &$1.094_{(0.1065)}$&$1.323_{(0.0758)}$&$1.578_{(0.0799)}$&$1.984_{(0.2020)}$\\
			TERM \cite{li2020tilted} &$\mathbf{1.029_{(0.0707)}}$&$1.126_{(0.0776)}$&$1.191_{(0.1091)}$&$1.201_{(0.1409)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$1.111_{(0.0924)}$&$\mathbf{1.067_{(0.0457)}}$&$\mathbf{1.071_{(0.0807)}}$&$\mathbf{1.138_{(0.1162)}}$\\
			Huber \cite{Huber2009} &$1.412_{(0.0474)}$&$1.501_{(0.2918)}$&$2.231_{(0.9054)}$&$2.247_{(1.0399)}$\\
			RANSAC \cite{RANSAC1981} &$1.238_{(0.0529)}$&$1.643_{(0.1331)}$&$2.092_{(0.1935)}$&$2.679_{(0.1365)}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 1-\eps$) &$\mathbf{0.887_{(0.1046)}}$&$\mathbf{0.976_{(0.0892)}}$&$\mathbf{0.933_{(0.1137)}}$&$\mathbf{1.028_{(0.1142)}}$\\
			\midrule 
			Genie ERM &$0.986_{(0.1039)}$&$0.955_{(0.0698)}$&$1.038_{(0.0886)}$&$1.030_{(0.0578)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Drug Discovery} Dataset. Empirical Risk over $P$ with oblivious noise}
		\label{tab:drug-discovery}
	\end{table}
	
	As we can see in Table \ref{tab:drug-discovery}, we obtain state of the art results throughout all noise regimes. This makes our model the strongest among the tested, due to our strength throughout the whole range of noises. This dataset is also 
	
	\section{Conclusion}
	In this work we provide a theoretical analysis for robust linear regression by minimizing the \textit{Sub-Quantile} of the Empirical Risk. Furthermore, we run various numerical experiments and compare against the current State of the Art in Robust Linear Regression. Since minimizing over the subquantile is a general machine learning framework, it is scalable to larger scale machine learning problems. In future work, more real world applications can be explored and the theory can be expanded beyond linear regression. 
	
	\newpage

	\bibliographystyle{iclr2023_conference}
	\bibliography{iclr2023_conference}
	
	\begin{appendices}
		
	\newpage
	\appendix
	\startcontents[sections]
	\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
	\newpage
	
	\newgeometry{left=2cm,right=2cm,bottom=2cm}
	
	\section{Linear Algebra and Probability Theory Preliminaries}
	
	\begin{fact} The spectral norm of a matrix, $A$, an $(m \times n)$ matrix, is defined as follows
		\begin{equation}
			\norm{A}_2 = \sqrt{\lambda_{\max}\left(A^T A\right)} = \sigma_{\max}(A)
		\end{equation}
		It similarly follows:
		\begin{equation}
			\norm{A^T A}_2 = \norm{A}^2_2
		\end{equation}
	\end{fact}

	\begin{fact}
		\textbf{Weyl's Inequality} states the following:\\ If $M$, $N$, and $R$ are $n \times n$ Hermitian Matrices with the following eigenvalues where $M = N + R$:
		\begin{equation*}
			M: \mu_1 \geq \cdots \geq \mu_n
		\end{equation*}
		\begin{equation*}
			N: \nu_1 \geq \cdots \geq \nu_n
		\end{equation*}
		\begin{equation*}
			R: \rho_1 \geq \cdots \geq \rho_n
		\end{equation*}
		Then the following equalities hold:
		\begin{equation*}
			\nu_i + \rho_n \leq \mu_i \leq \nu_i + \rho_1 \text { for } i = 1,\dots,n
		\end{equation*}
	\end{fact}

	\begin{fact}
		Let $A$ be a $n \times m$ matrix with $n \gg m$. It then follows:
		\begin{equation}
			A^T A = \left(U \Sigma V^T\right)^T \left(U \Sigma V^T \right) = \left(V\Sigma^T U^T\right)\left(U \Sigma V^T\right) = V \Sigma^T \Sigma V^T = V D V^T
		\end{equation}
		where $D = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2 & & & \\ & \sigma_2^2 & & \\ & & \ddots & \\ & & & \sigma_m^2 \end{pmatrix}$
	\end{fact}

	\begin{fact}
		This is a restatement from \cite{Wackerly2008}.\\
		Let $X_1,\cdots, X_n$ be i.i.d continuous random variables with common distribution function $F(y)$ and common probability density function $f(x)$. If $X_{(k)}$ denotes the $k$th-order statistic, then the density function of $X_{(k)}$ is given by:
		\begin{equation}
			g_{(k)}(x_k) = \frac{n!}{(k-1)!(n-k)!}[F(x_k)]^{k-1}[1 - F(x_k)]^{n-k}f(x_k)
		\end{equation}
	\end{fact}

	\begin{fact}
		The cdf of the $k$th order statistic from a sample of $n$ is:
		\begin{equation}
			F_{(k,n)} = \mathbb{P}\left[X_{(k)}\leq x \right] = \sum_{j=k}^n \begin{pmatrix} n \\ j\end{pmatrix} \left(1 - F(x)\right)^{n-j} F(x)^j
		\end{equation}
	\end{fact}

	\newpage
	
	\section{Theory for Ridge Regression Algorithm \ref{alg:sqo-ridge}}\label{app:sub-quantile-optimization}
	\subsection{Proof of Theorem~\ref{thm:expected-value-corrupted}}\label{app:expected-value-corrupted}
	\begin{proof}
	\begin{assumption}\label{asm:normal-sampling}
		The rows of $P$ and $Q$ are sampled from $\vzero$ centered Normal Distributions.
		\begin{equation*}
			P_i \sim \mathcal{N}\left(\vzero, \Sigma_P\right)
		\end{equation*}
		\begin{equation}
			Q_i \sim \mathcal{N}\left(\vzero, \Sigma_Q\right)
		\end{equation}
	\end{assumption}
	
	\begin{assumption}\label{asm:wishart}
		By assumption \ref{asm:normal-sampling}, it thus follows that the matrices $P^T P$ and $Q^T Q$ are sampled from Wishart Distributions.
		\begin{equation}
			P^T P \sim \mathcal{W}\left(n,\Sigma_P\right)
		\end{equation}
		\begin{equation}
			Q^T Q \sim \mathcal{W}\left(n, \Sigma_Q\right)
		\end{equation}
	\end{assumption}

	\begin{assumption}\label{asm:identity-covariance}
		Similar to the assumption made in \cite{bhatia2017}, to give theoretical bounds on the our algorithm, we assume the following:
		\begin{equation}
			\Sigma_P = \xi_P I
		\end{equation}
		\begin{equation} 
			\Sigma_Q = \xi_Q I
		\end{equation}
		where $\xi_P,\xi_Q \geq 0$ 
	\end{assumption}
	The closed form solution for Ridge Regression with regularization parameter $\lambda$ is equal to the following:
	\begin{equation}
		\hat{\vbeta} = \left(X^T X+ \lambda I\right)^{-1}X^T y
	\end{equation}
	We will use this to bound the difference of the $\vbeta_P$ and $\vtheta_{(k)}$.
	\begin{align*}
		\norm{\vbeta_P - \vtheta_{(k)}}_2 &= \norm{\vbeta_P - \left(S^T S + \lambda I\right)^{-1}S^T \vy}_2 &&\\
	\end{align*}
	Note the subquantile matrix $S$, consists of data points from $P$ and $Q$, we will reorganize $S$ into the following:
	\begin{equation*}
		S = \begin{pmatrix}P \\ Q \end{pmatrix} = 
		\begin{pmatrix}
			\leftarrow & \vp_1 & \rightarrow \\
			\vdots & \vdots & \vdots \\
			\leftarrow & \vp_{n(1-\varepsilon^{(t)})} & \rightarrow\\
			\leftarrow & \vq_1 & \rightarrow \\
			\vdots & \vdots & \vdots \\
			\leftarrow & \vq_{n\varepsilon^{(t)}} & \rightarrow
		\end{pmatrix}\text{ and } \vy = \begin{pmatrix}\vy_P \\ \vy_Q \end{pmatrix} = \begin{pmatrix} \vbeta_P\vp_1 + \eps_P \\ \vdots \\ \vbeta_P\vp_{(n(1-\varepsilon^{(t)}))} + \eps_P \\ \vbeta_Q\vq_1 + \eps_Q\\ \vdots \\ \vbeta_Q\vq_{n\varepsilon^{(t)}} + \eps_Q\end{pmatrix}
	\end{equation*}
	Let us also assume $\Var(\eps_P) = \eta_P$ and $\Var(\eps_Q) = \eta_Q$. Then we can make the following manipulations:
	\begin{align*}
		= &\norm{\vbeta_P - \left(P^T P + Q^T Q + \lambda I\right)^{-1}\begin{pmatrix}P^T Q^T\end{pmatrix}\begin{pmatrix}\vy_P \\ \vy_Q \end{pmatrix}}_2 &&\\
		= &\norm{\left(P^T P + \lambda I\right)^{-1} P^T \vy_P - \left(P^T P + Q^T Q + \lambda I\right)^{-1}P^T \vy_P  - \left(P^T P + Q^T Q + \lambda I\right)^{-1}Q^T \vy_Q}_2 &&\\
		\leq &\norm{\left(P^T P + \lambda I\right)^{-1} P^T \vy_P -\left(P^T P + Q^T Q+ \lambda I\right)^{-1}P^T \vy_P}_2 + \norm{\left(P^T P + Q^T Q+ \lambda I\right)^{-1}Q^T \vy_Q}_2 &&\\
		\leq& \norm{\left(P^T P + \lambda I\right)^{-1}}_2 \norm{P^T\vy_P}_2 + \norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{P^T \vy_P}_2 + \norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{Q^T \vy_Q}_2&&\\
		\leq & \sqrt{\lambda_{\max}\left(P^T P\right)}\left(\norm{\left(P^TP + \lambda I\right)^{-1}}_2+\norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\right)\norm{\vy_P}_2 + \sqrt{\lambda_{\max}\left(Q^T Q\right)}\norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{\vy_Q}_2 &&\\
		= &\frac{\sigma_{\max}(P)\norm{\vy_P}}{\sqrt{\lambda_{\max}(P^TP + \lambda I)}} + \frac{\sigma_{\max}(P)\norm{\vy_P}_2}{\sqrt{\lambda_{\max}\left(P^T P + Q^T Q + \lambda I\right)}} + \frac{\sigma_{\max}(Q)\norm{\vy_Q}_2}{\sqrt{\lambda_{\max}\left(P^T P + Q^T Q + \lambda I\right)}} &&\\
		\overset{\mathrm{(a)}}{\leq} &\frac{2\sigma_{\max}(P)\norm{P\vbeta_P + \veps_P}_2 + \sigma_{\max}{Q}\norm{Q\vbeta_Q + \veps_Q}_2}{\sqrt{\lambda_{\max}\left(P^T P + \lambda I\right)}} &&\\
		\overset{\mathrm{(b)}}{\leq} & \frac{2\sigma^2_{\max}(P)\norm{\vbeta_P} + 4\sigma_{\max}(P)n(1-\varepsilon^{(t)})\eta_P + 2\sigma^2_{\max}(Q)\norm{\vbeta_Q} + 4\sigma_{\max}(Q)n\varepsilon^{(t)}\eta_Q}{\sqrt{\lambda_{\max}(P^T P) + \lambda_{\min}(\lambda I)}} &&\\
		\leq &\frac{2\sigma^2_{\max}(P)\norm{\vbeta_P} + 4\sigma_{\max}(P)n(1-\varepsilon^{(t)})\eta_P + 2\sigma^2_{\max}(Q)\norm{\vbeta_Q} + 4\sigma_{\max}(Q)n\varepsilon^{(t)}\eta_Q}{\sqrt{\sigma^2_{\max}(P) + \lambda}} &&\\
	\end{align*}
	
	(a) is due to $Q^TQ$ being a positive semi definite symmetric matrix. \\ 
	(b) holds with probability $\left(1 - (\Phi(2) - \Phi(-2))\right)^{n(1-\varepsilon^{(t)})}$
	
	Thus we have shown that $\norm{\vbeta_P - \vtheta_{(t)}}_2$ is bounded above at any time-step $(t)$. 
	
	\end{proof}

	\newpage
	
	\section{Proofs for $\varepsilon^{(t)}$}
	Let us note the Subquantile Matrix $\displaystyle S = \begin{pmatrix} P \\ Q \end{pmatrix} = \begin{pmatrix} \leftarrow & \vp_1 & \rightarrow \\ \vdots & \vdots & \vdots \\ \leftarrow & \vp_{\eta_P} & \rightarrow \\ \leftarrow & \vq_1 & \rightarrow \\ \vdots & \vdots & \vdots \\ \leftarrow & \vq_{\eta_Q} & \rightarrow\end{pmatrix}$. Thus we can define: $\displaystyle \varepsilon^{(t)} = \frac{\eta_Q}{\eta_P + \eta_Q} = \frac{\eta_Q}{np}$. Where $n$ is the number of training examples and $p$ is the Subquantile we are minimizing over. In this section, we want to provide a theoretical upper bound on $\eta_Q$, from where we can upper bound $\varepsilon^{(t)}$ which is stronger than the trivial upper bound of $\displaystyle \frac{\epsilon}{p}$. We will approach this problem with Order Statistics.
	\begin{assumption}
		The optimal regressors are linearly independent, i.e. $\vbeta_P \neq \gamma \vbeta_Q\, \forall \gamma \in \mathbb{R}$
	\end{assumption}
	Let $\displaystyle \vtheta_{(t)} = \alpha_1 \vbeta_P + \alpha_2 \vbeta_Q + \sum_{i=3}^{d}\vr_i$.
	
	Let us denote $P_1 < P_2 < \dots < P_{n(1 - \epsilon)}$ as the order statistics of the random variable $P \sim \left(\vp_i \vtheta - (\vp_i \vbeta_P + \epsilon_P)\right)^2$ where $ \vp_i \sim \mathcal{N}_d(\vzero, \xi_P I)$. Let us also denote $Q_1 < Q_2 < \dots < Q_{n(1 - \epsilon)}$ as the order statistics of the random variable $Q \sim \left(\vq_i \vtheta - (\vp_i \vbeta_Q + \epsilon_Q)\right)^2$ where $ \vq_i \sim \mathcal{N}_d(\vzero, \xi_P I)$.\\
	First we will formalize the CDF of $P$ and $Q$. Note $\vp_i$ and $\vq_i$ represent the normally sampled gaussian data. 
	\begin{align}
		P_i = &\left(\vp_i\vtheta_{(t)} - (\vp_i \vbeta_P + \eps_P)\right)^2 &&\\
		 = &\left(\vp_i(\vtheta_{(t)} - \vbeta_P) - \eps_P\right)^2 &&\\
		= &\left(\vp_i(\vtheta_{(t)} - \vbeta_P)\right)^2 - 2 \eps_P \left(\vp_i(\vtheta_{(t)} - \vbeta_P)\right) + \eps_P^2 &&
	\end{align}
	As $\mathbb{E}\left[\epsilon_P\right] = 0$, we will only consider the case $\epsilon_P = 0$. This simplifies $P_i$ and $Q_i$:
	\begin{equation}
		P_i = \left(\mP_i(\vtheta_{(t)} - \vbeta_P)\right)^2 
	\end{equation}
	\begin{equation}
		Q_i = \left(\mQ_i(\vtheta_{(t)} - \vbeta_Q)\right)^2
	\end{equation}
	Let us note all the entries of $\mP_i$ are sampled from $\mathcal{N}(0,\xi_P)$ and all entries of $\mQ_i$ are sampled from $\mathcal{N}(0,\xi_Q)$.
	It thus follows:
	\begin{align*}
		\left(\mP_i(\vtheta_{(t)} - \vbeta_P)\right)^2 &= \left(\sum_{j=1}^d\mathcal{N}(0,\xi_P)\left(\theta^{(t)}_j - \beta_{Pj}\right)\right)^2 &&\\
		&= \left(\sum_{j=1}^d \mathcal{N}\left(0,\xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2\right)\right)^2  &&\\
		&= \left(\mathcal{N}\left(0, \sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2\right)\right)^2
	\end{align*}
	It thus similarly follows for $Q_i$:
	\begin{equation*}
		\left(\mQ_i(\vtheta_{(t)} - \vbeta_Q)\right)^2 = \left(\mathcal{N}\left(0, \sum_{j=1}^d \xi_Q\left(\theta_j^{(t)} - \beta_{Qj}\right)^2\right)\right)^2
	\end{equation*}
	Now we can define the cumulative distribution functions.
	\begin{align}
		F_P(z) &= \frac{1}{\sqrt{2\pi}}\int_{-\sqrt{z}}^{\sqrt{z}}\exp\left(-\frac{u^2}{2\sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2}\right)du &&\\
		&= \Phi\left(\frac{\sqrt{z}}{\sqrt{\sum_{j=1}^d \xi_P\left(\theta_j^{(t)}-\beta_{Pj}\right)^2}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\sum_{j=1}^d \xi_Q\left(\theta_j^{(t)}-\beta_{Pj}\right)^2}}\right) &&
	\end{align}
	Let us define $\displaystyle \phi = \sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2$ and $\displaystyle \psi = \sum_{j=1}^d \xi_Q\left(\theta_j^{(t)} - \beta_{Qj}\right)^2$
	Therefore it follows:
	\begin{equation}
		F_P(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\phi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\phi}}\right)
	\end{equation}
	Similarly for $F_Q(z)$ it follows:
	\begin{equation}
		F_Q(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\psi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\psi}}\right)
	\end{equation}
	Here we can note if $\phi < \psi$, then for all $z > 0$, it follows $F_P(z) > F_Q(z)$.
	
	We now want to find the max integer, $\eta$ such that with probability greater than $0.9$, $Q_{\eta} < P_{np - \eta}$ and for all $m \in \mathbb{N}$ such that $m > \eta$, $Q_{m} > P_{np - \eta}$. This is equivalent to the probability there will be $\eta$ elements from $Q$ within the Subquantile matrix.

	\begin{align*}
		\mathbb{P}\left[Q_\eta < P_{np - \eta} \bigcap_{i=1}^{n\epsilon - \eta} Q_{\eta+ i} > P_{np - \eta}\right] &= \mathbb{P}\left[Q_\eta < P_{np - \eta}\right] \prod_{i=1}^{n\epsilon - \eta} \mathbb{P}\left[Q_{n+i} > P_{np - \eta}\right] &&\\
		&= \mathbb{P}\left[Q_n < P_{np - \eta}\right]\prod_{i=1}^{n\epsilon - \eta}\left(1 - \mathbb{P}\left[Q_{n+i} < P_{np - \eta}\right]\right) &&\\
		&= \mathbb{P}\left[Q_n - P_{np-\eta} < 0\right]\prod_{i=1}^{n\epsilon - \eta} \left(1 - \mathbb{P}\left[Q_{n+i} - P_{np-\eta} < 0\right]\right) &&\\
	\end{align*}
	It follows $\eta$ will be lower depending on the difference of $\phi$ and $\psi$. In a further work we will give a tight upper bound on the probability.

	
	\newpage
	\section{Theory for Subquantile Minimization}\label{app:general-proofs}
	\subsection{Derivation of Lemma~\ref{lem:gfermat}}
	\label{app:gfermat}
	Since $\displaystyle g(t,\vtheta)$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
	Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T X - y)^2\right)$ and note $\displaystyle 0 < p < 1$
	\begin{align}
		\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) &= \partial{\left(-t + \frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)} &&\\
		&= \partial{(-t)} + \partial{\left(\frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)}&&\\
		&= -1 + \frac{1}{np}\sum_{i=1}^{n}\partial{(t-\hat{\vnu}_i)^+} &&\\
		&= -1 + \frac{1}{np}\sum_{i=1}^{n}
		\left\{
		\begin{array}{lr}
			1, & \text{if } t > \hat{\vnu}_i\\
			0, & \text{if } t < \hat{\vnu}_i \\
			\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
		\end{array}
		\right\}&&\\
		&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
	\end{align}
	This is the $p$-quantile of $\displaystyle \vnu$. Assuming no two points are equal in the dataset, this means the minimizing value for $t$ has a range of values, $\hat{\vnu}_{np} \leq t < \hat{\vnu}_{np+1}$. This means $g(t,\vtheta)$ is not strongly convex with respect to $t$. 
	\subsection{Derivation of Lemma~\ref{lem:gthetaderiv}}
	\label{app:gthetaderiv}
		Note that $t_k = \vnu_{np}$ which is equivalent to $(\vtheta_k^T\vx_{np} - y_{np})^2$
		\begin{align*}
			\nabla_{\vtheta_k} g(t_{k+1},\vtheta_k) &= \nabla_{\vtheta_k}\left(\vnu_{np} - \frac{1}{np}\sum_{i=1}^n(\vnu_{np} - (\vtheta_k^T\vx_i - y_i)^2)^+\right) &&\\
			&= \nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+\right) &&\\
			&= \nabla_{\vtheta_k}(\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+ &&\\				
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{n}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) \notag\\ &\qquad-2\vx_i(\vtheta_k^T\vx_i - y_i) \left\{
			\begin{array}{lr}
				1, & \text{if } t > v_i\\
				0, & \text{if } t < v_i \\
				\left[0,1\right], & \text{if } t = v_i \\
			\end{array} \right\} &&\\
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{np}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\	
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) - 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) + \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)&&
		\end{align*}
		This is the derivative of the $np$ samples with lowest error with respect to $\vtheta$.

	\subsection{Derivation of Lemma~\ref{lem:g-lsmooth}}
	The objective function 
	$\displaystyle g(\vtheta,t)$ is $L$-smooth w.r.t $\vtheta$ iff
	\begin{equation}
		||  \nabla_\vtheta g(\vtheta',t) - \nabla_\vtheta g(\vtheta,t) || \leq L|| \vtheta' - \vtheta || 
	\end{equation}
	\begin{align}
		\norm{ \nabla_{\vtheta} g(\vtheta^{'},t) - \nabla_{\vtheta} g(\vtheta,t) } = &\norm{ \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'\top} \vx_i - y_i) - \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T \vx_i - y_i) } &&\\
		= &\norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'\top}\vx_i - \vtheta_k^T\vx_i)}&&\\
		= &\norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i\vx_i^T(\vtheta_k^{'\top} - \vtheta_k^T)} &&\\
		\overset{\mathrm{Cauchy-Schwarz}}{\leq} &\norm{\frac{2}{np}\sum_{i=1}^{np}\vx_i\vx_i^T}\norm{\vtheta_k^{'\top} - \vtheta_k^T} &&\\
		= &L\norm{\vtheta_k^{'\top} - \vtheta_k^T} &&
	\end{align}
	where $\displaystyle L = \norm{\frac{2}{np}X^T X}$\\
	This concludes the derivation.
	\subsection{Proof of Lemma~\ref{lem:g-change-wrt-t}}
	\label{app:g-change-wrt-t}
	\begin{proof}
		We will investigate the two cases $t_{k+1} \leq t$ and $t_{k+1} > t_k$.\\
		\textbf{Case (i)} $t_{k+1} \leq t_k$\\
		Let us first expand out $g(t_k,\vtheta_k)$ with the knowledge that $t_k \geq \hat{\vnu_{k}}$
		\begin{align}
			g(t_k, \vtheta_k) &= t_k - \frac{1}{np}\sum_{i=1}^{n}(t_k-\vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}(np)t_k + \frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&\\
			g(t_{k+1},\vtheta_k) - g(t_k,\vtheta_k) &=  \frac{1}{np}\sum_{i=1}^{np}\vnu_i - \left(\frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+\right) &&\\
			&= - \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&
		\end{align}
		\textbf{Case (ii)} $t_{k+1} > t_k$\\
		Since we know $t_k$ is less than $\vnu_{np}$, WLOG we will say $t_k$ is greater than the lowest $n(p-\delta)$ elements, where $\delta \in (0,p)$. 
		\begin{align}
			g(t_k,\vtheta_k) &= t_k - \frac{1}{np}\sum_{i=1}^n(t_k - \vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}\sum_{i=1}^{n(p-\delta)}(t_k - \vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}(n(p-\delta))t_k + \frac{1}{np}\sum_{i=1}^{n(p-\delta)}\vnu_i &&\\
			g(t_k,\vtheta_{k+1}) - g(t_k,\vtheta_k) &= \frac{1}{np}\sum_{i=1}^{np}\vnu_i - \left(\delta t_k + \frac{1}{np}\sum_{i=1}^{n(p-\delta)}\vnu_i\right) &&\\
			&= \left(\frac{1}{np}\sum_{i=n(p-\delta)}^n\vnu_i\right) - \delta t_k &&
		\end{align}
		This concludes the proof.
	\end{proof}
  	
  	\newpage
	\section{Theory for Adaptive Linear Corruption}
	In this section, we provide rigorous theory for why Sub-Quantile Minimization works so well in the case of corruption of the form $\displaystyle \vbeta_Q^T \vmu = y_p + \eps_Q$.
	\begin{assumption}\label{asm:normal-error}
		The residuals of $\vtheta_k$ are normally distributed with respect to $\mathbb{P}$ and $\mathbb{Q}$. In other words, $\vtheta_k^T \vp - \vy_P$ and $\vtheta_k^T \vq - \vy_Q$ are normally distributed.
	\end{assumption}
	Assumption \ref{asm:normal-error} can be visually verified in figure \ref{fig:normal-residual}. Even after multiple iteration steps the residuals with respect to $\mathbb{P}$ and $\mathbb{Q}$ are still normal. Thus it follows by decreasing $\norm{\vtheta - \vbeta_P}_1$ more relative to $\norm{\vtheta - \vbeta_Q}_1$ then the SubQuantile will contain more points from $P$ by expectation. 
	\begin{figure}
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual0.tikz}
			%\caption{Caption for first figure}
			\label{fig:1}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual2.tikz}
			%\caption{Caption for second figure}
			\label{fig:2}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual4.tikz}
			%\caption{Caption for third figure}
			\label{fig:3}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual6.tikz}
			%\caption{Caption for fourth figure}
			\label{fig:4}
		\end{minipage}
		\caption{Residuals with respect to $\mathbb{P}$ and $\mathbb{Q}$, $k$ represents optimization step.}
		\label{fig:normal-residual}
	\end{figure}

	\subsection{Proof of Theorem~\ref{thm:improvement-of-theta}}
	\label{app:improvement-of-theta}
	\begin{proof}
		To show the change in $\varepsilon$ we will first calculate the expected change in $\vtheta$ by the $\vtheta$-update described in Equation \ref{eqn:theta-update}. We will also introduce some notation, $\mathcal{S}$ represents all $\vx \in X$ that are within the lowest $np$ losses, i.e. within the subquantile, $\vp \in \mathcal{S}$ represent all data vectors from $\mathbb{P}$ that are within the SubQuantile, similarly $\vq \in \mathbb{Q}$ represent all data vectors from $\mathbb{Q}$ that are within the SubQuantile. Furthermore, $|\mathcal{S}| = np$, there are $\varepsilon n$ points from $\mathbb{Q}$ in $\mathcal{S}$ and $(1-\varepsilon)n$ points from $\mathbb{P}$ within $\mathcal{S}$. 
		\begingroup
		%\addtolength{\jot}{0.5em}
		\begin{align*}
			\mathbb{E}\left[\vtheta_{k+1}\right] &= \vtheta_k - \mathbb{E}\left[\alpha \nabla g(\vtheta_k,t_{k+1})\right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vx \in \mathcal{S}}\vx(\vtheta^T\vx - y) \right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vx \in \mathcal{S}}\vx\vx^T\vtheta_k - \vx y \right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vp \in \mathcal{S}}\vp\vp^T\vtheta_k - \vp y_p + \sum_{\vq \in \mathcal{S}} \vq\vq^T\vtheta_k - \vq y_q \right] &&
			\intertext{We will use Assumption \ref{asm:normal-corruption} to rewrite $y_p$ and $y_q$}
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vp \in \mathcal{S}}\vp\vp^T\vtheta_k - \vp (\vbeta_P \vp + \eps_P) + \sum_{\vq \in \mathcal{S}} \vq\vq^T\vtheta_k - \vq (\vbeta_Q \vp + \eps_Q) \right] &&\\
			&= \vtheta_k - \alpha\left(\sum_{\vp \in \mathcal{S}} \left(\vmu \vmu^T + \Sigma\right)\vtheta_k - \left(\vmu \vmu^T + \Sigma\right)\vbeta_P - \sum_{\vq \in \mathcal{S}} \left(\vmu \vmu^T + \Sigma\right)\vtheta_k + \left(\vmu \vmu^T + \Sigma\right)\vbeta_Q\right) &&
		\end{align*}
		 \vspace{1em}
		\begin{equation}\vspace{1em}
			\mathbb{E}\left[\vtheta_{(t+1)}\right]= \vtheta_{(t)} - \alpha np \mC\left(\vtheta_{(t)} - (1-\varepsilon)\vbeta_P - \varepsilon\vbeta_Q\right) 
		\end{equation}
		
		\endgroup
		Now that we have the expected update for $\vtheta$ in terms of the linear regression coefficients, we now want to utilize Lemma \ref{lem:effect-of-projection}.\\
		Let $\displaystyle \vtheta_k = \alpha_1\vbeta_P + \alpha_2\vbeta_Q + \sum_{i=3}^{d}\alpha_i \vr_i$ in the same basis $\mB$ defined in Lemma \ref{lem:effect-of-projection}. Note in the case of data that is normally distributed about $0$ with no covariance amongst the predictor variables, then $\mC$ is a multiple of the identity. If we proprocess the data using a standard normal scalar such that all features follow a $\mathcal{N}(0,1)$ distribution, then we can assume it follows $\displaystyle \mC = (\vzero\vzero^T + I) = I$. Let $\gamma \coloneqq \alpha np$. Then the following manipulations hold:
		\begin{align*}
			\mathbb{E}\left[\vtheta_{(t+1)}\right] &= \vtheta_{(t)} - \gamma\left(\vtheta_{(t)} - (1-\varepsilon^{(t)})\vbeta_P - \varepsilon^{(t)} \vbeta_Q\right) &&\\
			&= \vtheta_{(t)}(1-\gamma) + \gamma (1-\varepsilon^{(t)})\vbeta_P + (1-\gamma)\varepsilon^{(t)}\vbeta_Q &&\\
			&= (1-\gamma)\left(\alpha_1 \vbeta_P + \alpha_2^{(t)} \vbeta_Q + \sum_{i=3}^d\alpha_i^{(t)}\vr_i\right) + \gamma(1-\varepsilon^{(t)})\vbeta_P + \gamma\varepsilon^{(t)}\vbeta_Q &&\\
			&= \left(\alpha_1^{(t)}(1-\gamma) + \gamma(1-\varepsilon^{(t)})\right)\vbeta_P + \left(\alpha_2^{(t)}(1-\gamma) + \gamma \varepsilon^{(t)}\right)\vbeta_Q + (1-\gamma)\sum_{i=3}^d\alpha_i^{(t)}\vr_i &&
		\end{align*}
		We will now calculate the difference. 
		\begin{align*}
			\mathbb{E}\left[\vtheta_{(t+1)} - \vtheta_{(t)}\right] &= \left(-\gamma\alpha_1^{(t)} + \gamma(1-\varepsilon^{(t)})\right)\vbeta_P + \left(-\gamma \alpha_2^{(t)} + \gamma \varepsilon^{(t)}\right)\vbeta_Q + (1-\gamma)\sum_{i=3}^d\alpha_i^{(t)}\vr_i &&\\
			&= \gamma\left((1-\varepsilon^{(t)}) - \alpha_1^{(t)}\right)\vbeta_P + \gamma\left(\varepsilon^{(t)} - \alpha_2^{(t)}\right)\vbeta_Q + (1-\gamma)\sum_{i=3}^d\alpha_i^{(t)}\vr_i &&
		\end{align*}
		Thus the conditions for $\varepsilon$ to decrease by expectation are:
		\begin{equation}
			\norm{\left((1-\varepsilon^{(t)}) - \alpha_1^{(t)}\right)\vbeta_P} > \norm{\left(\varepsilon^{(t)} - \alpha_2^{(t)}\right)\vbeta_Q}
		\end{equation}  
		This concludes the proof. 
	\end{proof}

	\subsection{Proof of Theorem ~\ref{thm:probability-epsilon-decreasing}}
	\label{app:decrease-of-epsilon}
	\begin{proof}
		To calculate the probability $\varepsilon^{(t+1)} < \varepsilon^{(t)}$, we will first calculate $\mathbb{E}\left[\varepsilon^{(t+1)}\right]$. We will start by calculating the expectation of the loss. 
		\begin{align*}
			\mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^2\right] &= \mathbb{E}\left[\vtheta_{(t+1)}^T\vp_i - y_i\right]^2 + \Var\left(\vtheta_{(t+1)}^T\vp_i - y_i\right) &&\\
			&= \mathbb{E}\left[\vtheta_{(t+1)}^T\vp_i - \vbeta_P^T\vp_i - \eps_P\right]^2 + \Var\left(\vtheta_{(t+1)}^T\vp_i - \vbeta_P^T\vp_i - \eps_P\right) &&\\
			&= \left(\left(\vtheta^T_{(t+1)} - \vbeta_P^T\right)\mathbb{E}\left[\vp_i\right]\right)^2 + \Var\left(\left(\vtheta^T_{(t+1)}-\vbeta_P^T\right)\vp_i\right) + \Var\left(\eps_P\right)	&&\\	
			&= \left(\left(\vtheta^T_{(t+1)}-\vbeta_P^T\right)\vmu\right)^2 + \left(\vtheta^T_{(t+1)} - \vbeta_P^T\right)\Sigma\left(\vtheta_{(t+1)} - \vbeta_P\right) + \Var\left(\eps_P\right)
		\end{align*}
		It thus follows similarly:
		\begin{equation*}
			\mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vq_i - y_i\right)^2\right] =  \left(\left(\vtheta^T_{(t+1)}-\vbeta_Q^T\right)\vmu\right)^2 + \left(\vtheta^T_{(t+1)} - \vbeta_Q^T\right)\Sigma\left(\vtheta_{(t+1)} - \vbeta_Q\right) + \Var\left(\eps_Q\right)
		\end{equation*}
		To simplify our notation, let $\zeta_P \coloneqq \mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^2\right]$ and $\zeta_Q \coloneqq \mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vq_i - y_i\right)^2\right]$ and let $\eta_P \coloneqq \mathbb{E}\left[\vtheta_{(t+1)}^T\vp_i - y_i\right]$ and $\eta_Q \coloneqq \mathbb{E}\left[\vtheta_{(t+1)}^T\vq_i - y_i\right]$
		We are now interested in calculating the variance of the loss. \\
		Let $\sigma^2_P \coloneqq \Var\left(\vtheta^T_{(t+1)}\vp_i - y_i\right) = \left(\vtheta^T_{(t+1)} - \vbeta_P^T\right)\Sigma\left(\vtheta_{(t+1)} - \vbeta_P\right) + \Var\left(\eps_P\right)$. 
		\begin{align*}
			\Var\left(\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^2\right)&= \mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^4\right] - \mathbb{E}\left[\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^2\right]^2 &&\\
			&= \mathbb{E}\left[\vtheta_{(t+1)}^T\vp_i - y_i\right]^4 + 6 \mathbb{E}\left[\vtheta_{(t+1)}^T\vp_i - y_i\right]^2\sigma_P^2 + 3\sigma_P^4 - \zeta_P^2&&\\
			&= \eta_P^4 + 6 \eta_P^2 \sigma_P^2 + 3\sigma_P^4 - \zeta_P^2 &&
		\end{align*}
		It similarly follows:
		\begin{align*}
			\Var\left(\left(\vtheta_{(t+1)}^T\vq_i - y_i\right)^2\right)&=\eta_Q^4 + 6 \eta_Q^2 \sigma_Q^2 + 3\sigma_Q^4 - \zeta_Q^2 &&
		\end{align*}
		Recall that the expected value of $\varepsilon^{(t+1)}$ is equal to the expected number of elements from $Q$ within the $p$-Quantile, $\mathcal{Q}_p$. So we will now calculate $\mathcal{Q}_p$. To this we will utilize the  loss of the distribution. 
		\begin{align*}
			\mathbb{E}\left[\left(\vtheta_{(t+1)}^T \vx_i - y_i\right)^2\right] &= (1-\varepsilon^{(t)})\mathbb{E}\left[\left(\vtheta_{(t+1)}^T \vp_i - y_i\right)^2\right] + \varepsilon^{(t)}\mathbb{E}\left[\left(\vtheta_{(t+1)}^T \vq_i - y_i\right)^2\right] &&\\
			&= (1-\varepsilon^{(t)})\zeta_P + \varepsilon^{(t)}\zeta_Q &&
		\end{align*}
		\begin{align*}
			\Var\left(\left(\vtheta_{(t+1)}^T\vx_i - y_i\right)^2\right) &= (1-\varepsilon^{(t)})^2\Var\left(\left(\vtheta_{(t+1)}^T\vp_i - y_i\right)^2\right) + \varepsilon^{(t)2}\Var\left(\left(\vtheta_{(t+1)}^T\vq_i - y_i\right)^2\right) &&\\
			&= (1-\varepsilon^{(t)})^2\left(\eta_P^4 + 6 \eta_P^2 \sigma_P^2 + 3\sigma_P^4 - \zeta_P^2\right) + (\varepsilon^{(t)})^2\left(\eta_Q^4 + 6 \eta_Q^2 \sigma_Q^2 + 3\sigma_Q^4 - \zeta_Q^2\right) &&
		\end{align*}
		It thus follows $\displaystyle \left(\vtheta_{(t+1)}^T\vq_i - y_i\right)$ follows a $\chi^2$ distribution with $1$ degree of freedom and \begin{equation*} \lambda = \frac{\zeta_Q}{\eta_Q^4 + 6 \eta_Q^2 \sigma_Q^2 + 3\sigma_Q^4 - \zeta_Q^2}\end{equation*}
		Let $\mathcal{Q}_p^{(t+1)}$ be given as the $np$ greatest loss of the data, i.e. $\mathcal{Q}_p^{(t+1)} = \vnu_{np}^{(t+1)}$. It then follows:
		\begin{equation*}
			\mathbb{P}\left[\left(\vtheta^T_{(t+1)}\vq_i -y_i\right)^2 \leq \mathcal{Q}_p\right] = 1 - Q_{1}\left(\sqrt{\lambda},\sqrt{\mathcal{Q}_p}\right)
		\end{equation*}
		It thus follows:
		\begin{equation*}
			\mathbb{E}\left[\varepsilon^{(t+1)}\right] = 1 - Q_{1}\left(\sqrt{\lambda},\sqrt{\mathcal{Q}_p}\right)
		\end{equation*}
		Now we can calculate the probability of improvement per iteration by invoking Markov's Inequality:
		\begin{align*}
			\mathbb{P}\left[\varepsilon^{(t+1)} \geq \varepsilon^{(t)}\right] &\leq \frac{\mathbb{E}\left[\varepsilon^{(t+1)}\right]}{\varepsilon^{(t)}} &&\\
			&= \frac{1 - Q_{1}\left(\sqrt{\lambda},\sqrt{\mathcal{Q}_p}\right)}{\varepsilon^{(t)}}
		\end{align*}
		It thus follows:
		\begin{align*}
			\mathbb{P}\left[\varepsilon^{(t+1)} \leq \varepsilon^{(t)}\right] &\geq 1 - \frac{1 - Q_{1}\left(\sqrt{\lambda},\sqrt{\mathcal{Q}_p}\right)}{\varepsilon^{(t)}}
		\end{align*} 
	\end{proof}
	This concludes the proof.
	\newpage
	
	\section{Proofs for Convergence}
	\label{app:convergence-proofs}
	\subsection{Proof of Theorem ~\ref{thm:convergence-guarantee}}
	\begin{proof}
	We will first start by introducing new notation. Let $S$ represent a matrix with $np$ data points from $X$, in other words it is a possible SubQuantile Matrix. Let $\displaystyle\vPi$ represent the set of all such possible matrices $S$ of $X$. Note $\displaystyle \left|\vPi\right| = \binom{n}{np}$. We can now redefine the min-max optimization problem of $g$ to a min-min optimization problem. Let us define the function $f(\vtheta, S) = \norm{\vtheta^T S - y_S}_2^2$
	\begin{equation}
		\vtheta^*, S^* = \argmin_{\vtheta \in \mathbb{R}^d} \argmin_{S \in \vPi} \norm{\vtheta^T S - y_S}_2^2
	\end{equation}
	Note we have a $\mathcal{O}(n)$ oracle for the $\displaystyle \argmin_{S\in \vPi}f(\vtheta_T,S_T)$. 
	\begin{restatable}{lemma}{unique-S-minimizer}
		\label{lem:unique-S-minimizer}
		The resultant $\displaystyle \widetilde{S} = \argmin_{S \in \vPi} f(\vtheta,S)$ is a unique minimizer iff all points in $X$ are different. 
	\end{restatable}
	We will now show $f$ is a monotonically decreasing function.\\
	First let us define $\phi(\cdot) = \min_{S \in \vPi}\left(\cdot,S\right)$. 
	Let us also note $f$ is $\ell$ smooth with respect to $\vtheta$. This is following notation from \cite{Jin_2019}.It thus follows:
	\begin{align*}
		f(\vtheta_{k+1}, S_k) &\leq f(\vtheta_k, S_k) + \langle\nabla_\vtheta f(\vtheta_k,S_k), \vtheta_{k+1}-\vtheta_k \rangle + \frac{\ell}{2}\norm{\vtheta_{k+1}-\vtheta_k}_2^2 &&\\
		&= \phi(\vtheta_k) + \langle\nabla_\vtheta f(\vtheta_k,S_k), -\frac{1}{\ell}\nabla_\vtheta f(\vtheta_k,S_k)\rangle + \frac{\ell}{2}\norm{\frac{1}{\ell}\nabla_\vtheta f(\vtheta_k,S_k)}_2^2 &&\\
		&= \phi(\vtheta_k) - \frac{1}{\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 + \frac{1}{2\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 &&\\
		&= \phi(\vtheta_k) - \frac{1}{2\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 &&\\
	\end{align*} 
	Thus we have proved the inner optimization problem is monotonically decreasing. Since the outer minimization is strictly less than or equal to the result from the inner optimization, it follows after each two step optimization: 
	\begin{align*}
		\phi(\vtheta_{k+1}) \leq \phi(\vtheta_k)
	\end{align*}
	
	Since $f$ is lower bounded by $0$. We can invoke Montonicity Convergence Theorem, since $f$ is a monotically decreasing function and is lower bounded, it therefore converges to either a local or global minimum. 
	\end{proof}
	
	\subsection{Expectation of Improvement}
	We are also interested in expected improvement after the Subquantile matrix update. 
	\begin{align*}
		\mathbb{E}&\left[f\left(\vtheta_{(k)}, S^{(k+1)}\right) - f\left(\vtheta_{(k)}, S^{(k)}\right)\right] = \mathbb{E}\left[\varepsilon^{(t)} - \varepsilon^{(t-1)}\right]\mathbb{E}\left[\sum_{i=1}^{n(\varepsilon^{(t)}-\varepsilon^{(t-1)})}\norm{\vtheta_{(k)}^T\vp_i -y_i}_2^2 - \norm{\vtheta_{(k)}^T\vq_i - y_i}_2^2\right] &&\\
		&= \mathbb{E}\left[\varepsilon^{(t)} - \varepsilon^{(t-1)}\right]\mathbb{E}\left[\sum_{i=1}^{n(\varepsilon^{(t)}-\varepsilon^{(t-1)})}\norm{\vtheta_{(k)}^T\vp_i -y_i}_2^2 - \norm{\vtheta_{(k)}^T\vq_i - y_i}_2^2\right] &&
	\end{align*}
	We can now use reverse triangle inequality.
	\begin{align*}
		&\leq \mathbb{E}\left[\varepsilon^{(t)} - \varepsilon^{(t-1)}\right] \mathbb{E}\left[\sum_{i=1}^{n\left(\varepsilon^{(t)}-\varepsilon^{(t-1)}\right)}\norm{\vtheta_{(k)}^T\vp_i - \vbeta_P\vp_i - \eps_P - \vtheta^T_{(k)}\vq_i + \vbeta_Q\vq_i + \eps_Q}_2^2 \right] &&\\
		&= \mathbb{E}\left[\varepsilon^{(t)} - \varepsilon^{(t-1)}\right] \mathbb{E}\left[\sum_{i=1}^{n\left(\varepsilon^{(t)}-\varepsilon^{(t-1)}\right)}\norm{\left(\vtheta^T_{(k)} - \vbeta_P\right)\vp_i + \left(\vbeta_Q - \vtheta^T_{(k)}\right)\vq_i - \eps_P+ \eps_Q}_2^2 \right] &&
	\end{align*}
	Now we can use theorem \ref{thm:expected-value-corrupted}. To simplify notation, let $\Xi \triangleq \mathbb{E}\left[\varepsilon^{(t)} - \varepsilon^{(t-1)}\right]$
	\begin{align*}
		&= \Xi \mathbb{E}\left[\sum_{i=1}^{n\Xi}\norm{\left((1-\varepsilon^{(t)})\vbeta_P + \varepsilon^{(t)}\vbeta_Q - \vbeta_P\right)\vp_i + \left(\vbeta_Q - (1 - \varepsilon^{(t)})\vbeta_P - \varepsilon^{(t)}\vbeta_Q \right)q_i - \eps_P+ \eps_Q}_2^2 \right] &&\\
		&= \Xi \mathbb{E}\left[\sum_{i=1}^{n\Xi}\norm{\left(-\varepsilon^{(t)}\vbeta_P + \varepsilon^{(t)}\vbeta_Q\right)\vp_i + \left((1 - \varepsilon^{(t)})\vbeta_Q - (1 - \varepsilon^{(t)})\vbeta_P\right)\vq_i - \eps_P+ \eps_Q}_2^2 \right] &&\\
		&= \Xi \mathbb{E}\left[\sum_{i=1}^{n\Xi}\norm{\varepsilon^{(t)}\left(\vbeta_Q - \vbeta_P\right)\vp_i + (1 - \varepsilon^{(t)})\left(\vbeta_Q - \vbeta_P\right)\vq_i - \eps_P+ \eps_Q}_2^2 \right] &&
	\end{align*}
	Here we can use the fact that $\mathbb{E}\left[X^2\right] = \mathbb{E}\left[X\right]^2 + \Var(X)$ for a random variable $X$

	\bigskip
	Let us define the function $h(\vtheta_k) = f(\vtheta_k,S_{k-1}) - f(\vtheta_k,S_k)$, a strictly non-negative function. From our results above it follows:
	\begin{align*}
		\phi(\vtheta_{k+1}) &\leq \phi(\vtheta_k) - \frac{1}{2\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 &&\\
		&= f(\vtheta_k, S_{k-1}) - h(\vtheta_k) +  - \frac{\ell}{2}\norm{\vtheta_{k+1} - \vtheta_k}^2_2 \\
		&= f(\vtheta_k, S_{k-1}) - h(\vtheta_k) - \frac{\ell}{2}\norm{\vtheta_{k+1} - \frac{1}{\ell} \nabla_\vtheta f(\vtheta_k,S_k) - \vtheta_k}_2^2 &&\\
		&= f(\vtheta_k, S_{k-1}) - h(\vtheta_k) - \frac{\ell}{2}\left(\norm{\vtheta_{k+1}-\vtheta_k}_2^2 + \frac{2}{\ell}\norm{\vtheta_{k+1}-\vtheta_k}\nabla_\vtheta f(\vtheta_k,S_k) + \frac{1}{\ell^2}\nabla_\vtheta f(\vtheta_k,S_k)\right)
	\end{align*}
	\newpage
	
	\section{Stochastic Sub-Quantile Optimization}
	In the age of big data, stochastic methods are necessary for fast training of models to handle large amounts of data. In this section we provide an algorithm for Stochastic Sub-Quantile Optimization.
	\begin{algorithm}[H]
		\DontPrintSemicolon
		
		\KwInput{Training iterations $T$, Quantile $p$, Corruption Percentage $\eps$, Input Parameters $d$, Batch Size $m$}
		\KwOutput{Trained Parameters, $\displaystyle \vtheta$}
		$\vtheta_1 \gets \mathcal{N}(0,\sigma)^d$\\
		\For{$k \in 1,2,\ldots,T$}
		{
			$I \subseteq \left[n\right]$ of size $m$\\
			$\displaystyle \vnu = \left(X_I\vtheta_k - y_I\right)^2$\\
			$\hat{\vnu} = sorted(\vnu)$\\
			$\displaystyle t_{k+1} = \hat{\vnu}_{mp}$\\
			$L \coloneqq \sum_{i=1}^{mp} \vx_i^T\vx_i$\\
			$\alpha \coloneqq \frac{1}{2L}$\\
			$\displaystyle \vtheta_{k+1} = \vtheta_k - \alpha \nabla_{\vtheta_k}g(t_{k+1},\vtheta_k)$
		}
		\KwRet{$\vtheta_T$}
		\caption{Stochastic Sub-Quantile Minimization Optimization Algorithm}
		\label{alg:sqo-stochastic}
	\end{algorithm}
	
	\newpage
	
	\section{Additional Experiments}
	\label{app:additional-experiments}
	\subsection{\texttt{Quadratic Regression}}
	\begin{table}[!h]
	\centering
	\begin{tabular}{lccc}
		\toprule 
		\textbf{Objectives}&\multicolumn{3}{c}{Test RMSE (\texttt{Quadratic Regression})}\\                   
		\cmidrule(rl){2-4}
		&\subhead{$\eps = 0$}& \subhead{$\eps = 0.2$}& \subhead{$\eps = 0.4$}\\ 
		\midrule
		ERM  &$0.0099_{(0.0002)}$&$2.078_{(0.146)}$&$4.104_{(0.442)}$\\
		Huber \cite{Huber2009} &$1.000_{(0.0002)}$&$1.000_{(0.0003)}$&$1.13_{(0.087)}$\\
		RANSAC \cite{RANSAC1981} &$0.010_{(0.0002)}$&$0.011_{(0.0002)}$&$0.061_{(0.053)}$\\
		TERM \cite{li2020tilted} &$0.010_{(0.0001)}$&$0.012_{(0.0008)}$&$0.017_{(0.0016)}$\\
		SEVER \cite{DiakonikolasKKLSS19} &$0.0166_{(0.007)}$&$0.011_{(0.0004)}$&$0.0267_{(0.036)}$\\
		\rowcolor{LightCyan}
		SubQuantile($p = 0.6$) &$\mathbf{0.0099_{(0.0002)}}$&$\mathbf{0.00998_{(0.0002)}}$&$\mathbf{0.010_{(0.0001)}}$\\
		\midrule 
		Genie ERM &$0.0099_{(0.0002)}$&$0.00997_{(0.0002)}$&$0.010_{(0.0001)}$\\
		\bottomrule
	\end{tabular}
	\caption{\texttt{Quadratic Regression} Synthetic Dataset. Empirical Risk over $\mathbb{P}$}
	\label{tab:quadratic-regression}
	\end{table}

	\subsection{\texttt{Abalone}}
	We now provide results on \texttt{Abalone} Dataset introduced in \cite{Dua2019}.
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcc}
			\toprule 
			\textbf{Objectives}&\multicolumn{2}{c}{Test RMSE (\texttt{Abalone Linear Regression})}\\                   
			\cmidrule(rl){2-3}
			&\subhead{Clean}& \subhead{Noisy}\\ 
			\midrule
			ERM  &$2.213_{(0.0528)}$&$4845.335_{(117.5557)}$\\
			CRR \cite{bhatia2017}  &$2.345_{(0.0430)}$&$396.872_{(96.5632)}$\\
			STIR \cite{pmlr-v89-mukhoty19a}  &$2.240_{(0.0473)}$&$931.845_{(32.0864)}$\\
			Huber \cite{Huber2009} &$5.535_{(0.0665)}$&$971.362_{(28.8863)}$\\
			RANSAC \cite{RANSAC1981} &$2.522_{(0.1407)}$&$2.621_{(0.1719)}$\\
			TERM \cite{li2020tilted} &$10.686_{(0.2616)}$&$10.853_{(0.4245)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$\mathbf{2.238_{(0.0901)}}$&$\mathbf{2.287_{(0.0757)}}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 0.8$) &$\mathbf{2.292_{(0.0413)}}$&$\mathbf{2.261_{(0.0790)}}$\\
			\midrule 
			Genie ERM &$2.213_{(0.0528)}$&$2.238_{(0.0901)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Abalone Regression} Real Dataset. Empirical Risk over $\mathbb{P}$}
		\label{tab:abalone-regression}
	\end{table}
	This experiment has both feature and label noise in the Noisy Data. SubQuantile minimization no longer always converges to the $\mathbb{P}$ SubQuantile.
	
	\subsection{\texttt{Cal-Housing}}
	We now provide results on \texttt{Cal-Housing} Dataset introduced in \cite{Kelley1997}.
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcc}
			\toprule 
			\textbf{Objectives}&\multicolumn{2}{c}{Test RMSE (\texttt{Cal-Housing Linear Regression})}\\                   
			\cmidrule(rl){2-3}
			&\subhead{Clean}& \subhead{Noisy}\\ 
			\midrule
			ERM  &$0.598_{(0.0077)}$&$81.758_{(2.6230)}$\\
			CRR \cite{bhatia2017}  &$\mathbf{0.602_{(0.0081)}}$&$75.777_{(2.9403)}$\\
			STIR \cite{pmlr-v89-mukhoty19a}  &$\mathbf{0.604_{(0.0070)}}$&$65.555_{(2.1899)}$\\
			Huber \cite{Huber2009} &$\mathbf{0.601_{(0.0077)}}$&$71.813_{(2.0755)}$\\
			RANSAC \cite{RANSAC1981} &$0.681_{(0.0389)}$&$0.679_{(0.0253)}$\\
			TERM \cite{li2020tilted} &$0.737_{(0.0070)}$&$0.741_{(0.0155)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$0.640_{(0.0067)}$&$0.642_{(0.0088)}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 0.9$) &$\mathbf{0.615_{(0.0076)}}$&$\mathbf{0.612_{(0.0096)}}$\\
			\midrule 
			Genie ERM &$0.598_{(0.0077)}$&$0.603_{(0.0068)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Cal-Housing Regression} Real Dataset. Empirical Risk over $\mathbb{P}$}
		\label{tab:Cal-Housing-regression}
	\end{table}
	This experiment has both feature and label noise in the Noisy Data.
	
	
	
	In both the \texttt{Cal-Housing} and \texttt{Abalone} datasets there exists feature and label noise that exist with $5\%$ probability. In this the case, the probability is low, however since the noise is very large, even having a few points from $\mathbb{Q}$ in the final subquantile matrix can largely the bias the predictions away from the optimal parameters for $\mathbb{P}$. Therefore, we reduce $p$, the size of the subquantile to reduce the probability of obtaining corrupted samples within the subquantile. However, what we get in a decrease in variance, we do increase the bias error, albeit very slightly. 
	
	\newpage
	\section{Experimental Details}\label{app:experimental-details}
	
	\subsection{\texttt{Adaptive Linear Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Structured Linear Regression} Dataset.\\
	$\vx \sim \mathcal{N}(4,4)^{200}$\\
	$\vm \sim \mathcal{N}(4,4)^{200}$\\
	$b \sim \mathcal{N}(4,4)$\\
	$\vm' \sim \mathcal{N}(4,4)^{200}$\\
	$b' \sim \mathcal{N}(4,4)$\\
	$n_{\text{train}} = 1\text{e}4$\\
	$\mathbb{P}: y|\vx \sim \mathcal{N}(\vm^T\vx + b, 0.1)$\\
	$\mathbb{Q}: y|\vx \sim \mathcal{N}(\vm^{'\top}\vx + b', 0.1)$\\
	Please note $\vm$, $b$, $\vm'$, $b'$, are all sampled independently. The noise is added after normalization of the dataset to the standard normal $\mathcal{N}(0,1)$. 
	
	\subsection{\texttt{Oblivious Linear Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Noisy Linear Regression} Dataset.\\
	$\vx \sim \mathcal{N}(0,3)^{500}$\\
	$\vm \sim \mathcal{N}(4,4)^{500}$\\
	$b \sim \mathcal{N}(4,4)$\\
	$\vm' = \vzero$\\
	$b' \sim \mathcal{N}(5,5)$\\
	$n_{\text{train}} = 8\text{e}3$\\
	$n_{\text{test}} = 2\text{e}3$\\
	$\mathbb{P}: y|\vx \sim \mathcal{N}(\vm^T\vx + b, 0.01)$\\
	$\mathbb{Q}: y|\vx \sim \mathcal{N}(5,5)$\\
	Please note $\vm$, $b$, $\vm'$, $b'$, are all sampled independently. The noise is added after normalization of the dataset to the standard normal. 
	
	\subsection{\texttt{Quadratic Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Quadratic Regression} dataset.\\
	$x \sim \mathcal{N}(0,1)$\\
	$n_{\text{train}} = 1\text{e}4$\\
	$\mathbb{P}: y|x \sim \mathcal{N}(x^2 - x + 2, 0.01)$\\
	$\mathbb{Q}: y|x \sim \mathcal{N}(-x^2 + x + 4, 0.01)$
	
	\subsection{\texttt{Drug Discovery} Dataset}
	This dataset is downloaded from \cite{DiakonikolasKKLSS19}. We utilize the same noise procedure as in \cite{li2020tilted}.\\
	$\mathbb{P}$ is given from an 80/20 train test split from the dataset. \\
	$\mathbb{Q}$ is random noise sampled from $\mathcal{N}(5,5)$.\\
	The noise represents a noisy worker
		
	\subsection{Feature Noise}
	Take $5$\% of the training data and multiply features by 100 and responses by 10000. 
	
	\end{appendices}
\end{document}
