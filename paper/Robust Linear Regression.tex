
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{mathtools}

\usepackage{geometry}

\usepackage{subcaption}
\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{bbm}

\usepackage{color, colortbl}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{darkgray176}{RGB}{176,176,176}

\DeclareMathOperator{\proj}{proj}

\newcommand\cauchyschwarz{\stackrel{\mathclap{\normalfont\mbox{Cauchy-Schwarz}}}{\leq}}

\newcommand\woodbury{\stackrel{\mathclap{\normalfont\mbox{Woodbury}}}{=}}

\newenvironment{proofsketch}{%
	\renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\concat}{%
	\mathchoice%
	{\Big\Vert}%
	{\big\Vert}%
	{\Vert}%
	{\Vert}%
}

\usepackage{hyperref}

\usepackage{cleveref}

\usepackage{tikz,pgfplots,float}
\usetikzlibrary {datavisualization.formats.functions}
\usepgfplotslibrary{fillbetween}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{matrix}
\usepackage{xcolor}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\declaretheorem[name=Theorem]{thm}
\declaretheorem[name=Lemma,numberwithin=section]{lemma}
\declaretheorem[name=Definition]{definition}
\declaretheorem[name=Assumption]{assumption}
\declaretheorem[name=Interpretation]{interpretation}
\declaretheorem[name=Proposition]{proposition}
\declaretheorem[name=Corollary,numberwithin=thm]{corollary}
\declaretheorem[name=Fact]{fact}

\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled]{algorithm2e}
\DontPrintSemicolon
% Set algorithm keyword formatting
\SetKwComment{tcc}{$\triangleright$~}{}
\SetCommentSty{normalfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
% Set algorithm line numbers
\SetNlSty{}{}{:}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\usepackage{url}

\usepackage{appendix}
\usepackage{titletoc}

\usepackage{booktabs,caption,dcolumn}
\newcolumntype{d}[1]{D{.}{.}{4}}% column type for figures with 4 decimals
\newcommand{\subhead}[1]{\multicolumn{1}{c}{#1}}% to format sub-headings of d-type columns

\title{Robust Linear Regression by Subquantile Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Arvind Rathnashyam, Fatih Orhan, Joshua Myers, \& Jake Herman  \thanks{ Work done as a part of ML and Optimization Spring 2023 Group Project.} \\
	Department of Computer Science\\
	Rensselaer Polytechnic University\\
	Troy, NY 12180, USA \\
	\texttt{\{rathna, orhanf, myersj5, hermaj2\}@rpi.edu} \\
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
	
	\maketitle
	
	\begin{abstract}
		Robust Linear Regression is the problem of fitting data to a distribution, $P$ when there exists contaminated samples, $Q$. We consider the Huber Contamination modeled as $\hat{P} = (1-\varepsilon)P + \varepsilon Q$ where $\varepsilon \in (0,0.5)$. Traditional Least Squares Methods fit the empirical risk model to all training data in $\displaystyle \hat{P}$. In this paper we show theoretical and experimental results of Subquantile optimization to extract the target distribution, $P$ from $\hat{P}$, where we optimize with respect to the $p$-quantile of the empirical loss. Our algorithm produces state of the art results in various baselines and is theoretically proven to converge.  
	\end{abstract}
	
	\section{Introduction}
	
	Linear Regression is one of the most widely used statistical estimators throughout science. Robustness Learning in High Dimensions on Huber Contamination Models, \cite{Huber2009}, has gained much attention in the last decade, \cite{Diakonikolas2019RecentAI}. The key motivating factor in investigating robust linear regression is the sheer vastness of probability distributions that are not drawn from a normal distribution schema. Given that outliers in data sets occur so frequent, the ability for a linear regression model to be robust is necessary to compensate for the various distributions being analyzed. 
	\subsection{Motivations}
	The failure of classical regression techniques being unable to model data highly corrupted by outliers can be conveyed clearly in numerous datasets, including those featuring data in the medical, economic, and meteorological fields. Ultimately, in many real data sets, the samples may not be collected from even or fair distributions; thus, classical analyses such as standard regression or least-squares may not represent the actual distribution of the data well. 
	
	The quantile is a statistical measure that is distribution-agnostic, this makes it very suitable for robust estimation in the Huber Contamination Model. 
	
	\subsection{Contributions}
	Our goal is to provide a theoretic analysis and convergence conditions for Subquantile optimization and offer practitioners a method for robust linear regression.
	SEVERal popular methods have been utilized due to their simplicity and high effectiveness including quantile regression \cite{quantile-regression}, Theil-Sen Estimator \cite{thiel-sen}, and Huber Regression \cite{Huber2009}. These methods, although rudimentary, serve to show the effectiveness of building resistance against outliers in data. By improving upon existing methods, namely least-squares estimation in these cases, models can be designed to better estimate data sets with considerably corruptive outliers.
	
	Subquantile Optimization aims to address the shortcomings of ERM in applications such as noisy/corrupted data (\cite{khetan2018learning},\cite{jiang2018mentornet}), classification with imbalanced classes, (\cite{lin2017dense},\cite{he2009imbalanced}), as well as fair learning (\cite{Corbett2018fairness}).
	
	\begin{figure}[!t]
		\begin{tikzpicture}
			\begin{groupplot}[group style={group size= 2 by 4},height=5cm,width=6.4cm,xmax=4,xmin=-4,
				ymin= -10,ymax=10,
				xtick={-4,-3,-2,...,4},
				ytick={-10,-8,-6,...,10},]
				\nextgroupplot[title=Adaptive Noise,ylabel={$y$},xlabel={$x$}]
				\coordinate (c1) at (current axis.left of origin);
				\addplot[mark=*,color=cyan,only marks] coordinates{(0.5454756141119426,4.20207144062236) (2.6952740390873733,8.475671650669172) (-0.1612438784423731,2.688323570846449) (1.9998807196500725,7.011477368661404) (0.6573126643085468,4.432503359641061) (0.822541876079238,4.691099695987436) (-1.665736406528514,-0.2771610262088303) (0.2955820690953599,3.6958661148678047) (4.329988449821567,11.633259356797302) (-4.412313203501535,-5.77063957797815) (3.5629538644332275,10.091868592245838) (-0.6175885445775965,1.7473187369931718) (0.8108793561773843,4.772417668204566) (1.4214273053887914,5.818934958869865) (3.081315566792172,9.08926804908307) (-0.12089034451099817,2.6605946971138144) (0.4628376617792958,3.923002461330265) (0.37617729415550505,3.676581100428469) (0.977114787165429,4.918947422974899) (1.314754156260769,5.688804682122404) (-0.36235424331683597,2.2267827837116263) (-0.8007556950385752,1.4886373858967947) (-0.7015664550907283,1.7651694045148667) (-2.1753647987904734,-1.482231084924366) (-1.495574696589605,0.04857682714064265) (-1.0588112744961395,0.8183551891451499) (-3.827119610541674,-4.796523392389788) (-0.8559789159750391,1.1837704089349095) (-0.11761120022533014,2.7730611413376285) (3.3752350984949295,9.778804511993851) (2.4222770389407957,7.727148811092623) (-1.5410032016624133,-0.2633438113497304) (2.480017633230306,7.979402356067379) (0.5911583134769168,4.283748748593926) (-0.71501674964969,1.6324881095139794) (0.6516043112113717,4.294631666773893) (-3.079711134644328,-3.285226613281087) (-1.5088703031224757,-0.08081402676688078) (0.7827672445662932,4.570750601954812) (1.8939059587103262,6.858172766747038)};
				\addplot[mark=*,color=orange,only marks] coordinates{(-2.946629011640607,-8.793723527498958) (0.8607384957406246,-1.538989556510056) (-1.1903001477494382,-5.345751610044574) (1.0305878148541996,-0.8812658035679226) (2.7345343942849927,2.5442143953062244) (-1.6113746066635852,-6.284678207630399) (-0.8883952236652384,-4.5410943399901775) (0.8157297797465353,-1.2566730413348421) (0.9642301396642339,-0.9032576567416104) (-0.14448993411816005,-3.334946134068381)};
				\addplot[domain=-5:5,samples=120,color=black,very thick] {1.99804993*x+2.99972117} ;
				\addplot[domain=-5:5,samples=120,color=red,very thick] {0.55184722*x+0.77673256} ;
				\addplot[domain=-5:5,samples=120,color=green,very thick] {0.61291924*x+1.73788515} ;
				\nextgroupplot[title=Oblivious Noise,xlabel={$x$},legend to name={IntroLegend},legend style={legend columns=5}]
				\coordinate (c2) at (current axis.right of origin);% I moved this to the upper right corner
				\addplot[domain=-5:5,samples=120,color=black,very thick] {2.00041502*x+3.00091281} ;
				\addlegendentry{SubQuantile}
				\addplot[domain=-5:5,samples=120,color=red,very thick] {0.000166177232*x-1.89493121} ;
				\addlegendentry{TERM}
				\addplot[domain=-5:5,samples=120,color=green,very thick] {1.90359844*x+2.82276139} ;
				\addlegendentry{SEVER}
				\addplot[mark=*,color=cyan,only marks] coordinates{(0.788762485297498,4.619409754983769) (-1.8444557076323929,-0.5824544981134812) (-3.8667186653234786,-4.754991283615981) (-1.0770702633013416,0.7722164021812354) (-0.35460032498749144,2.2768627851664966) (-6.0822151692288084,-9.06286764938399) (-1.3972518211012221,0.1885381821988659) (2.9097896507970673,8.788974171667162) (-0.3463437020806441,2.420626808504963) (-2.1062359514012012,-1.2085000641078332) (4.336773668705573,11.632780878244247) (-0.6106102670869219,1.8184898177512854) (2.964822961712252,8.905021515559394) (-1.5637145371389622,-0.07761865537709459) (1.0345144344033383,5.0204590940620575) (2.056356483528722,7.198480210723839) (-0.7446791767339869,1.5853174482562558) (0.9631985535487033,4.9300987804715914) (0.8591431516732644,4.6302171243067765) (2.136527593488855,7.225942702339085) (-0.22230647531299189,2.5877920816746207) (1.4255477338524918,5.833868092502212) (-1.4608390115429133,0.12305693654859831) (0.8455335501589681,4.769483861326801) (-2.1885026970448886,-1.4714423105566306) (0.31837311592194245,3.604291796660203) (-2.5244214783210617,-2.0353095069267404) (0.9463907944621844,4.957203119009067) (0.8146787696108556,4.669681514124957) (0.7171745412553174,4.525279797042462)};
				\addlegendentry{clean}
				\addplot[mark=*,color=orange,only marks] coordinates{(-3.2474795820658593,6.450035960386647) (1.8753826722797218,-0.41858598306318395) (1.4931353198669002,-1.2946095099790276) (-6.390816833283051,0.40645831126308823) (0.39048632994899735,2.323962397589362) (-0.08068388106259215,2.8091622671588747) (1.3134080902531131,-3.6866845075408796) (-0.5728261692279717,-1.6637925896534693) (-3.795620644314326,3.901941933326407) (-2.2805802625691634,2.1567188534369586) (-1.1331317978018236,-1.0620378556289347) (-4.057270733769615,6.22788036393642) (3.580927646622491,1.4134339560728868) (1.1473909216831844,0.7892374536150136) (4.60155271303633,1.1396713973323012) (0.777450139132804,-2.5904275309220073) (1.2193257645912936,0.9783812850138183) (-2.0137550195781966,0.08990758847097716) (-3.0052924293950043,0.9558207339897568) (-1.6843214424199917,3.0989236738078767)};
				\addlegendentry{noise}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
				{\pgfplotslegendfromname{IntroLegend}};
		\end{tikzpicture}
		\caption{Oblivious Outliers are generated without knowledge of the clean distribution. Adaptive outliers are generated with knowledge of the clean distribution.}
		\label{fig:structure-unstructured-noise}
	\end{figure}
	
	As seen in the above comparison, current models fail to estimate data sets corrupted by structured noise, with some models even failing to estimate trends plagued with unstructured noise. Through this, Subquantile optimization is shown to prevail at overcoming these challenges current models currently face. In Table 
	
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcccc}
			\toprule 
			Paper & Adversary & Threshold & Resampling & Iteration Complexity\\   
			\midrule                
			SEVER \cite{DiakonikolasKKLSS19} & Adaptive & Gradient of Loss & \xmark & $\mathcal{O}(nd^2)$\\
			\midrule 
			Subquantile Minimization (Ours) & Adaptive & Loss & \cmark & $\mathcal{O}(n)$\\
			\bottomrule
		\end{tabular}
		\caption{A comparison between Subquantile Minimization and the most similar work, SEVER. Note this is the complexity of the thresholding procedure after running a base solver.}
		\label{tab:related-work}
	\end{table}
	
	\section{Related Work}
	
	Tilted Empirical Risk Minimization (TERM) \cite{li2020tilted} is a framework built to similarly handle the shortcomings of emperical risk minimization (ERM) with respect to robustness. The TERM framework instead minimizes the following quantity, where $t$ is a hyperparameter known as tilt
	\begin{equation}
		\tilde{R}(t;\vtheta) \coloneqq \frac{1}{t} \log\left(\frac{1}{N}\sum_{i \in \left[N\right]}e^{tf(\vx_i;\vtheta)} \right)
	\end{equation}
	By using the tilt hyperparameter to change the individual impact of each specific loss, the model is more resistant to outliers found in the data.
	
	SMART \cite{https://doi.org/10.48550/arxiv.2206.04777} proposes the \textit{iterative trimmed maximum likelihood estimator} against adversarially corrupted samples in General Linear Models (GLM). The estimator is defined as follows, where $S = \{(\vx_i,y_i)\}_{i=1}^n$ represents the training data. \vspace{1em}
	\begin{equation}
		\hat{\vtheta}(S) = \min_{\vtheta} \min_{\hat{S} \subset S, |\hat{S}| = (1-\eps)n} \sum_{(\vx_i,y_i) \in S} -\log f(y_i|\vtheta^T\vx_i)
	\end{equation}
	This estimator is proven to return near-optimal risk on a variety of linear models, including Gaussian regression, Poisson regression, and binomial regression; these achievements can be demonstrated on label and covariate corruptions.
	
	
	SEVER \cite{DiakonikolasKKLSS19} is a gradient filtering algorithm which removes elements whose gradients have the furthest distance from the average gradient of all points
	\begin{equation}
		\tau_i = \left((\nabla f_i(\vw) - \hat{\nabla})\cdot \vv\right)^2
	\end{equation}
	This method is novel in that it is highly scalable, making it robust against high-dimension data with structured outliers. Similarly, SEVER is easily implemented with standard machine learning libraries and can be applied to many typical learning problems, including classification and regression. Despite this, the algorithm still falls short when features have high covariance or when features have low predictive power of the target. Moreover, SEVER requires approximate learners to be run after every iteration, making SEVER unfeasible for large-scale machine learning tasks. 
	
	Super-Quantile Optimization \cite{ROCKAFELLAR2014140} aims to solve error minimization problems by building upon the aforementioned quantile regression by centering around a conditional value-at-risk, or a superquantile. For $\alpha \in [0,1)$, the $\alpha$-superquantile for a random variable $Y$ is defined as \begin{equation}
		\Bar{q}_\alpha (Y) := \frac{1}{1-\alpha} \int_\alpha^1 q_\beta (Y) d\beta
	\end{equation}
	In doing so, more conservatively fitted curves are produced. As with quantile regression, such curves do require the solution of a linear program.
	This concept of superquantile error provides insight into tail behavior for quantities of error and an overall unique approach to linear regression.
		
	\section{Subquantile Optimization}
	\label{sec:Subquantile-optimization}
	
	\begin{definition}
		Let $F_X$ represent the Cumulative Distribution Function (CDF) of the random variable $X$. The \textbf{$\mathbf{p}$-Quantile} of a Random Variable $X$ is defined as follows \vspace{1em}
		\begin{equation}
			Q_p(p) = \inf\{x\in\mathbb{R}: p \leq F(x)\} 
		\end{equation}
	\end{definition}
	
	\begin{definition}
		Let $\ell$ be the loss function. \textbf{Risk} is defined as follows
		\begin{equation}
			U = \mathbb{E}_{(\vx,y) \sim \mathbb{P}}\left[\ell \left( f(\vx;\vtheta,y)\right)\right]
		\end{equation}
	\end{definition}
	The $\mathbf{p}$-\textbf{Quantile} of the Empirical Risk is given
	\begin{equation}\label{eqn:Subquantile}
		\mathbb{L}_p(U) = \frac{1}{p}\int_0^p \mathcal{Q}_q(U)\,dq = \mathbb{E}\left[U|U \leq \mathcal{Q}_p(U) \right] = \max_{t\in \mathbb{R}}\left\{t - \frac{1}{p}\mathbb{E}\left[(t-U)^+\right]\right\}
	\end{equation}
	In equation \ref{eqn:Subquantile}, $t$ represents the $p$-quantile of $U$. We also show that we can calculate $t$ by a maximizing optimization function. 
	The Subquantile Optimization problem is posed as follows
	\begin{equation}
		\vtheta_{SM} = \argmin_{\vtheta \in \mathbb{R}^d} \max_{t \in \mathbb{R}} \left\{t - \frac{1}{p}\mathbb{E}(t - \ell(f(\vx;\vtheta),y))^+\right\}
	\end{equation}
	
	For the linear regression case, this equation becomes 
	\begin{equation}
		\label{eqn:theta_sm}
		\vtheta_{SM} = \argmin_{\vtheta \in \mathbb{R}^d} \max_{t \in \mathbb{R}} \left\{t - \frac{1}{np}\sum_{i=1}^n\left(t-(\vx_i \vtheta - y_i)^2\right)^+\right\}
	\end{equation}
	
	The objective function we define will be: 
	\begin{equation}
		g(t,\vtheta) = t - \frac{1}{np} \sum_{i=1}^n\left(t - (\vx_i \vtheta - y_i)^2\right)^+
	\end{equation}
		
	The two-step optimization for Subquantile optimization is given as follows \vspace{1em}
	\begin{equation}
		\label{eqn:t-update}
		t_{k+1} = \argmax_t g(t,\vtheta_k) 
	\end{equation}
	\begin{equation}
		\label{eqn:theta-update}
		\vtheta_{k+1} = \vtheta_k + \alpha \nabla_{\vtheta_k} g(t,\vtheta_k)
	\end{equation}
		
	This algorithm is adopted from \cite{Razaviyayn}. Theoretically, it has been proven to converge to a local nash equilibrium in \cite{Jin_2019} when $g(t,\vtheta)$ is $\ell$-smooth with respect to $\vtheta$ and there exists an $\epsilon$-maximizer for $g(t,\vtheta)$ with respect to $t$ .\\
	\begin{minipage}{0.48\textwidth}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		
		\KwInput{Training iterations $T$, Quantile $p$, Data Matrix: $X, (n \times d), n \gg d$}
		\KwOutput{Trained Parameters, $\displaystyle \vtheta_{(T)}$}
		$\vtheta_{(0)} \gets (X^T X)^{-1}X^T y$\\
		\For{$k \in 1,2,\ldots,T$}
		{
			$\displaystyle \vnu \gets \left(X\vtheta_{(k)} - y\right)^2$\\
			$S_{(k)} \gets \textsc{Subquantile}(\vtheta_{(k)},X)$\\
			$L_{(k)} \gets \frac{1}{np}\norm{S_{(k)}^T S_{(k)}}_2$\\
			$\alpha_{(k)} \gets 1/2L_{(k)}$\\
			$\displaystyle \vtheta_{(k+1)} \gets \vtheta_k - \alpha_{(k)} \nabla_\vtheta g(t_{(k+1)},\vtheta_{(k)})$
		}
		\KwRet{$ \vtheta_{(T)} $}
		\caption{Subquantile Minimization Gradient Descent}
		\label{alg:sqo1}
	\end{algorithm}
	\end{minipage}
	\hfill
	\begin{minipage}{0.48\textwidth}
		\begin{algorithm}[H]
			\KwInput{Training Iterations $T$,Quantile $p$}
			\KwOutput{Trained Parameters, $\vtheta_{(T)}$}
			$\vtheta_{(0)} \gets (X^T X + \lambda I)^{-1}X^T y$\\
			\For{$k \in \left\{1,2,\dots,T\right\}$}
			{
				$S_{(k)} \gets \textsc{Subquantile}(\vtheta_{(k)},X)$\\
				$\vtheta_{(k+1)} \gets (S_{(k)}^T S_{(k)} + \lambda I)^{-1}S_{(k)}^T \vy_S$\\
			}
			\KwRet{$ \vtheta_{(T)} $}
			\caption{Subquantile Minimization for Ridge Regression}
			\label{alg:sqo-ridge}
		\end{algorithm}
		\begin{algorithm}[H]
			\KwInput{Parameters $\vtheta$, Data Matrix: $X, (n \times d)$}
			\KwOutput{Subquantile Matrix $S$}
			$\hat{\vnu} \gets \operatorname*{sorted}(X \vtheta_{(k)} - \vy)^2$\\
			$t \gets \hat{\vnu}_{np}$\\
			Let $\mX_1,...,\mX_{np}$ be $np$ points such that $\left(\mX_i \vtheta - y_i\right)^2 \leq t$\\
			$S \gets \begin{pmatrix} \mX_1^T & \dots & \mX_{np}^T\end{pmatrix}^T$\\
			\KwRet{$ S $}
			\caption{\textsc{Subquantile}}
		\end{algorithm}
	\end{minipage}


	\section{Theory} 
	In this section, we will explore the fundamental aspects of $g(t,\vtheta)$. This will motivate the convergence analysis in the next section. Throughout this section we will denote $\vx_i$ as the $i$th row in the data matrix $X$ and $y_i$ as its corresponding label. We will denote $\eta_P$ as the number of data points from $P$ within the subquantile, i.e. within the lowest $np$ losses, and $\eta_Q$ as the number of data points from $Q$ within the subquantile. We also define $\varepsilon^{(t)} \triangleq \frac{\eta_P}{\eta_P + \eta_Q} = \frac{\eta_P}{np}$ , as the ratio of corrupted points within the subquantile at optimization iteration $t$, where $p \in (0,1)$ is the subquantile we are optimizing over. 
	
	\subsection{Analysis of $g(t,\theta)$}
		
	\begin{restatable}{lemma}{gtconcavelemma}
		\label{lem:gtcomcavelemma}
		$g(t_{k+1},\vtheta_k)$ is concave with respect to $t$.
	\end{restatable}
	\begin{proof}
		We provide a simple argument for concavity. Note $t$ is a concave and convex function. Also $(\cdot)^+$ is a convex strictly non-negative function. Therefore we have a concave function minus the non-negative multiple of a summation of an affine function composed with a convex function. Therefore this is a concave function with respect to $t$. 
	\end{proof}	
	
	\begin{restatable}{lemma}{gfermat}
		\label{lem:gfermat}
		The maximizing value of $t$ in $g(t,\vtheta)$ in $t$-update step of optimization as described by Equation \ref{eqn:t-update} is maximized when $t = Q_p(U)$
	\end{restatable}
		Since $\displaystyle g(t,\vtheta)$ with respect to $t$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
		Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T X - y)^2\right)$ and note $\displaystyle 0 < p < 1$
		\begin{align}
			\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) 
			&= -1 + \frac{1}{np}\sum_{i=1}^{n}
			\left\{
			\begin{array}{lr}
				1, & \text{if } t > \hat{\vnu}_i\\
				0, & \text{if } t < \hat{\vnu}_i \\
				\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
			\end{array}
			\right\}&&\\
			&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
		\end{align}
		This is the $p$-quantile of $U$. A full derivation is provided in Appendix \ref{app:gfermat}.
	
	\begin{restatable}{lemma}{gthetaderiv}\label{lem:gthetaderiv}
		Let $t = \hat{\vnu}_{np}$. The $\vtheta$-update step described in Equation \ref{eqn:theta_sm} is equivalent to minimizing the least squares loss of the $np$ elements with the lowest squared loss.
	   \begin{equation}
			\nabla_{\vtheta}g(t_{k+1},\vtheta_k) = \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)
		\end{equation}
	\end{restatable}
	We provide a derivation in Appendix \ref{app:gthetaderiv}. However, this result is quite intuitive as it shows we are optimizing over the $p$ Subquantile of the Risk.
	\begin{interpretation}
		\label{int:minimize-small}
		Subquantile Minimization continously minimizes the risk over the $p$-quantile of the error. In each iteration, this means we reduce the error of the points within the lowest $np$ errors.
	\end{interpretation}

		
	\begin{restatable}{lemma}{gthetaconvexlemma}
		\label{lem:gthetaconvex}
		$g(t_{k+1},\vtheta_k)$ is convex with respect to $\vtheta_k$.
	\end{restatable}
	\begin{proof}
		We see by lemma \ref{lem:gfermat} and interpretation \ref{int:minimize-small}, we are optimizing by the $np$ points with the lowest squared error. Mathematically, 
		\begin{align*}
			g(t_{k+1},\vtheta_k) &= t_{k+1} - \frac{1}{np}\sum_{i=1}^{n}\left(t_{k+1} - (\vtheta^T\vx_i - y_i)^2\right)^+ &&\\
			&= t_{(k+1)} - t_{(k+1)} + \frac{1}{np}\sum_{i=1}^{np}(\vtheta^T\vx_i - y_i)^2 
			= \frac{1}{np}\sum_{i=1}^{np}(\vtheta^T\vx_i - y_i)^2 &&
		\end{align*}
		Now we can make a simple argument for convexity. We have a non-negative multiple of the sum of the composition of an affine function with a convex function. Thus $g(t,\vtheta)$ is convex with respect to $\vtheta$.
	\end{proof}
	
	\begin{restatable}{lemma}{g-lsmooth}
		\label{lem:g-lsmooth}
		$g(t,\vtheta)$ is $L$-smooth with respect to $\vtheta$ with $\displaystyle L = \norm{\frac{2}{np}\sum_{i=1}^{np}\norm{\vx_i}^2}$ 
	\end{restatable}
	
	Now we will state two properties regarding the effect of the $t$-update step and the $\vtheta$-update step as described in Equations \ref{eqn:t-update} and \ref{eqn:theta-update}, respectively. 
	\begin{restatable}{lemma}{g-change-wrt-t}
		\label{lem:g-change-wrt-t}
		If $t_{k+1} \leq t_k$ then $ g(t_{k+1},\vtheta_k) = g(t_{k}) + \frac{1}{np}\sum_{i=np}^n(t_k - \hat{\vnu}_i)^+$. If $t_{k+1} > t_k$, then $ g(t_{k+1},\vtheta_k) = g(t_k) + \frac{1}{np}\sum_{i=n(p-\delta)}^{np}(t-\hat{\vnu}_i)^+  - \delta t$. For a small $\delta$. Note $\hat{\vnu}$ represents the ascending order of the sorted errors over all data points with respect to $\vtheta_{(k)}$. 
	\end{restatable}
	Clearly, this result is not overly intuitive, thus it is difficult to analyze the convergence of this algorithm as the effect of $t$ on the objective function $g$ is not consistent. Empirically, we find $t$ is not a monotonically decreasing value. Therefore, in the next section, we will provide a different characterization of $g$ so we can better analyze its convergence. 

	\subsection{Optimization}

	We are solving a min-max convex-concave problem, thus we are looking for a Nash Equilibrium Point. 
	
	\begin{restatable}{definition}{nash-equilibrium}
		\label{def:nash-equilibrium}
		$(t^*,\vtheta^*)$ is a \textbf{Nash Equilibrium} of $g$ if for any $(t,\vtheta) \in \mathbb{R}\times\mathbb{R}^d$\vspace{1em}
		\begin{equation}
			g(t^*,\vtheta) \leq g(t^*,\vtheta^*) \leq g(t,\vtheta^*)
		\end{equation}
	\end{restatable}	
	\begin{restatable}{definition}{local-nash-equilibrium}
		\label{def:local-nash-equilibrium}
		$(t^*,\vtheta^*)$ is a \textbf{Local Nash Equilibrium} of $g$ if there exists $\delta > 0$ such that for any $t,\vtheta$ $(t,\vtheta)$ satsifying $\norm{t - t^*} \leq \delta$ and $\norm{\vtheta -\vtheta^*} \leq \delta$ then: 
		\vspace{1em}
		\begin{equation}
			g(t^*,\vtheta) \leq g(t^*,\vtheta^*) \leq g(t,\vtheta^*)
		\end{equation}
	\end{restatable}
	\begin{proposition}\label{prop:first-order-nash}
		As $g$ is first-order differentiable,  any local Nash Equilibrium satisfies $\nabla_\vtheta g(t,\vtheta) = \mathbf{0}$ and $\nabla_t g(t,\vtheta) = 0$
	\end{proposition}
	
	We are now interested in what it means to be at a Local Nash Equilibrium. By Proposition \ref{prop:first-order-nash}, this means both first-order partial derivatives are equal to $0$. By lemma \ref{lem:gfermat}, we have shown $\nabla_tg(t,\vtheta) = 0$ when $\vnu_{np} \leq t < \vnu_{np+1}$. Furthermore, by lemma \ref{lem:gthetaderiv}, we have shown $\nabla_\vtheta(g,\vtheta) = 0$ when the least squares error is minimized for the $np$ points with lowest squared error.
	This means that for a subset of $np$ points from $X$, the least squares error is minimized. What we are interested in is how many points within those $np$ points come from $P$ and how many of those points from $Q$. Our goal is to minimize the number of points within the $np$ lowest squared losses from $Q$, as they will introduce error to our predictions on points from $P$. 
	
	
	
	\begin{restatable}{thm}{convergence-guarantee}
		\label{thm:convergence-guarantee}
		Let $g(t,\vtheta)$ be differentiable and $g(t,\vtheta)$ be $L$-smooth in $\vtheta$. Let $t_{(k)}$ and $\vtheta_{(k)}$ be iterates from algorithm \ref{alg:sqo1}. Then, $\lim_{k \to \infty} \mathbb{E}\left[\norm{\nabla_\vtheta g(t_{(k+1)},\vtheta_{(k)})}\right] = \vzero$.
	\end{restatable}
	\begin{proofsketch}
		From the intuitions we have gained on Subquantile Minimization. We can state the following:
		\begin{equation}
			\argmin_{\vtheta \in \mathbb{R}^d}\min_{S \in \Pi}\norm{\vtheta^T S - y_S}_2^2 \iff \argmin_{\vtheta \in \mathbb{R}^d}\max_{t \in \mathbb{R}}\norm{\vtheta^T S - y_S}_2^2
		\end{equation}
		where $\Pi$ represents the entire distribution of Subquantile matrices. This characterization of the min-max optimization problem into a min-min optimization problem allows us to intuitively see the convergence properties of this algorithm.
	\end{proofsketch}
	

	\subsection{Convergence of Algorithm~\ref{alg:sqo1}}

	\begin{restatable}{lemma}{effect-of-projection}
		\label{lem:effect-of-projection}
		The expected value of error on points in $P$ will be lower than the expected value of error on points in $Q$ if $\displaystyle \norm{\proj_{\vbeta_P}(\vtheta) - \vbeta_P} < \norm{\proj_{\vbeta_Q}(\vtheta) - \vbeta_Q}$
	\end{restatable}
	
	Lemma \ref{lem:effect-of-projection} gives us an intuitive result. If in each optimization step, our projection on $\vbeta_P$ is closer than our projection on to $\vbeta_Q$, we know the number of data points from $\mathbb{Q}$ in the Subquantile will increase from the previous iteration.
	\begin{restatable}{thm}{improvement-of-theta}
		\label{thm:improvement-of-theta}
		Given a data matrix $X = \begin{pmatrix}P\\Q\end{pmatrix}$ where the rows of $P$ are sampled from $\mathcal{N}_d(\vzero, \xi_P I)$ and the rows of $Q$ are sampled from $\mathcal{N}_d(\vzero, \xi_Q I)$. After a $\vtheta$ update, 
		\begin{equation*}
			\norm{\proj_{\vbeta_P}\vtheta_{(t+1)} - \vbeta_P} - \norm{\proj_{\vbeta_P}\vtheta_{(t)} - \vbeta_P} <  \norm{\proj_{\vbeta_Q}\vtheta_{(t+1)} - \vbeta_Q} - \norm{\proj_{\vbeta_Q}\vtheta_{(t)} - \vbeta_Q}
		\end{equation*}
			  if the following holds
		\begin{equation}
			\norm{\left(\alpha_1^{(t)}\left(\Xi^{(t)}-1\right) + \gamma\left(1-\varepsilon^{(t)}\right)\xi_P\right)\vbeta_P} > \norm{\left(\alpha_2^{(t)}\left(\Xi^{(t)}-1\right) + \gamma \varepsilon^{(t)}\xi_Q\right)\vbeta_Q}
		\end{equation}
		where $\alpha_1$ and $\alpha_2$ represents the coefficients for the linear combination of $\vtheta$ in the basis defined as $\mB = \begin{bmatrix} \vbeta_P & \vbeta_Q & \mR\end{bmatrix}$ and $\displaystyle \Xi^{(t)} \triangleq \left(1 - \gamma\left(\left(1 - \varepsilon^{(t)}\right)\xi_P + \varepsilon^{(t)}\xi_Q\right)\right)$ and $\gamma \triangleq np\alpha$ where $\alpha$ is the learning rate. 
	\end{restatable}

	\begin{restatable}{thm}{thm:expectation-of-improvement}
		\label{thm:expectation-of-improvement}
		Given a data matrix $X = \begin{pmatrix}P\\Q\end{pmatrix}$ where the rows of $P$ are sampled from $\mathcal{N}_d(\vzero, \xi_P I)$ and the rows of $Q$ are sampled from $\mathcal{N}_d(\vzero, \xi_Q I)$. Given $\varepsilon^{(t)}$ of the subquantile at iteration $t$ and $\varepsilon^{(t+1)}$ at iteration $t+1$, assuming $\varepsilon^{(t+1)} < \varepsilon^{(t)}$, the subquantile update improves the objective function in expectation by :
			\begin{equation*}
			\mathbb{E}\left[f\left(\vtheta_{(k)}, S_{(k+1)}\right) - f\left(\vtheta_{(k)}, S_{(k)}\right)\right]= n\left(\varepsilon^{(t)} - \varepsilon^{(t-1)}\right) \left(\xi_P\norm{\vtheta_{(k)}^T - \vbeta_P^T}^2_2 + \xi_Q\norm{\vtheta_{(k)}^T - \vbeta_Q^T}^2_2\right)
		\end{equation*}
	\end{restatable}

	\subsection{Convergence of Algorithm~\ref{alg:sqo-ridge}}
	\begin{restatable}{thm}{bounded-from-optimal}\label{thm:bounded-from-optimal}
		Given a subquantile matrix $S = \begin{pmatrix}P \\Q \end{pmatrix}$ where the rows of $P$ are sampled from $\mathcal{N}_d(\vzero, \xi_P I)$ and the rows of $Q$ are sampled from $\mathcal{N}_d(\vzero, \xi_Q I)$ and for all data points in the subquantile matrix it holds $(\vx_i\vtheta - y_i) \leq t_{(k)}$. Let $\sigma_{\max}(P)$ be the maximum singular value of $P$ and $\sigma_{\max}(Q)$ be the maximum singular value of $Q$. Then at any iteration, it holds: 
		\begin{equation*}
			\displaystyle \norm{\vbeta_P - \vtheta_{(k)}}_2 \leq \frac{2\sigma^2_{\max}(P)\norm{\vbeta_P} + 6\sigma_{\max}(P)n(1-\varepsilon^{(k)})\eta_P + \sigma^2_{\max}(Q)\norm{\vbeta_Q} + 3\sigma_{\max}(Q)n\varepsilon^{(k)}\eta_Q}{\sqrt{\sigma^2_{\max}(P) + \lambda}}
		\end{equation*}
		where $\Var(\eps_P) = \eta_P$ and $\Var(\eps_Q) = \eta_Q$
	\end{restatable}

	Theorem \ref{thm:bounded-from-optimal} gives us a bound on the $2$-norm distance from the optimal parameters in each iteration.
	
	\begin{restatable}{lemma}{probability-of-hard-convergence}
		\label{thm:probability-of-hard-convergence}
		Given a subquantile matrix $X = \begin{pmatrix}P\\Q\end{pmatrix}$ where the rows of $P$ are sampled from $\mathcal{N}_d(\vzero, \xi_P I)$ and the rows of $Q$ are sampled from $\mathcal{N}_d(\vzero, \xi_Q I)$, where there are $\ell$ data points of $P$ and $m$ data points of $Q$. Then the PDF of the hard convergence distribution,i.e. there are no points from $Q$ within the subquantile, is given as follows:
		\begin{equation}h(x) = \ell f_P(x) \left(F_P(x)\right)^{\ell-1}\left(1 - F_Q(x)\right)^{m}\end{equation}
		where $f_P(x) = \frac{1}{\sqrt{2\pi}}\left(\frac{x}{\phi}\right)$ is the PDF for the $\chi^2$ distribution with $1$ degree of freedom, and similarly $F_P(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\phi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\phi}}\right)$ and $F_Q(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\psi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\psi}}\right)$ where $\phi = \norm{\vtheta - \vbeta_P}_2$ and $\psi = \norm{\vtheta - \vbeta_Q}_2$
	\end{restatable}
	This formulation follows from the probability theory concept of order statistics and the normal distribution of the data vectors. Intuitively, if $\phi$ is significantly smaller than $\psi$ the probability of hard-convergence should be higher. The maximal value of $h(x)$ will be at $\mathbb{E}\left[F_{P(\ell)}\right] $, the $\ell$-th order statistic of $P$.

	
	\section{Empirical Results}\label{sec:numerical-experiments}
	In all experiments, we utilize Algorithm \ref{alg:sqo-ridge} due to its fast convergence and strong theoretical properties. Ransac \cite{RANSAC1981}, and Huber \cite{Huber2009}, are standard regression techniques implemented in \texttt{sklearn} for outlier detection. 
	\subsection{Synthetic Data}
	
	\begin{figure}[!b]
		\centering
		\begin{tikzpicture}
			\begin{groupplot}[group style={
					group name=myplot,
					group size= 2 by 1},height=6cm,width=0.5\linewidth,
					tick align=outside,
					tick pos=left,
					legend pos=outer north east,
					x grid style={darkgray176},
					xmin=0.1, xmax=0.4,
					xtick style={color=darkgray176},
					y grid style={darkgray176},
					ymin=0, ymax=1000,
					ytick style={color=darkgray176},
					ytick={0,0.001,0.01,0.1,1,10,100,1000},
					ymajorgrids=true,
					grid style=dashed,
					ymode=log,
					log ticks with fixed point,
				]
			\nextgroupplot[ylabel=Test RMSE,title=\texttt{Adaptive Linear Regression}, xlabel=$\varepsilon^{(0)}$]
			\coordinate (c1) at (current axis.left of origin);
			\addplot [very thick, black,mark=square*]
			table {%
				0.1 0.0470812581479549
				0.13 0.0481603480875492
				0.16 0.050400760024786
				0.19 0.0517568401992321
				0.22 0.0543827041983604
				0.25 0.0556097775697708
				0.28 0.0590417385101318
				0.31 0.0629159957170486
				0.34 0.0656534433364868
				0.37 0.06785549223423
				0.4 0.0717944204807281
			};
			\addplot [very thick, green,mark=square*]
			table {%
				0.1 0.36738908290863
				0.13 0.342920124530792
				0.16 0.314845204353333
				0.19 0.28143972158432
				0.22 0.257287442684174
				0.25 0.234187826514244
				0.28 0.210285276174545
				0.31 0.190747454762459
				0.34 0.165870726108551
				0.37 0.136498242616653
				0.4 0.102688848972321
			};
			\addplot [very thick, red,mark=square*]
			table {%
				0.1 80.540153503418
				0.13 78.2785491943359
				0.16 79.4500045776367
				0.19 78.0468368530273
				0.22 79.6445236206055
				0.25 79.5632705688477
				0.28 79.6250457763672
				0.31 80.7044982910156
				0.34 79.593620300293
				0.37 80.6381225585938
				0.4 82.4261169433594
			};
			\addplot [very thick, orange,mark=square*]
			table {%
				0.1 0.55393660068512
				0.13 0.702713727951059
				0.16 0.8559549450874336
				0.19 1.00228035449982
				0.22 1.14807796478271
				0.25 1.30311346054077
				0.28 1.44514167308807
				0.31 1.58323192596436
				0.34 1.73416042327881
				0.37 1.8974506855011
				0.4 2.0302050113678
			};
			\addplot [very thick, purple,mark=square*]
			table {%
				0.1 1.87000024318695
				0.13 3.27856779098511
				0.16 1.90161323547363
				0.19 1.14748334884644
				0.22 3.25915050506592
				0.25 2.77480888366699
				0.28 1.93823385238647
				0.31 1.64351272583008
				0.34 2.00752592086792
				0.37 1.70363962650299
				0.4 3.40018320083618
			};
			\addplot [very thick, cyan,mark=square*]
			table {%
				0.1 0.554642379283905
				0.13 0.703209280967712
				0.16 0.856137990951538
				0.19 1.00242388248444
				0.22 1.1481751203537
				0.25 1.30330908298492
				0.28 1.44537603855133
				0.31 1.58298897743225
				0.34 1.73431789875031
				0.37 1.89734447002411
				0.4 2.03032040596008
			};
			\addplot [very thick, blue,mark=square*]
			table {%
				0.1 0.386876940727234
				0.13 0.571252167224884
				0.16 0.743153214454651
				0.19 0.883608460426331
				0.22 1.02778625488281
				0.25 1.1869922876358
				0.28 1.31735336780548
				0.31 1.44525456428528
				0.34 1.59675407409668
				0.37 1.76741755008698
				0.4 1.89613056182861
			};
			\addplot [very thick, teal,mark=square*]
			table {%
				0.1 0.165539637207985
				0.13 0.233492165803909
				0.16 0.330621749162674
				0.19 0.466441482305527
				0.22 0.568290293216705
				0.25 0.774224400520325
				0.28 0.951132595539093
				0.31 1.15776085853577
				0.34 1.3817971944809
				0.37 1.6569504737854
				0.4 1.84088504314423
			};
			\nextgroupplot[legend to name={CommonLegend},legend style={legend columns=8},title=\texttt{Oblivious Linear Regression}]
			\coordinate (c2) at (current axis.right of origin);
			\addplot [very thick, black,mark=square*]
			table {%
				0.1 0.0248506348580122
				0.13 0.0266600362956524
				0.16 0.0260318480432034
				0.19 0.0289147105067968
				0.22 0.0287831928580999
				0.25 0.0302000977098942
				0.28 0.0302627310156822
				0.31 0.0327312126755714
				0.34 0.0341136306524277
				0.37 0.0361219607293606
				0.4 0.0360446088016033
			};\addlegendentry{SubQuantile}
			\addplot [very thick, green,mark=square*]
			table {%
				0.1 0.176778882741928
				0.13 0.173329889774323
				0.16 0.148729845881462
				0.19 0.14880645275116
				0.22 0.131500840187073
				0.25 0.119577743113041
				0.28 0.102301254868507
				0.31 0.0941185131669044
				0.34 0.0827301070094109
				0.37 0.0734678506851196
				0.4 0.0611478313803673
			};\addlegendentry{SEVER}
			\addplot [very thick, red,mark=square*]
			table {%
				0.1 80.537727355957
				0.13 80.0273361206055
				0.16 78.0828552246094
				0.19 82.857292175293
				0.22 80.457160949707
				0.25 81.2084121704102
				0.28 78.1319122314453
				0.31 80.4724807739258
				0.34 81.4710922241211
				0.37 80.9352798461914
				0.4 76.3578338623047
			};\addlegendentry{TERM}
			\addplot [very thick, orange,mark=square*]
			table {%
				0.1 4.93412590026855
				0.13 7.72555828094482
				0.16 14.3062973022461
				0.19 25.0096282958984
				0.22 38.9553298950195
				0.25 46.8739929199219
				0.28 53.8878555297852
				0.31 63.2952194213867
				0.34 71.4158630371094
				0.37 74.0310897827148
				0.4 79.1682586669922
			};\addlegendentry{Ransac}
			\addplot [very thick, purple,mark=square*]
			table {%
				0.1 2.05954170227051
				0.13 2.18071556091309
				0.16 2.61477184295654
				0.19 2.74768042564392
				0.22 2.72520542144775
				0.25 2.4791693687439
				0.28 3.42575645446777
				0.31 2.23458814620972
				0.34 1.45350193977356
				0.37 2.8192400932312
				0.4 8.48482227325439
			};\addlegendentry{Huber}
			\addplot [very thick, cyan,mark=square*]
			table {%
				0.1 8.94177627563477
				0.13 11.2084474563599
				0.16 13.1594343185425
				0.19 16.7554378509521
				0.22 18.4683818817139
				0.25 21.1728744506836
				0.28 23.1441192626953
				0.31 26.084171295166
				0.34 27.9081382751465
				0.37 30.6380157470703
				0.4 31.126895904541
			};\addlegendentry{ERM}
			\addplot [very thick, blue,mark=square*]
			table { 
				0.1 0.820617198944092
				0.13 1.68926250934601
				0.16 2.87315225601196
				0.19 4.52729034423828
				0.22 6.02502059936523
				0.25 7.91049146652222
				0.28 9.78747940063477
				0.31 12.1807632446289
				0.34 14.0376138687134
				0.37 16.7470684051514
				0.4 17.964750289917
			};\addlegendentry{CRR}
			\addplot [very thick, teal,mark=square*]
			table { 
				0.1 0.130227774381638
				0.13 0.370556622743607
				0.16 0.273394376039505
				0.19 0.312625408172607
				0.22 0.610911905765533
				0.25 0.965445756912231
				0.28 1.35678029060364
				0.31 1.85954880714417
				0.34 2.33366990089417
				0.37 2.99191808700562
				0.4 5.27555418014526
			};\addlegendentry{STIR}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
			{\pgfplotslegendfromname{CommonLegend}};
		\end{tikzpicture}
		\caption{\texttt{Structured Linear Regression} \& \texttt{Noisy Linear Regression} Datasets. Oblivious noise is sampled from $\mathcal{N}(5,5)$, adaptive noise multiplies the labels by $-1$. TERM was ran with open source code, we are unsure why it performed poorly in the synthetic case.}
		\label{fig:synthetic-linear-regression}
	\end{figure}
	We now demonstrate SubQuantile Regression in the presence of Gaussian Random Noise. 
		
	In our first synthetic experiment, we run Algorithm \ref{alg:sqo1} on synthetically generated structured linear regression data, the noise is sampled from a linear distribution that is dependent on the vector of $X$. Our results show the near optimal performance of Subquantile Minimization. The results and comparison with other methods can be seen in Table \ref{tab:quadratic-regression}. We see in Table \ref{tab:quadratic-regression}, Subquantile Minimization produces State of the Art Results in the Quadratic Regression Case. Furthermore, it performs significantly better than baseline methods in the high-noise regimes $(\eps = 0.4)$, this is confirmed in both the small data and large data datasets. Please refer to Appendix \ref{app:experimental-details} for more details on the \texttt{Structured Linear Regression} Dataset. 

	\subsection{Real Data}
	
	We provide results on the \texttt{Drug Discovery} Dataset in \cite{DiakonikolasKKLSS19} utilizing the noise procedure described in \cite{li2020tilted}. For each algorithm, if possible we use Ridge Regression for its robust properties, otherwise we use typical least squares. SubQuantile Minimization, ERM, RANSAC, SEVER, TERM, and SMART are all capable of Ridge Regression. 
	
		\begin{figure}
		\centering
		\pgfplotsset{scaled y ticks=false}
		\begin{tikzpicture}
			\begin{groupplot}[group style={
					group name=myplot,
					group size= 2 by 1},height=6cm,width=0.5\linewidth,
				tick align=outside,
				tick pos=left,
				legend pos=outer north east,
				x grid style={darkgray176},
				xmin=0.1, xmax=0.4,
				xtick style={color=darkgray176},
				y grid style={darkgray176},
				ymin=0.8, ymax=4,
				ytick style={color=darkgray176},
				%ytick={0.1,1,6},
				ymajorgrids=true,
				grid style=dashed,
				ymode=log,
				log basis y={2},
				log ticks with fixed point,
				]
				\nextgroupplot[ylabel=Test RMSE,title=Oblivious Noise]
				\coordinate (c1) at (current axis.left of origin);
				\addplot [very thick, black,mark=square*]
				table {%
					0.1 1.05894804000854
					0.13 1.08117437362671
					0.16 0.981778621673584
					0.19 1.07855093479156
					0.22 0.952977478504181
					0.25 0.95321524143219
					0.28 1.00289475917816
					0.31 1.07451725006104
					0.34 1.00772738456726
					0.37 1.10959303379059
					0.4 1.20861554145813
				};
				\addplot [very thick, green,mark=square*]
				table {%
					0.1 1.09437942504883
					0.13 1.1125750541687
					0.16 1.01856243610382
					0.19 1.10203158855438
					0.22 0.979086935520172
					0.25 0.971916973590851
					0.28 1.01723539829254
					0.31 1.08105278015137
					0.34 0.988381624221802
					0.37 1.08411836624146
					0.4 1.18908596038818
				};
				\addplot [very thick, red,mark=square*]
				table {%
					0.1 1.28610801696777
					0.13 1.26911568641663
					0.16 1.17995369434357
					0.19 1.25433683395386
					0.22 1.1894918680191
					0.25 1.14218485355377
					0.28 1.18342161178589
					0.31 1.21262955665588
					0.34 1.18143332004547
					0.37 1.22365653514862
					0.4 1.34175312519073
				};
				\addplot [very thick, orange,mark=square*]
				table {%
					0.1 1.25890374183655
					0.13 1.42965865135193
					0.16 1.38786888122559
					0.19 1.61200165748596
					0.22 1.76458513736725
					0.25 1.85363411903381
					0.28 2.01117467880249
					0.31 2.25760555267334
					0.34 2.36710119247437
					0.37 2.39467430114746
					0.4 2.74260425567627
				};
				\addplot [very thick, purple,mark=square*]
				table {%
					0.1 1.27051901817322
					0.13 1.43797767162323
					0.16 1.46554577350616
					0.19 1.4952220916748
					0.22 1.49277687072754
					0.25 1.47613513469696
					0.28 1.51563453674316
					0.31 1.63231635093689
					0.34 1.82555913925171
					0.37 2.11563324928284
					0.4 2.01430583000183
				};
				\addplot [very thick, cyan,mark=square*]
				table {%
					0.1 1.34950256347656
					0.13 1.47964179515839
					0.16 1.50942897796631
					0.19 1.78661096096039
					0.22 1.90832722187042
					0.25 1.93754911422729
					0.28 2.07222437858582
					0.31 2.21759843826294
					0.34 2.37727379798889
					0.37 2.51313090324402
					0.4 2.7425274848938
				};
				\addplot [very thick, blue,mark=square*]
				table { 
					0.1 1.14254951477051
					0.13 1.11852920055389
					0.16 1.02382397651672
					0.19 1.17711758613586
					0.22 1.08173727989197
					0.25 1.14027178287506
					0.28 1.36521399021149
					0.31 1.34000027179718
					0.34 1.4935063123703
					0.37 1.72718107700348
					0.4 1.9017117023468
				};
				\addplot [very thick, teal,mark=square*]
				table { 
					0.1 1.0913804769516
					0.13 1.11872482299805
					0.16 1.04714918136597
					0.19 1.20770883560181
					0.22 1.15711236000061
					0.25 1.13065195083618
					0.28 1.2987471818924
					0.31 1.30847465991974
					0.34 1.3973708152771
					0.37 1.58278977870941
					0.4 1.74616122245789
				};
				\nextgroupplot[legend to name={DrugLegend},legend style={legend columns=8},title=Adaptive Noise]
				\coordinate (c2) at (current axis.right of origin);% I moved this to the upper right corner
				\addplot [very thick, black,mark=square*]
				table {%
					0.1 0.919014692306519
					0.13 1.03824925422668
					0.16 1.02291512489319
					0.19 0.97149395942688
					0.22 1.06282556056976
					0.25 1.09392309188843
					0.28 1.13588070869446
					0.31 1.13273966312408
					0.34 1.09808433055878
					0.37 1.37765526771545
					0.4 1.60075807571411
				};\addlegendentry{SubQuantile}
				\addplot [very thick, green,mark=square*]
				table {%
					0.1 0.97418487071991
					0.13 1.08860313892365
					0.16 1.05181241035461
					0.19 1.00930380821228
					0.22 1.08544754981995
					0.25 1.1134250164032
					0.28 1.07825410366058
					0.31 1.08377873897552
					0.34 1.10134482383728
					0.37 1.47154355049133
					0.4 1.8292989730835
				};\addlegendentry{SEVER}
				\addplot [very thick, red,mark=square*]
				table {%
					0.1 1.02624189853668
					0.13 1.12964737415314
					0.16 1.10623836517334
					0.19 1.08269572257996
					0.22 1.145876288414
					0.25 1.16499924659729
					0.28 1.14380311965942
					0.31 1.11899161338806
					0.34 1.05105876922607
					0.37 1.08660757541656
					0.4 1.12682223320007
				};\addlegendentry{TERM}
				\addplot [very thick, orange,mark=square*]
				table {%
					0.1 1.15098297595978
					0.13 1.3056538105011
					0.16 1.39799237251282
					0.19 1.47036576271057
					0.22 1.73723542690277
					0.25 1.85315310955048
					0.28 2.06100988388062
					0.31 2.26056098937988
					0.34 2.41652727127075
					0.37 2.49730658531189
					0.4 2.8336808681488
				};\addlegendentry{Ransac}
				\addplot [very thick, purple,mark=square*]
				table {%
					0.1 1.42236399650574
					0.13 1.44193506240845
					0.16 1.59146368503571
					0.19 1.41443765163422
					0.22 1.76296734809875
					0.25 1.65182590484619
					0.28 2.12984800338745
					0.31 2.21157455444336
					0.34 2.27279591560364
					0.37 2.51102018356323
					0.4 2.22447919845581
				};\addlegendentry{Huber}
				\addplot [very thick, cyan,mark=square*]
				table {%
					0.1 1.1504899263382
					0.13 1.30415141582489
					0.16 1.40286934375763
					0.19 1.46106946468353
					0.22 1.67240214347839
					0.25 1.80807554721832
					0.28 1.90281105041504
					0.31 2.0385856628418
					0.34 2.09466600418091
					0.37 2.21419143676758
					0.4 2.34415864944458
				};\addlegendentry{ERM}
				\addplot [very thick, blue,mark=square*]
				table { 
					0.1 0.988592743873596
					0.13 1.07592439651489
					0.16 1.11025428771973
					0.19 1.15290307998657
					0.22 1.44147706031799
					0.25 1.60702550411224
					0.28 1.67903292179108
					0.31 1.8964900970459
					0.34 1.91637253761292
					0.37 2.25444078445435
					0.4 2.34002900123596
				};\addlegendentry{CRR}
				\addplot [very thick, teal,mark=square*]
				table { 
					0.1 0.992274522781372
					0.13 1.11690032482147
					0.16 1.17126822471619
					0.19 1.13344883918762
					0.22 1.32121670246124
					0.25 1.39682495594025
					0.28 1.50348544120789
					0.31 1.638547539711
					0.34 1.64548587799072
					0.37 1.95285272598267
					0.4 2.04142189025879
				};\addlegendentry{STIR}
			\end{groupplot}
			\coordinate (c3) at ($(c1)!.5!(c2)$);
			\node[below] at (c3 |- current bounding box.south)
			{\pgfplotslegendfromname{DrugLegend}};
		\end{tikzpicture}
		\caption{\texttt{Drug Discovery} Dataset with Normal Noise and Structured Noise. Oblivious noise is sampled from $\mathcal{N}(5,5)$, adaptive noise multiplies the labels by $-1$}
		\label{fig:drug-discovery}
	\end{figure}

	\begin{table}[!h]
		\centering
		\begin{tabular}{lcccc}
			\toprule 
			\textbf{Objectives}&\multicolumn{4}{c}{Test RMSE (\texttt{Drug Discovery})}\\                   
			\cmidrule(rl){2-5}
			&\subhead{$\eps = 0.1$}& \subhead{$\eps = 0.2$}& \subhead{$\eps = 0.3$}& \subhead{$\eps = 0.4$}\\ 
			\midrule
			ERM  &$1.303_{(0.0665)}$&$1.790_{(0.0849)}$&$2.198_{(0.0645)}$&$2.623_{(0.1010)}$\\
			CRR \cite{bhatia2017}  &$1.079_{(0.0899)}$&$1.125_{(0.0832)}$&$1.385_{(0.1372)}$&$1.725_{(0.1136)}$\\
			STIR \cite{pmlr-v89-mukhoty19a} &$1.087_{(0.1256)}$&$1.167_{(0.0750)}$&$1.403_{(0.0987)}$&$1.668_{(0.1142)}$\\
			Robust Risk \cite{RRM} &$1.176_{(0.1110)}
			$&$1.336_{(0.1882)}$&$1.437_{(0.1723)}$&$1.800_{(0.0820)}$\\
			SMART \cite{https://doi.org/10.48550/arxiv.2206.04777} &$1.094_{(0.1065)}$&$1.323_{(0.0758)}$&$1.578_{(0.0799)}$&$1.984_{(0.2020)}$\\
			TERM \cite{li2020tilted} &$\mathbf{1.029_{(0.0707)}}$&$1.126_{(0.0776)}$&$1.191_{(0.1091)}$&$1.201_{(0.1409)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$1.043_{(0.0970)}$&$\mathbf{1.067_{(0.0457)}}$&$\mathbf{1.071_{(0.0807)}}$&$\mathbf{1.138_{(0.1162)}}$\\
			Huber \cite{Huber2009} &$1.412_{(0.0474)}$&$1.501_{(0.2918)}$&$2.231_{(0.9054)}$&$2.247_{(1.0399)}$\\
			RANSAC \cite{RANSAC1981} &$1.238_{(0.0529)}$&$1.643_{(0.1331)}$&$2.092_{(0.1935)}$&$2.679_{(0.1365)}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 1-\eps$) &$\mathbf{0.966_{(0.1119)}}$&$\mathbf{1.002_{(0.1025)}}$&$\mathbf{1.010_{(0.0630)}}$&$\mathbf{1.065_{(0.1112)}}$\\
			\midrule 
			Genie ERM &$0.960_{(0.0845)}$&$0.982_{(0.0842)}$&$1.006_{(0.0879)}$&$1.030_{(0.0578)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Drug Discovery} Dataset. Empirical Risk over $P$ with oblivious noise. The Genie has knowledge of where corruptions are so only trains on clean data in training set.}
		\label{tab:drug-discovery}
	\end{table}
	
	As we can see in Table \ref{tab:drug-discovery}, we obtain state of the art results throughout all noise regimes. This makes our model the strongest among the tested, due to our strength throughout the whole range of noises. This dataset is also 
	
	\section{Conclusion}
	In this work we provide a theoretical analysis for robust linear regression by minimizing the \textit{Subquantile} of the Empirical Risk. Furthermore, we run various numerical experiments and compare against the current State of the Art in Robust Linear Regression. Since minimizing over the subquantile is a general machine learning framework, it is scalable to larger scale machine learning problems. In future work, more real world applications can be explored and the theory can be expanded beyond linear regression. It is also possible to further explore the theorems in this paper to upper bound the number of iterations it takes for convergence of algorithm \ref{alg:sqo-ridge}.
	
	\newpage

	\bibliographystyle{iclr2023_conference}
	\bibliography{iclr2023_conference}
	
	\begin{appendices}
		
	\newpage
	\appendix
	\startcontents[sections]
	\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}
	\newpage
	
	\newgeometry{left=2cm,right=2cm,bottom=2cm}
	
	\section{Linear Algebra and Probability Theory Preliminaries}
	
	\begin{fact} The spectral norm of a matrix, $A$, an $(m \times n)$ matrix, is defined as follows
		\begin{equation}
			\norm{A}_2 = \sqrt{\lambda_{\max}\left(A^T A\right)} = \sigma_{\max}(A)
		\end{equation}
		It similarly follows:
		\begin{equation}
			\norm{A^T A}_2 = \norm{A}^2_2
		\end{equation}
	\end{fact}

	\begin{fact}
		\textbf{Weyl's Inequality} states the following:\\ If $M$, $N$, and $R$ are $n \times n$ Hermitian Matrices with the following eigenvalues where $M = N + R$:
		\begin{equation*}
			M: \mu_1 \geq \cdots \geq \mu_n
		\end{equation*}
		\begin{equation*}
			N: \nu_1 \geq \cdots \geq \nu_n
		\end{equation*}
		\begin{equation*}
			R: \rho_1 \geq \cdots \geq \rho_n
		\end{equation*}
		Then the following equalities hold:
		\begin{equation*}
			\nu_i + \rho_n \leq \mu_i \leq \nu_i + \rho_1 \text { for } i = 1,\dots,n
		\end{equation*}
	\end{fact}

	\begin{fact}
		Let $A$ be a $n \times m$ matrix with $n \gg m$. It then follows:
		\begin{equation}
			A^T A = \left(U \Sigma V^T\right)^T \left(U \Sigma V^T \right) = \left(V\Sigma^T U^T\right)\left(U \Sigma V^T\right) = V \Sigma^T \Sigma V^T = V D V^T
		\end{equation}
		where $D = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2 & & & \\ & \sigma_2^2 & & \\ & & \ddots & \\ & & & \sigma_m^2 \end{pmatrix}$
	\end{fact}

	\begin{fact}
		This is a restatement from \cite{Wackerly2008}.\\
		Let $X_1,\cdots, X_n$ be i.i.d continuous random variables with common distribution function $F(y)$ and common probability density function $f(x)$. If $X_{(k)}$ denotes the $k$th-order statistic, then the density function of $X_{(k)}$ is given by:
		\begin{equation}
			g_{(k)}(x_k) = \frac{n!}{(k-1)!(n-k)!}[F(x_k)]^{k-1}[1 - F(x_k)]^{n-k}f(x_k)
		\end{equation}
	\end{fact}

	\begin{fact}
		The cdf of the $k$th order statistic from a sample of $n$ is:
		\begin{equation}
			F_{(k,n)} = \mathbb{P}\left[X_{(k)}\leq x \right] = \sum_{j=k}^n \begin{pmatrix} n \\ j\end{pmatrix} \left(1 - F(x)\right)^{n-j} F(x)^j
		\end{equation}
	\end{fact}

	\newpage
	\section{Theory for Subquantile Minimization Algorithm \ref{alg:sqo1}}\label{app:general-proofs}
	\subsection{Derivation of Lemma~\ref{lem:gfermat}}
	\label{app:gfermat}
	Since $\displaystyle g(t,\vtheta)$ is a concave function. Maximizing $g(t,\vtheta)$ is equivalent to minimizing $-g(t,\vtheta)$. We will find fermat's optimality condition for the function $\displaystyle -g(t,\vtheta)$, which is convex. 
	Let $\displaystyle \hat{\vnu} = sorted\left((\vtheta^T X - y)^2\right)$ and note $\displaystyle 0 < p < 1$
	\begin{align}
		\partial{\displaystyle (-g(t,\boldsymbol{\theta}})) &= \partial{\left(-t + \frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)} &&\\
		&= \partial{(-t)} + \partial{\left(\frac{1}{np}\sum_{i=1}^{n}(t-\hat{\vnu}_i)^+\right)}&&\\
		&= -1 + \frac{1}{np}\sum_{i=1}^{n}\partial{(t-\hat{\vnu}_i)^+} &&\\
		&= -1 + \frac{1}{np}\sum_{i=1}^{n}
		\left\{
		\begin{array}{lr}
			1, & \text{if } t > \hat{\vnu}_i\\
			0, & \text{if } t < \hat{\vnu}_i \\
			\left[0,1\right], & \text{if } t = \hat{\vnu}_i \\
		\end{array}
		\right\}&&\\
		&= 0 \text{ when }\displaystyle t = \hat{\vnu}_{np}&&
	\end{align}
	This is the $p$-quantile of $\displaystyle \vnu$. Assuming no two points are equal in the dataset, this means the minimizing value for $t$ has a range of values, $\hat{\vnu}_{np} \leq t < \hat{\vnu}_{np+1}$. This means $g(t,\vtheta)$ is not strongly convex with respect to $t$. 
	\subsection{Derivation of Lemma~\ref{lem:gthetaderiv}}
	\label{app:gthetaderiv}
		Note that $t_k = \vnu_{np}$ which is equivalent to $(\vtheta_k^T\vx_{np} - y_{np})^2$
		\begin{align*}
			\nabla_{\vtheta_k} g(t_{k+1},\vtheta_k) &= \nabla_{\vtheta_k}\left(\vnu_{np} - \frac{1}{np}\sum_{i=1}^n(\vnu_{np} - (\vtheta_k^T\vx_i - y_i)^2)^+\right) &&\\
			&= \nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+\right) &&\\
			&= \nabla_{\vtheta_k}(\vtheta_k^T\vx_{np}-y_{np})^2 - \frac{1}{np}\sum_{i=1}^n\nabla_{\vtheta_k}\left((\vtheta_k^T\vx_{np} - y_{np})^2 - (\vtheta_k^T\vx_i - y_i)^2\right)^+ &&\\				
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{n}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) \notag\\ &\qquad-2\vx_i(\vtheta_k^T\vx_i - y_i) \left\{
			\begin{array}{lr}
				1, & \text{if } t > v_i\\
				0, & \text{if } t < v_i \\
				\left[0,1\right], & \text{if } t = v_i \\
			\end{array} \right\} &&\\
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -\frac{1}{np}\sum_{i=1}^{np}2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) -2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\	
			&= 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) - 2\vx_{np}(\vtheta_k^T\vx_{np} - y_{np}) + \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i) &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T\vx_i - y_i)&&
		\end{align*}
		This is the derivative of the $np$ samples with lowest error with respect to $\vtheta$.

	\subsection{Derivation of Lemma~\ref{lem:g-lsmooth}}
	The objective function 
	$\displaystyle g(\vtheta,t)$ is $L$-smooth w.r.t $\vtheta$ iff
	\begin{equation}
		||  \nabla_\vtheta g(\vtheta',t) - \nabla_\vtheta g(\vtheta,t) || \leq L|| \vtheta' - \vtheta || 
	\end{equation}
	\begin{align}
		\norm{ \nabla_{\vtheta} g(\vtheta^{'},t) - \nabla_{\vtheta} g(\vtheta,t) } = &\norm{ \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'\top} \vx_i - y_i) - \frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^T \vx_i - y_i) } &&\\
		= &\norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i(\vtheta_k^{'\top}\vx_i - \vtheta_k^T\vx_i)}&&\\
		= &\norm{\frac{1}{np}\sum_{i=1}^{np}2\vx_i\vx_i^T(\vtheta_k^{'\top} - \vtheta_k^T)} &&\\
		\overset{\mathrm{Cauchy-Schwarz}}{\leq} &\norm{\frac{2}{np}\sum_{i=1}^{np}\vx_i\vx_i^T}\norm{\vtheta_k^{'\top} - \vtheta_k^T} &&\\
		= &L\norm{\vtheta_k^{'\top} - \vtheta_k^T} &&
	\end{align}
	where $\displaystyle L = \norm{\frac{2}{np}X^T X}$\\
	This concludes the derivation.
	\subsection{Proof of Lemma~\ref{lem:g-change-wrt-t}}
	\label{app:g-change-wrt-t}
	\begin{proof}
		We will investigate the two cases $t_{k+1} \leq t$ and $t_{k+1} > t_k$.\\
		\textbf{Case (i)} $t_{k+1} \leq t_k$\\
		Let us first expand out $g(t_k,\vtheta_k)$ with the knowledge that $t_k \geq \hat{\vnu_{k}}$
		\begin{align}
			g(t_k, \vtheta_k) &= t_k - \frac{1}{np}\sum_{i=1}^{n}(t_k-\vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}(np)t_k + \frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&\\
			&= \frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&\\
			g(t_{k+1},\vtheta_k) - g(t_k,\vtheta_k) &=  \frac{1}{np}\sum_{i=1}^{np}\vnu_i - \left(\frac{1}{np}\sum_{i=1}^{np}\vnu_i + \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+\right) &&\\
			&= - \frac{1}{np}\sum_{i=np}^{n}(t_k - \vnu_i)^+ &&
		\end{align}
		\textbf{Case (ii)} $t_{k+1} > t_k$\\
		Since we know $t_k$ is less than $\vnu_{np}$, WLOG we will say $t_k$ is greater than the lowest $n(p-\delta)$ elements, where $\delta \in (0,p)$. 
		\begin{align}
			g(t_k,\vtheta_k) &= t_k - \frac{1}{np}\sum_{i=1}^n(t_k - \vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}\sum_{i=1}^{n(p-\delta)}(t_k - \vnu_i)^+ &&\\
			&= t_k - \frac{1}{np}(n(p-\delta))t_k + \frac{1}{np}\sum_{i=1}^{n(p-\delta)}\vnu_i &&\\
			g(t_k,\vtheta_{k+1}) - g(t_k,\vtheta_k) &= \frac{1}{np}\sum_{i=1}^{np}\vnu_i - \left(\delta t_k + \frac{1}{np}\sum_{i=1}^{n(p-\delta)}\vnu_i\right) &&\\
			&= \left(\frac{1}{np}\sum_{i=n(p-\delta)}^n\vnu_i\right) - \delta t_k &&
		\end{align}
		This concludes the proof.
	\end{proof}
  	
  	\newpage
	\section{Theory for Adaptive Linear Corruption}
	In this section, we provide the conditions for Subquantile Minimization to improve in the case of corruption of the form $\displaystyle \vbeta_Q^T \vmu = y_p + \eps_Q$.
	\begin{assumption}\label{asm:normal-error}
		The residuals of $\vtheta_k$ are normally distributed with respect to $\mathbb{P}$ and $\mathbb{Q}$. In other words, $\vtheta_k^T \vp - \vy_P$ and $\vtheta_k^T \vq - \vy_Q$ are normally distributed.
	\end{assumption}
	Assumption \ref{asm:normal-error} can be visually verified in figure \ref{fig:normal-residual}. Even after multiple iteration steps the residuals with respect to $\mathbb{P}$ and $\mathbb{Q}$ are still normal. Thus it follows by decreasing $\norm{\vtheta - \vbeta_P}_1$ more relative to $\norm{\vtheta - \vbeta_Q}_1$ then the SubQuantile will contain more points from $P$ by expectation. 
	\begin{figure}
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual0.tikz}
			%\caption{Caption for first figure}
			\label{fig:1}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual2.tikz}
			%\caption{Caption for second figure}
			\label{fig:2}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual4.tikz}
			%\caption{Caption for third figure}
			\label{fig:3}
		\end{minipage}
		\hfill
		\begin{minipage}[htbp]{0.24\textwidth}
			\centering
			\input{residual6.tikz}
			%\caption{Caption for fourth figure}
			\label{fig:4}
		\end{minipage}
		\caption{Residuals with respect to $\mathbb{P}$ and $\mathbb{Q}$, $k$ represents optimization step.}
		\label{fig:normal-residual}
	\end{figure}

	\subsection{Proof of Theorem~\ref{thm:improvement-of-theta}}
	\label{app:improvement-of-theta}
	\begin{proof}
		To show the change in $\varepsilon$ we will first calculate the expected change in $\vtheta$ by the $\vtheta$-update described in Equation \ref{eqn:theta-update}. We will also introduce some notation, $\mathcal{S}$ represents all $\vx \in X$ that are within the lowest $np$ losses, i.e. within the subquantile, $\vp \in \mathcal{S}$ represent all data vectors from $\mathbb{P}$ that are within the SubQuantile, similarly $\vq \in \mathbb{Q}$ represent all data vectors from $\mathbb{Q}$ that are within the SubQuantile. Furthermore, $|\mathcal{S}| = np$, there are $\varepsilon np$ points from $\mathbb{Q}$ in $\mathcal{S}$ and $(1-\varepsilon)np$ points from $\mathbb{P}$ within $\mathcal{S}$.  We assume $\vp \sim \mathcal{N}(\vzero, \Sigma_P)$ and $\vq \sim \mathcal{N}(\vzero, \Sigma_Q)$ where $\Sigma_P = \xi_P I$ and $\Sigma_Q = \xi_Q I$ where $\xi_P$ and $\xi_Q$ are greater than $0$, then we can assume it follows $\displaystyle \vmu\vmu^T = \vzero\vzero^T = 0$. 
		\begingroup
		%\addtolength{\jot}{0.5em}
		\begin{align*}
			\mathbb{E}\left[\vtheta_{k+1}\right] &= \vtheta_k - \mathbb{E}\left[\alpha \nabla g(\vtheta_k,t_{k+1})\right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vx \in \mathcal{S}}\vx(\vtheta^T\vx - y) \right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vx \in \mathcal{S}}\vx\vx^T\vtheta_k - \vx y \right] &&\\
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vp \in \mathcal{S}}\vp\vp^T\vtheta_k - \vp y_p + \sum_{\vq \in \mathcal{S}} \vq\vq^T\vtheta_k - \vq y_q \right] &&
			\intertext{We will use Assumption \ref{asm:normal-corruption} to rewrite $y_p$ and $y_q$}
			&= \vtheta_k - \alpha \mathbb{E}\left[\sum_{\vp \in \mathcal{S}}\vp\vp^T\vtheta_k - \vp (\vbeta_P \vp + \eps_P) + \sum_{\vq \in \mathcal{S}} \vq\vq^T\vtheta_k - \vq (\vbeta_Q \vp + \eps_Q) \right] &&\\
			&= \vtheta_k - \alpha\left(\sum_{\vp \in \mathcal{S}} \left(\vmu \vmu^T + \Sigma_P\right)\vtheta_k - \left(\vmu \vmu^T + \Sigma_P\right)\vbeta_P + \sum_{\vq \in \mathcal{S}} \left(\vmu \vmu^T + \Sigma_Q\right)\vtheta_k - \left(\vmu \vmu^T + \Sigma_Q\right)\vbeta_Q\right) &&
		\end{align*}
		 \vspace{1em}
		\begin{equation}\vspace{1em}
			\mathbb{E}\left[\vtheta_{(t+1)}\right]= \vtheta_{(t)} - \alpha np \left( (1-\varepsilon^{(t)})\xi_P(\vtheta_{(t)} - \vbeta_P ) + \varepsilon^{(t)}\xi_Q(\vtheta_{(t)} - \vbeta_Q)\right) 
		\end{equation}
		
		\endgroup
		Now that we have the expected update for $\vtheta$ in terms of the linear regression coefficients, we now want to utilize Lemma \ref{lem:effect-of-projection}.\\
		Let $\displaystyle \vtheta_{(t)} = \alpha_1^{(t)}\vbeta_P + \alpha_2^{(t)}\vbeta_Q + \sum_{i=3}^{d}\alpha_i^{(t)} \vr_i$ in the same basis $\mB$ defined in Lemma \ref{lem:effect-of-projection}.Let $\gamma \triangleq \alpha np$. Then the following manipulations hold:
		\begin{align*}
			\mathbb{E}\left[\vtheta_{(t+1)}\right] &= \vtheta_{(t)} - \gamma \left( (1-\varepsilon^{(t)})\xi_P(\vtheta_{(t)} - \vbeta_P ) + \varepsilon^{(t)}\xi_Q(\vtheta_{(t)} - \vbeta_Q)\right) &&\\
			&= \vtheta_{(t)}\left(1 - \gamma\left(\left(1 - \varepsilon^{(t)}\right)\xi_P + \varepsilon^{(t)}\xi_Q\right)\right) + \gamma \left(1 - \varepsilon^{(t)}\right)\xi_P\vbeta_P + \gamma \varepsilon^{(t)} \xi_Q \vbeta_Q&&\\
			&= \left(1 - \gamma\left(\left(1 - \varepsilon^{(t)}\right)\xi_P + \varepsilon^{(t)}\xi_Q\right)\right)\left(\alpha_1^{(t)} \vbeta_P + \alpha_2^{(t)} \vbeta_Q + \sum_{i=3}^d\alpha_i^{(t)}\vr_i\right) + \gamma \left(1 - \varepsilon^{(t)}\right)\xi_P\vbeta_P + \gamma \varepsilon^{(t)} \xi_Q \vbeta_Q &&
		\end{align*}
		To simplify the notation, let us define the constant for this iteration $\displaystyle \Xi^{(t)} \triangleq \left(1 - \gamma\left(\left(1 - \varepsilon^{(t)}\right)\xi_P + \varepsilon^{(t)}\xi_Q\right)\right)$ 
		\begin{align*}
			&= \left(\alpha_1^{(t)}\Xi^{(t)} + \gamma\left(1-\varepsilon^{(t)}\right)\xi_P\right)\vbeta_P + \left(\alpha_2^{(t)}\Xi^{(t)} + \gamma \varepsilon^{(t)}\xi_Q\right)\vbeta_Q + \Xi^{(t)}\sum_{i=3}^d\alpha_i^{(t)}\vr_i &&
		\end{align*}
		We will now calculate the difference. 
		\begin{align*}
			\mathbb{E}\left[\vtheta_{(t+1)} - \vtheta_{(t)}\right] &= \left(\alpha_1^{(t)}\left(\Xi^{(t)}-1\right) + \gamma\left(1-\varepsilon^{(t)}\right)\xi_P\right)\vbeta_P + \left(\alpha_2^{(t)}\left(\Xi^{(t)}-1\right) + \gamma \varepsilon^{(t)}\xi_Q\right)\vbeta_Q + \left(\Xi^{(t)}-1\right)\sum_{i=3}^d\alpha_i^{(t)}\vr_i&&\\
		\end{align*}
		Thus the conditions for $\varepsilon$ to decrease by expectation are:
		\begin{equation}
			\norm{\left(\alpha_1^{(t)}\left(\Xi^{(t)}-1\right) + \gamma\left(1-\varepsilon^{(t)}\right)\xi_P\right)\vbeta_P} > \norm{\left(\alpha_2^{(t)}\left(\Xi^{(t)}-1\right) + \gamma \varepsilon^{(t)}\xi_Q\right)\vbeta_Q}
		\end{equation}  
		This concludes the proof. Note we are not interested in the change on the vectors $\vr_3, \dots, \vr_d$ as they do not have an effect on the projection of $\vtheta$ onto $\vbeta_P$ and $\vbeta_Q$. 
	\end{proof}

	\newpage
	
	\section{Proofs for Convergence}
	\label{app:convergence-proofs}
	\subsection{Proof of Theorem ~\ref{thm:convergence-guarantee}}
	\begin{proof}
	We will first start by introducing new notation. Let $S$ represent a matrix with $np$ data points from $X$, in other words it is a possible SubQuantile Matrix. Let $\displaystyle\vPi$ represent the set of all such possible matrices $S$ of $X$. Note $\displaystyle \left|\vPi\right| = \binom{n}{np}$. We can now redefine the min-max optimization problem of $g$ to a min-min optimization problem. Let us define the function $f(\vtheta, S) = \norm{\vtheta^T S - y_S}_2^2$
	\begin{equation}
		\vtheta^*, S^* = \argmin_{\vtheta \in \mathbb{R}^d} \argmin_{S \in \vPi} \norm{\vtheta^T S - y_S}_2^2
	\end{equation}
	Note we have a $\mathcal{O}(n)$ time-complexity oracle for the $\displaystyle \argmin_{S\in \vPi}f(\vtheta_T,S_T)$. 
	\begin{restatable}{lemma}{unique-S-minimizer}
		\label{lem:unique-S-minimizer}
		The resultant $\displaystyle \widetilde{S} = \argmin_{S \in \vPi} f(\vtheta,S)$ is a unique minimizer iff all points in $X$ are different. 
	\end{restatable}
	We will now show $f$ is a monotonically decreasing function.\\
	First let us define $\phi(\cdot) = \min_{S \in \vPi}\left(\cdot,S\right)$. 
	Let us also note $f$ is $\ell$ smooth with respect to $\vtheta$. This is following notation from \cite{Jin_2019}.It thus follows:
	\begin{align*}
		f(\vtheta_{k+1}, S_k) &\leq f(\vtheta_k, S_k) + \langle\nabla_\vtheta f(\vtheta_k,S_k), \vtheta_{k+1}-\vtheta_k \rangle + \frac{\ell}{2}\norm{\vtheta_{k+1}-\vtheta_k}_2^2 &&\\
		&= \phi(\vtheta_k) + \langle\nabla_\vtheta f(\vtheta_k,S_k), -\frac{1}{\ell}\nabla_\vtheta f(\vtheta_k,S_k)\rangle + \frac{\ell}{2}\norm{\frac{1}{\ell}\nabla_\vtheta f(\vtheta_k,S_k)}_2^2 &&\\
		&= \phi(\vtheta_k) - \frac{1}{\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 + \frac{1}{2\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 &&\\
		&= \phi(\vtheta_k) - \frac{1}{2\ell}\left(\nabla_\vtheta f(\vtheta_k,S_k)\right)^2 &&\\
	\end{align*} 
	Thus we have proved the inner optimization problem is monotonically decreasing. Since the outer minimization is strictly less than or equal to the result from the inner optimization, it follows after each two step optimization: 
	\begin{align*}
		\phi(\vtheta_{k+1}) \leq \phi(\vtheta_k)
	\end{align*}
	
	Since $f$ is lower bounded by $0$. We can invoke Montonicity Convergence Theorem, since $f$ is a monotically decreasing function and is lower bounded, it therefore converges to either a local or global minimum. 
	\end{proof}
	
	\subsection{Proof of Theorem~\ref{thm:expectation-of-improvement}}
	\begin{proof}
	Recall $\varepsilon^{(t)}$ represents the ratio of points of $Q$ within the subquantile matrix at iteration $t$ and $\varepsilon^{(t+1)}$ represents the ratio of points of $Q$ within the subquantile matrix at iteration $t+1$. If $\varepsilon^{(t)} = \varepsilon^{(t+1)}$, then we would expect $0$ improvement.
	\begin{align*}
		\mathbb{E}&\left[f\left(\vtheta_{(k)}, S^{(k+1)}\right) - f\left(\vtheta_{(k)}, S^{(k)}\right)\right] = \mathbb{E}\left[\sum_{i=1}^{n(\varepsilon^{(t)}-\varepsilon^{(t-1)})}\norm{\vtheta_{(k)}^T\vp_i -y_i}_2^2 - \norm{\vtheta_{(k)}^T\vq_i - y_i}_2^2\right] &&\\
		&= \mathbb{E}\left[\sum_{i=1}^{n(\varepsilon^{(t)}-\varepsilon^{(t-1)})}\norm{\vtheta_{(k)}^T\vp_i -y_i}_2^2 - \norm{\vtheta_{(k)}^T\vq_i - y_i}_2^2\right] &&
	\end{align*}
	We can now use reverse triangle inequality.
	\begin{align*}
		&\leq  \mathbb{E}\left[\sum_{i=1}^{n\left(\varepsilon^{(t)}-\varepsilon^{(t-1)}\right)}\norm{\vtheta_{(k)}^T\vp_i - \vbeta_P\vp_i - \eps_P - \vtheta^T_{(k)}\vq_i + \vbeta_Q\vq_i + \eps_Q}_2^2 \right] &&\\
		&=  \mathbb{E}\left[\sum_{i=1}^{n\left(\varepsilon^{(t)}-\varepsilon^{(t-1)}\right)}\norm{\left(\vtheta^T_{(k)} - \vbeta_P\right)\vp_i + \left(\vbeta_Q - \vtheta^T_{(k)}\right)\vq_i - \eps_P+ \eps_Q}_2^2 \right] &&\\
		&\leq \mathbb{E}\left[\sum_{i=1}^{n(\varepsilon^{(t)} - \varepsilon^{(t-1)})}\norm{\left(\vtheta_{(k)}^T - \vbeta_P^T\right)\vp_i}_2^2 + \norm{\left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\vq_i}_2^2 + \norm{\eps_P} + \norm{\eps_Q}\right]
	\end{align*}
	Note that $\mathbb{E}\left[X^2\right] = \mathbb{E}\left[X\right]^2 + \Var(X)$ for a random variable $X$ then:
	\begin{align*}
		&\leq \sum_{i=1}^{n(\varepsilon^{(t)} - \varepsilon^{(t-1)})} \left(\left(\vtheta_{(k)}^T - \vbeta_P^T\right)\mathbb{E}\left[\vp_i\right]\right)^2 + \left(\left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\mathbb{E}\left[\vq_i\right]\right)^2 + \left(\vtheta_{(k)}^T - \vbeta_P^T\right)\Var\left(\vp_i\right)\left(\vtheta_{(k)} - \vbeta_P\right) \notag\\ &\qquad+ \left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\Var(\vq_i)\left(\vtheta_{(k)} - \vbeta_Q\right) &&\\
		&= \sum_{i=1}^{n(\varepsilon^{(t)} - \varepsilon^{(t-1)})} \left(\left(\vtheta_{(k)}^T - \vbeta_P^T\right)\mathbb{E}\left[\vp_i\right]\right)^2 + \left(\left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\mathbb{E}\left[\vq_i\right]\right)^2 + \left(\vtheta_{(k)}^T - \vbeta_P^T\right)\Sigma_P\left(\vtheta_{(k)} - \vbeta_P\right) \notag\\ &\qquad+ \left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\Sigma_Q\left(\vtheta_{(k)} - \vbeta_Q\right)
	\end{align*}
	If we assume the data is centered around $\vzero$, then it simplifies to the following:
	\begin{equation}
		= n\left(\varepsilon^{(t)} - \varepsilon^{(t-1)}\right) \left(\left(\vtheta_{(k)}^T - \vbeta_P^T\right)\Sigma_P\left(\vtheta_{(k)} - \vbeta_P\right) + \left(\vtheta_{(k)}^T - \vbeta_Q^T\right)\Sigma_Q\left(\vtheta_{(k)} - \vbeta_Q\right)\right)
	\end{equation}
	Furthermore, since we typically assume linear independence in the data, let $\Sigma_P = \xi_P I$ and $\Sigma_Q = \xi_Q I$ where $\xi_P$ and $\xi_Q$ are constants greater than $0$.
	\begin{equation}
		= n\left(\varepsilon^{(t)} - \varepsilon^{(t-1)}\right) \left(\xi_P\norm{\vtheta_{(k)}^T - \vbeta_P^T}^2_2 + \xi_Q\norm{\vtheta_{(k)}^T - \vbeta_Q^T}^2_2\right)
	\end{equation}
	\end{proof}
	\newpage
	
	\section{Theory for Ridge Regression Algorithm \ref{alg:sqo-ridge}}\label{app:Subquantile-optimization}
	\subsection{Proof of Theorem~\ref{thm:bounded-from-optimal}}\label{app:expected-value-corrupted}
	\begin{proof}
		\begin{assumption}\label{asm:normal-sampling}
			The rows of $P$ and $Q$ are sampled from $\vzero$ centered Normal Distributions.
			\begin{equation*}
				P_i \sim \mathcal{N}\left(\vzero, \Sigma_P\right)
			\end{equation*}
			\begin{equation}
				Q_i \sim \mathcal{N}\left(\vzero, \Sigma_Q\right)
			\end{equation}
		\end{assumption}
		
		\begin{assumption}\label{asm:wishart}
			By assumption \ref{asm:normal-sampling}, it thus follows that the matrices $P^T P$ and $Q^T Q$ are sampled from Wishart Distributions.
			\begin{equation}
				P^T P \sim \mathcal{W}\left(n,\Sigma_P\right)
			\end{equation}
			\begin{equation}
				Q^T Q \sim \mathcal{W}\left(n, \Sigma_Q\right)
			\end{equation}
		\end{assumption}
		
		\begin{assumption}\label{asm:identity-covariance}
			Similar to the assumption made in \cite{bhatia2017}, to give theoretical bounds on the our algorithm, we assume the following:
			\begin{equation}
				\Sigma_P = \xi_P I
			\end{equation}
			\begin{equation} 
				\Sigma_Q = \xi_Q I
			\end{equation}
			where $\xi_P,\xi_Q \geq 0$ 
		\end{assumption}
		The closed form solution for Ridge Regression with regularization parameter $\lambda$ is equal to the following:
		\begin{equation}
			\hat{\vbeta} = \left(X^T X+ \lambda I\right)^{-1}X^T y
		\end{equation}
		We will use this to bound the difference of the $\vbeta_P$ and $\vtheta_{(k)}$.
		\begin{align*}
			\norm{\vbeta_P - \vtheta_{(k)}}_2 &= \norm{\vbeta_P - \left(S^T S + \lambda I\right)^{-1}S^T \vy}_2 &&\\
		\end{align*}
		Note the subquantile matrix $S$, consists of data points from $P$ and $Q$, we will reorganize $S$ into the following:
		\begin{equation*}
			S = \begin{pmatrix}P \\ Q \end{pmatrix} = 
			\begin{pmatrix}
				\leftarrow & \vp_1 & \rightarrow \\
				\vdots & \vdots & \vdots \\
				\leftarrow & \vp_{n(1-\varepsilon^{(t)})} & \rightarrow\\
				\leftarrow & \vq_1 & \rightarrow \\
				\vdots & \vdots & \vdots \\
				\leftarrow & \vq_{n\varepsilon^{(t)}} & \rightarrow
			\end{pmatrix}\text{ and } \vy = \begin{pmatrix}\vy_P \\ \vy_Q \end{pmatrix} = \begin{pmatrix} \vbeta_P\vp_1 + \eps_P \\ \vdots \\ \vbeta_P\vp_{(n(1-\varepsilon^{(t)}))} + \eps_P \\ \vbeta_Q\vq_1 + \eps_Q\\ \vdots \\ \vbeta_Q\vq_{n\varepsilon^{(t)}} + \eps_Q\end{pmatrix}
		\end{equation*}
		Let us also assume $\Var(\eps_P) = \eta_P$ and $\Var(\eps_Q) = \eta_Q$. Then we can make the following manipulations:
		\begin{align*}
			= &\norm{\vbeta_P - \left(P^T P + Q^T Q + \lambda I\right)^{-1}\begin{pmatrix}P^T Q^T\end{pmatrix}\begin{pmatrix}\vy_P \\ \vy_Q \end{pmatrix}}_2 &&\\
			= &\norm{\left(P^T P + \lambda I\right)^{-1} P^T \vy_P - \left(P^T P + Q^T Q + \lambda I\right)^{-1}P^T \vy_P  - \left(P^T P + Q^T Q + \lambda I\right)^{-1}Q^T \vy_Q}_2 &&\\
			\leq &\norm{\left(P^T P + \lambda I\right)^{-1} P^T \vy_P -\left(P^T P + Q^T Q+ \lambda I\right)^{-1}P^T \vy_P}_2 + \norm{\left(P^T P + Q^T Q+ \lambda I\right)^{-1}Q^T \vy_Q}_2 &&\\
			\leq& \norm{\left(P^T P + \lambda I\right)^{-1}}_2 \norm{P^T\vy_P}_2 + \norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{P^T \vy_P}_2 + \norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{Q^T \vy_Q}_2&&\\
			\leq & \sqrt{\lambda_{\max}\left(P^T P\right)}\left(\norm{\left(P^TP + \lambda I\right)^{-1}}_2+\norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\right)\norm{\vy_P}_2 + \sqrt{\lambda_{\max}\left(Q^T Q\right)}\norm{\left(P^T P + Q^T Q + \lambda I\right)^{-1}}_2\norm{\vy_Q}_2 &&\\
			= &\frac{\sigma_{\max}(P)\norm{\vy_P}}{\sqrt{\lambda_{\max}(P^TP + \lambda I)}} + \frac{\sigma_{\max}(P)\norm{\vy_P}_2}{\sqrt{\lambda_{\max}\left(P^T P + Q^T Q + \lambda I\right)}} + \frac{\sigma_{\max}(Q)\norm{\vy_Q}_2}{\sqrt{\lambda_{\max}\left(P^T P + Q^T Q + \lambda I\right)}} &&\\
			\overset{\mathrm{(a)}}{\leq} &\frac{2\sigma_{\max}(P)\norm{P\vbeta_P + \veps_P}_2 + \sigma_{\max}{Q}\norm{Q\vbeta_Q + \veps_Q}_2}{\sqrt{\lambda_{\max}\left(P^T P + \lambda I\right)}} &&\\
			\overset{\mathrm{(b)}}{\leq} & \frac{2\sigma^2_{\max}(P)\norm{\vbeta_P} + 6\sigma_{\max}(P)n(1-\varepsilon^{(t)})\eta_P + \sigma^2_{\max}(Q)\norm{\vbeta_Q} + 3\sigma_{\max}(Q)n\varepsilon^{(t)}\eta_Q}{\sqrt{\lambda_{\max}(P^T P) + \lambda_{\min}(\lambda I)}} &&\\
			\leq &\frac{2\sigma^2_{\max}(P)\norm{\vbeta_P} + 6\sigma_{\max}(P)n(1-\varepsilon^{(t)})\eta_P + \sigma^2_{\max}(Q)\norm{\vbeta_Q} + 3\sigma_{\max}(Q)n\varepsilon^{(t)}\eta_Q}{\sqrt{\sigma^2_{\max}(P) + \lambda}} &&\\
		\end{align*}
		
		(a) is due to $Q^TQ$ being a positive semi definite symmetric matrix. \\ 
		(b) holds with high probability due to the variance of the $\chi^2$ distribution and due to Weyl's inequality\\
		
		Thus we have shown that $\norm{\vbeta_P - \vtheta_{(t)}}_2$ is bounded above at any time-step $(t)$ in terms of the maximal singular values of the data matrices and the variance of the white noise. 
		
		This concludes the proof.
		
	\end{proof}
	
	\subsection{Derivation for Lemma~\ref{thm:probability-of-hard-convergence}}
	Let us note the Subquantile Matrix $\displaystyle S = \begin{pmatrix} P \\ Q \end{pmatrix} = \begin{pmatrix} \leftarrow & \vp_1 & \rightarrow \\ \vdots & \vdots & \vdots \\ \leftarrow & \vp_{\eta_P} & \rightarrow \\ \leftarrow & \vq_1 & \rightarrow \\ \vdots & \vdots & \vdots \\ \leftarrow & \vq_{\eta_Q} & \rightarrow\end{pmatrix}$. Thus we can define: $\displaystyle \varepsilon^{(t)} \triangleq \frac{\eta_Q}{\eta_P + \eta_Q} = \frac{\eta_Q}{np}$. Where $n$ is the number of training examples and $p$ is the Subquantile we are minimizing over. In this section, we want to provide a theoretical upper bound on $\eta_Q$, from where we can upper bound $\varepsilon^{(t)}$ which is stronger than the trivial upper bound of $\displaystyle \mathcal{O}\left(\epsilon/p\right)$. We will approach this problem with Order Statistics.
	\begin{assumption}
		The optimal regressors are linearly independent, i.e. $\vbeta_P \neq \gamma \vbeta_Q\, \forall \gamma \in \mathbb{R}$
	\end{assumption}
	Let us denote $P_1 < P_2 < \dots < P_{n(1 - \epsilon)}$ as the order statistics of the random variable $P \sim \left(\vp_i \vtheta - (\vp_i \vbeta_P + \epsilon_P)\right)^2$ where $ \vp_i \sim \mathcal{N}_d(\vzero, \xi_P I)$. Let us also denote $Q_1 < Q_2 < \dots < Q_{n(1 - \epsilon)}$ as the order statistics of the random variable $Q \sim \left(\vq_i \vtheta - (\vp_i \vbeta_Q + \epsilon_Q)\right)^2$ where $ \vq_i \sim \mathcal{N}_d(\vzero, \xi_P I)$.\\
	First we will formalize the CDF of $P$ and $Q$. Note $\vp_i$ and $\vq_i$ represent the normally sampled gaussian data. 
	\begin{align}
		P_i = &\left(\vp_i\vtheta_{(t)} - (\vp_i \vbeta_P + \eps_P)\right)^2 &&\\
		= &\left(\vp_i(\vtheta_{(t)} - \vbeta_P) - \eps_P\right)^2 &&\\
		= &\left(\vp_i(\vtheta_{(t)} - \vbeta_P)\right)^2 - 2 \eps_P \left(\vp_i(\vtheta_{(t)} - \vbeta_P)\right) + \eps_P^2 &&
	\end{align}
	As $\mathbb{E}\left[\epsilon_P\right] = 0$, we will only consider the case $\epsilon_P = 0$. This simplifies $P_i$ and $Q_i$:
	\begin{equation}
		P_i = \left(\mP_i(\vtheta_{(t)} - \vbeta_P)\right)^2 
	\end{equation}
	\begin{equation}
		Q_i = \left(\mQ_i(\vtheta_{(t)} - \vbeta_Q)\right)^2
	\end{equation}
	Let us note all the entries of $\mP_i$ are sampled from $\mathcal{N}(0,\xi_P)$ and all entries of $\mQ_i$ are sampled from $\mathcal{N}(0,\xi_Q)$.
	It thus follows:
	\begin{align*}
		\left(\mP_i(\vtheta_{(t)} - \vbeta_P)\right)^2 &= \left(\sum_{j=1}^d\mathcal{N}(0,\xi_P)\left(\theta^{(t)}_j - \beta_{Pj}\right)\right)^2 &&\\
		&= \left(\sum_{j=1}^d \mathcal{N}\left(0,\xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2\right)\right)^2  &&\\
		&= \left(\mathcal{N}\left(0, \sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2\right)\right)^2
	\end{align*}
	It thus similarly follows for $Q_i$:
	\begin{equation*}
		\left(\mQ_i(\vtheta_{(t)} - \vbeta_Q)\right)^2 = \left(\mathcal{N}\left(0, \sum_{j=1}^d \xi_Q\left(\theta_j^{(t)} - \beta_{Qj}\right)^2\right)\right)^2
	\end{equation*}
	Now we can define the cumulative distribution functions.
	\begin{align}
		F_P(z) &= \frac{1}{\sqrt{2\pi}}\int_{-\sqrt{z}}^{\sqrt{z}}\exp\left(-\frac{u^2}{2\sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2}\right)du &&\\
		&= \Phi\left(\frac{\sqrt{z}}{\sqrt{\sum_{j=1}^d \xi_P\left(\theta_j^{(t)}-\beta_{Pj}\right)^2}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\sum_{j=1}^d \xi_Q\left(\theta_j^{(t)}-\beta_{Pj}\right)^2}}\right) &&
	\end{align}
	Let us define $\displaystyle \phi = \sum_{j=1}^d \xi_P\left(\theta_j^{(t)} - \beta_{Pj}\right)^2$ and $\displaystyle \psi = \sum_{j=1}^d \xi_Q\left(\theta_j^{(t)} - \beta_{Qj}\right)^2$
	Therefore it follows:
	\begin{equation}
		F_P(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\phi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\phi}}\right)
	\end{equation}
	Similarly for $F_Q(z)$ it follows:
	\begin{equation}
		F_Q(z) = \Phi\left(\frac{\sqrt{z}}{\sqrt{\psi}}\right) - \Phi\left(\frac{-\sqrt{z}}{\sqrt{\psi}}\right)
	\end{equation}
	where $\Phi$ represents the CDF for the standard normal.
	Here we can note if $\phi < \psi$, then for all $z > 0$, it follows $F_P(z) > F_Q(z)$.
	
	Let us first note $F_P(z)$ is equal to the $\chi^2$ CDF with $1$ degree of freedom. Therefore $f_P(z)$ is equal to the PDF of the $\chi^2$ distribution $1$ degree of freedom. 
	
	We will first consider a simple case, calculating the probability $\varepsilon^{(t)} = 0$, i.e., the points in the subquantile are all the points in $P$, and there are no points from $Q$ within the subquantile.
	
	To simplify notation, let $\ell \triangleq n(1-\epsilon)$ which represents number of points from $P$ in the data matrix, $X$, and let $m \triangleq n(\epsilon)$ represent the number of points from $Q$ in the data matrix, $X$. We define $F_{P(\ell)}$ represent the CDF of the $\ell$th-order statistic of $P$ and $F_{Q(m)}$ represent the CDF of the $m$th-order statistic of $Q$. Finally, let $P_{(i)}$ represent the $i$th order statistic of $P$, in other words, it represents the $i$th highest error among the data points in $P$ with respect to $\vtheta$, similarly let $Q_{(j)}$ represent the $j$th order statistic of $Q$. 
	
	We will first calculate the probablity there exists $0$ points from $Q$ within the subquantile. This is equivalent to:
	\begin{align}
		\mathbb{P}\left[\bigcap_{i=1}^m Q_{(i)} < P_{(\ell)}\right] &= F_{Q(1)}\left(P_{(\ell)}\right)
	\end{align}

	The joint cumulative distribution function of this distribution is equivalent to:
	\begin{align*}
		H(x) &= \ell f_P(x) \left(\prod_{i=1}^{\ell-1} \mathbb{P}\left[P < x\right]\right) \left(\prod_{j=1}^{m}\mathbb{P}\left[Q > x\right]\right) &&\\
		&= \ell f_P(x) \left(F_P(x)\right)^{\ell-1}\left(1 - F_Q(x)\right)^{m} &&\\
	\end{align*}
	In a future work we will provide a tight lower bound on this probability. 
	
	\newpage
	
	\section{Additional Experiments}
	\label{app:additional-experiments}
	\subsection{\texttt{Quadratic Regression}}
	\begin{table}[!h]
	\centering
	\begin{tabular}{lccc}
		\toprule 
		\textbf{Objectives}&\multicolumn{3}{c}{Test RMSE (\texttt{Quadratic Regression})}\\                   
		\cmidrule(rl){2-4}
		&\subhead{$\eps = 0$}& \subhead{$\eps = 0.2$}& \subhead{$\eps = 0.4$}\\ 
		\midrule
		ERM  &$0.0099_{(0.0002)}$&$2.078_{(0.146)}$&$4.104_{(0.442)}$\\
		Huber \cite{Huber2009} &$1.000_{(0.0002)}$&$1.000_{(0.0003)}$&$1.13_{(0.087)}$\\
		RANSAC \cite{RANSAC1981} &$0.010_{(0.0002)}$&$0.011_{(0.0002)}$&$0.061_{(0.053)}$\\
		TERM \cite{li2020tilted} &$0.010_{(0.0001)}$&$0.012_{(0.0008)}$&$0.017_{(0.0016)}$\\
		SEVER \cite{DiakonikolasKKLSS19} &$0.0166_{(0.007)}$&$0.011_{(0.0004)}$&$0.0267_{(0.036)}$\\
		\rowcolor{LightCyan}
		SubQuantile($p = 0.6$) &$\mathbf{0.0099_{(0.0002)}}$&$\mathbf{0.00998_{(0.0002)}}$&$\mathbf{0.010_{(0.0001)}}$\\
		\midrule 
		Genie ERM &$0.0099_{(0.0002)}$&$0.00997_{(0.0002)}$&$0.010_{(0.0001)}$\\
		\bottomrule
	\end{tabular}
	\caption{\texttt{Quadratic Regression} Synthetic Dataset. Empirical Risk over $\mathbb{P}$}
	\label{tab:quadratic-regression}
	\end{table}

	\subsection{\texttt{Abalone}}
	We now provide results on \texttt{Abalone} Dataset introduced in \cite{Dua2019}.
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcc}
			\toprule 
			\textbf{Objectives}&\multicolumn{2}{c}{Test RMSE (\texttt{Abalone Linear Regression})}\\                   
			\cmidrule(rl){2-3}
			&\subhead{Clean}& \subhead{Noisy}\\ 
			\midrule
			ERM  &$2.213_{(0.0528)}$&$4845.335_{(117.5557)}$\\
			CRR \cite{bhatia2017}  &$2.345_{(0.0430)}$&$396.872_{(96.5632)}$\\
			STIR \cite{pmlr-v89-mukhoty19a}  &$2.240_{(0.0473)}$&$931.845_{(32.0864)}$\\
			Huber \cite{Huber2009} &$5.535_{(0.0665)}$&$971.362_{(28.8863)}$\\
			RANSAC \cite{RANSAC1981} &$2.522_{(0.1407)}$&$2.621_{(0.1719)}$\\
			TERM \cite{li2020tilted} &$10.686_{(0.2616)}$&$10.853_{(0.4245)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$\mathbf{2.238_{(0.0901)}}$&$\mathbf{2.287_{(0.0757)}}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 0.8$) &$\mathbf{2.292_{(0.0413)}}$&$\mathbf{2.261_{(0.0790)}}$\\
			\midrule 
			Genie ERM &$2.213_{(0.0528)}$&$2.238_{(0.0901)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Abalone Regression} Real Dataset. Empirical Risk over $\mathbb{P}$}
		\label{tab:abalone-regression}
	\end{table}
	This experiment has both feature and label noise in the Noisy Data. SubQuantile minimization no longer always converges to the $\mathbb{P}$ SubQuantile.
	
	\subsection{\texttt{Cal-Housing}}
	We now provide results on \texttt{Cal-Housing} Dataset introduced in \cite{Kelley1997}.
	\begin{table}[!h]
		\centering
		\begin{tabular}{lcc}
			\toprule 
			\textbf{Objectives}&\multicolumn{2}{c}{Test RMSE (\texttt{Cal-Housing Linear Regression})}\\                   
			\cmidrule(rl){2-3}
			&\subhead{Clean}& \subhead{Noisy}\\ 
			\midrule
			ERM  &$0.598_{(0.0077)}$&$81.758_{(2.6230)}$\\
			CRR \cite{bhatia2017}  &$\mathbf{0.602_{(0.0081)}}$&$75.777_{(2.9403)}$\\
			STIR \cite{pmlr-v89-mukhoty19a}  &$\mathbf{0.604_{(0.0070)}}$&$65.555_{(2.1899)}$\\
			Huber \cite{Huber2009} &$\mathbf{0.601_{(0.0077)}}$&$71.813_{(2.0755)}$\\
			RANSAC \cite{RANSAC1981} &$0.681_{(0.0389)}$&$0.679_{(0.0253)}$\\
			TERM \cite{li2020tilted} &$0.737_{(0.0070)}$&$0.625_{(0.0083)}$\\
			SEVER \cite{DiakonikolasKKLSS19} &$0.640_{(0.0067)}$&$0.642_{(0.0088)}$\\
			\rowcolor{LightCyan}
			SubQuantile($p = 0.9$) &$\mathbf{0.615_{(0.0076)}}$&$\mathbf{0.612_{(0.0096)}}$\\
			\midrule 
			Genie ERM &$0.598_{(0.0077)}$&$0.603_{(0.0068)}$\\
			\bottomrule
		\end{tabular}
		\caption{\texttt{Cal-Housing Regression} Real Dataset. Empirical Risk over $\mathbb{P}$}
		\label{tab:Cal-Housing-regression}
	\end{table}
	This experiment has both feature and label noise in the Noisy Data.
	
	
	
	In both the \texttt{Cal-Housing} and \texttt{Abalone} datasets there exists feature and label noise that exist with $5\%$ probability. In this the case, the probability is low, however since the noise is very large, even having a few points from $\mathbb{Q}$ in the final subquantile matrix can largely the bias the predictions away from the optimal parameters for $\mathbb{P}$. Therefore, we reduce $p$, the size of the subquantile to reduce the probability of obtaining corrupted samples within the subquantile. However, what we get in a decrease in variance, we do increase the bias error, albeit very slightly. 
	
	\newpage
	\section{Experimental Details}\label{app:experimental-details}
	
	\subsection{\texttt{Adaptive Linear Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Structured Linear Regression} Dataset.\\
	$\vx \sim \mathcal{N}(4,4)^{200}$\\
	$\vm \sim \mathcal{N}(4,4)^{200}$\\
	$b \sim \mathcal{N}(4,4)$\\
	$\vm' \sim \mathcal{N}(4,4)^{200}$\\
	$b' \sim \mathcal{N}(4,4)$\\
	$n_{\text{train}} = 1\text{e}4$\\
	$\mathbb{P}: y|\vx \sim \mathcal{N}(\vm^T\vx + b, 0.1)$\\
	$\mathbb{Q}: y|\vx \sim \mathcal{N}(\vm^{'\top}\vx + b', 0.1)$\\
	Please note $\vm$, $b$, $\vm'$, $b'$, are all sampled independently. The noise is added after normalization of the dataset to the standard normal $\mathcal{N}(0,1)$. 
	
	\subsection{\texttt{Oblivious Linear Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Noisy Linear Regression} Dataset.\\
	$\vx \sim \mathcal{N}(0,3)^{500}$\\
	$\vm \sim \mathcal{N}(4,4)^{500}$\\
	$b \sim \mathcal{N}(4,4)$\\
	$\vm' = \vzero$\\
	$b' \sim \mathcal{N}(5,5)$\\
	$n_{\text{train}} = 8\text{e}3$\\
	$n_{\text{test}} = 2\text{e}3$\\
	$\mathbb{P}: y|\vx \sim \mathcal{N}(\vm^T\vx + b, 0.01)$\\
	$\mathbb{Q}: y|\vx \sim \mathcal{N}(5,5)$\\
	Please note $\vm$, $b$, $\vm'$, $b'$, are all sampled independently. The noise is added after normalization of the dataset to the standard normal. 
	
	\subsection{\texttt{Quadratic Regression} Dataset}
	We will describe $\mathbb{P}$ and $\mathbb{Q}$ in the \texttt{Quadratic Regression} dataset.\\
	$x \sim \mathcal{N}(0,1)$\\
	$n_{\text{train}} = 1\text{e}4$\\
	$\mathbb{P}: y|x \sim \mathcal{N}(x^2 - x + 2, 0.01)$\\
	$\mathbb{Q}: y|x \sim \mathcal{N}(-x^2 + x + 4, 0.01)$
	
	\subsection{\texttt{Drug Discovery} Dataset}
	This dataset is downloaded from \cite{DiakonikolasKKLSS19}. We utilize the same noise procedure as in \cite{li2020tilted}.\\
	$\mathbb{P}$ is given from an 80/20 train test split from the dataset. \\
	$\mathbb{Q}$ is random noise sampled from $\mathcal{N}(5,5)$.\\
	The noise represents a noisy worker
		
	\subsection{Feature Noise}
	Take $5$\% of the training data and multiply features by 100 and responses by 10000. 
	
	\end{appendices}
\end{document}
