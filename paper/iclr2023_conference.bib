@incollection{Bengio+chapter2007,
	author = {Bengio, Yoshua and LeCun, Yann},
	booktitle = {Large Scale Kernel Machines},
	publisher = {MIT Press},
	title = {Scaling Learning Algorithms Towards {AI}},
	year = {2007}
}

@article{Hinton06,
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
	journal = {Neural Computation},
	pages = {1527--1554},
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	volume = {18},
	year = {2006}
}

@book{goodfellow2016deep,
	title={Deep learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	volume={1},
	year={2016},
	publisher={MIT Press}
}

@article{Lu_2020,
	doi = {10.4208/cicp.oa-2020-0165},
	url = {https://doi.org/10.4208%2Fcicp.oa-2020-0165},
	year = {2020},
	publisher = {Global Science Press},
	volume = {28},
	number = {5},
	pages = {1671--1706},
	author = {Lu Lu},
	title = {Dying {ReLU} and Initialization: Theory and Numerical Examples},
	journal = {Communications in Computational Physics}
}

@misc{Jin_2019,
	doi = {10.48550/ARXIV.1902.00618},
	url = {https://arxiv.org/abs/1902.00618},
	author = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I.},
	title = {What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?},
	publisher = {arXiv},
	year = {2019},
}


@article{LTS,
	abstract = {The linear least trimmed squares (LTS) estimator is a statistical technique for fitting a linear model to a set of points. Given a set of n points in ℝdand given an integer trimming parameter h≤n, LTS involves computing the (d−1)-dimensional hyperplane that minimizes the sum of the smallest h squared residuals. LTS is a robust estimator with a 50 {\%}-breakdown point, which means that the estimator is insensitive to corruption due to outliers, provided that the outliers constitute less than 50 {\%} of the set. LTS is closely related to the well known LMS estimator, in which the objective is to minimize the median squared residual, and LTA, in which the objective is to minimize the sum of the smallest 50 {\%} absolute residuals. LTS has the advantage of being statistically more efficient than LMS. Unfortunately, the computational complexity of LTS is less understood than LMS. In this paper we present new algorithms, both exact and approximate, for computing the LTS estimator. We also present hardness results for exact and approximate LTS. A number of our results apply to the LTA estimator as well.},
	author = {Mount, David M. and Netanyahu, Nathan S. and Piatko, Christine D. and Silverman, Ruth and Wu, Angela Y.},
	date = {2014/05/01},
	date-added = {2023-03-10 22:48:14 -0500},
	date-modified = {2023-03-10 22:48:14 -0500},
	doi = {10.1007/s00453-012-9721-8},
	id = {Mount2014},
	isbn = {1432-0541},
	journal = {Algorithmica},
	number = {1},
	pages = {148--183},
	title = {On the Least Trimmed Squares Estimator},
	url = {https://doi.org/10.1007/s00453-012-9721-8},
	volume = {69},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00453-012-9721-8}}

@article{LAD,
	ISSN = {00063444, 14643510},
	URL = {http://www.jstor.org/stable/20441446},
	abstract = {We develop a unified L₁-based analysis-of-variance-type method for testing linear hypotheses. Like the classical L ₂ -based analysis of variance, the method is coordinate-free in the sense that it is invariant under any linear transformation of the covariates or regression parameters. Moreover, it allows singular design matrices and heterogeneous error terms. A simple approximation using stochastic perturbation is proposed to obtain cut-off values for the resulting test statistics. Both test statistics and distributional approximations can be computed using standard linear programming. An asymptotic theory is derived for the method. Special cases of one- and multi-way analysis of variance and analysis of covariance models are worked out in detail. The main results of this paper can be extended to general quantile regression. Extensive simulations show that the method works well in practical settings. The method is also applied to a dataset from General Social Surveys.},
	author = {Kani Chen and Zhiliang Ying and Hong Zhang and Lincheng Zhao},
	journal = {Biometrika},
	number = {1},
	pages = {107--122},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Analysis of Least Absolute Deviation},
	urldate = {2023-03-11},
	volume = {95},
	year = {2008}
}

@article{li2020tilted,
	author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
	journal={arXiv preprint arXiv:2007.01162},
	title={Tilted empirical risk minimization},
	year={2020}
}

@misc{https://doi.org/10.48550/arxiv.2206.04777,
	doi = {10.48550/ARXIV.2206.04777},
	url = {https://arxiv.org/abs/2206.04777},
	author = {Awasthi, Pranjal and Das, Abhimanyu and Kong, Weihao and Sen, Rajat},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized Linear Models},
	publisher = {arXiv},
	year = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{DiakonikolasKKLSS19,
	author        = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
	title         = {Sever: A Robust Meta-Algorithm for Stochastic Optimization},
	booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
	series        = {ICML '19},
	year          = {2019},
	pages         = {1596--1606},
	publisher     = {JMLR, Inc.}
}

@unknown{Razaviyayn,
	author = {Razaviyayn, Meisam and Huang, Tianjian and Lu, Songtao and Nouiehed, Maher and Sanjabi, Maziar and Hong, Mingyi},
	year = {2020},
	month = {06},
	pages = {},
	title = {Non-convex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances}
}

@article{ROCKAFELLAR2014140,
	title = {Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk},
	journal = {European Journal of Operational Research},
	volume = {234},
	number = {1},
	pages = {140-154},
	year = {2014},
	issn = {0377-2217},
	doi = {https://doi.org/10.1016/j.ejor.2013.10.046},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221713008692},
	author = {R.T. Rockafellar and J.O. Royset and S.I. Miranda},
	keywords = {Generalized regression, Superquantiles, Conditional value-at-risk, Uncertainty quantification, Buffered failure probability, Stochastic programming},
	abstract = {The paper presents a generalized regression technique centered on a superquantile (also called conditional value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of regression functions, discuss the stability of regression functions under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management.}
}

@article{quantile-regression,
	Author = {Koenker, Roger and Hallock, Kevin F.},
	Title = {Quantile Regression},
	Journal = {Journal of Economic Perspectives},
	Volume = {15},
	Number = {4},
	Year = {2001},
	Month = {December},
	Pages = {143-156},
	DOI = {10.1257/jep.15.4.143},
	URL = {https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143}}

@article{thiel-sen,
	author = { Pranab   Kumar   Sen },
	title = {Estimates of the Regression Coefficient Based on Kendall's Tau},
	journal = {Journal of the American Statistical Association},
	volume = {63},
	number = {324},
	pages = {1379-1389},
	year  = {1968},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.1968.10480934},
	URL = { 
	https://www.tandfonline.com/doi/abs/10.1080/01621459.1968.10480934
	},
	eprint = { https://www.tandfonline.com/doi/pdf/10.1080/01621459.1968.10480934
	}
}

@Book{Huber2009,
	author={Huber, Peter J.
	and Ronchetti, Elvezio.},
	title={Robust statistics},
	series={Wiley series in probability and statistics},
	year={2009},
	edition={2nd ed.},
	publisher={Wiley},
	address={Hoboken, N.J.},
	url={http://catdir.loc.gov/catdir/toc/ecip0824/2008033283.html},
	language={English}
}

@inproceedings{McWilliams2014,
	author = {McWilliams, Brian and Krummenacher, Gabriel and Lucic, Mario and Buhmann, Joachim M.},
	title = {Fast and Robust Least Squares Estimation in Corrupted Linear Models},
	year = {2014},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	abstract = {Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates.The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence - for which we also develop a randomized approximation - motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
	pages = {415–423},
	numpages = {9},
	location = {Montreal, Canada},
	series = {NIPS'14}
}

@article{RRM,
	author = {Muhammad Osama and Dave Zachariah and Petre Stoica},
	title = {Robust Risk Minimization for Statistical Learning from Corrupted Data},
	journal = { IEEE Open Journal of Signal Processing},
	volume = {1},
	year = {2020},
	pages = {287--294}
}

@article{nydick2012wishart,
	title={The wishart and inverse wishart distributions},
	author={Nydick, Steven W},
	journal={Electronic Journal of Statistics},
	volume={6},
	number={1-19},
	year={2012}
}

@article{RANSAC1981,
	author = {Fischler, Martin A. and Bolles, Robert C.},
	title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
	year = {1981},
	issue_date = {June 1981},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {24},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/358669.358692},
	doi = {10.1145/358669.358692},
	abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
	journal = {Commun. ACM},
	month = {jun},
	pages = {381–395},
	numpages = {15},
	keywords = {scene analysis, location determination, model fitting, camera calibration, image matching, automated cartography}
}

@book{Matrix-Computations,
	author = {Golub, Gene H. and Van Loan, Charles F.},
	title = {Matrix Computations (3rd Ed.)},
	year = {1996},
	isbn = {0801854148},
	publisher = {Johns Hopkins University Press},
	address = {USA}
}

@article{yu_quantile_2003,
	author = {Yu, Keming and Lu, Zudi and Stander, Julian},
	title = {Quantile regression: applications and current research areas},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	volume = {52},
	number = {3},
	pages = {331-350},
	keywords = {Check function, Conditional distribution, Quantile, Regression fitting, Skew distribution},
	doi = {https://doi.org/10.1111/1467-9884.00363},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9884.00363},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9884.00363},
	abstract = {Summary. Quantile regression offers a more complete statistical model than mean regression and now has widespread applications. Consequently, we provide a review of this technique. We begin with an introduction to and motivation for quantile regression. We then discuss some typical application areas. Next we outline various approaches to estimation. We finish by briefly summarizing some recent research areas.},
	year = {2003}
}

@inproceedings{
	khetan2018learning,
	title={Learning From Noisy Singly-labeled Data},
	author={Ashish Khetan and Zachary C. Lipton and Anima Anandkumar},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=H1sUHgb0Z},
}

@inproceedings{jiang2018mentornet,
	title={MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
	author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
	booktitle={ICML},
	year={2018}
}

@INPROCEEDINGS {lin2017dense,
	author = {T. Lin and P. Goyal and R. Girshick and K. He and P. Dollar},
	booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
	title = {Focal Loss for Dense Object Detection},
	year = {2017},
	volume = {},
	issn = {2380-7504},
	pages = {2999-3007},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
	keywords = {detectors;training;entropy;object detection;proposals;robustness;computer vision},
	doi = {10.1109/ICCV.2017.324},
	url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.324},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {oct}
}

@ARTICLE{he2009imbalanced,
	author={He, Haibo and Garcia, Edwardo A.},
	journal={IEEE Transactions on Knowledge and Data Engineering}, 
	title={Learning from Imbalanced Data}, 
	year={2009},
	volume={21},
	number={9},
	pages={1263-1284},
	doi={10.1109/TKDE.2008.239}}

@article{Corbett2018fairness,
	author    = {Sam Corbett{-}Davies and
	Sharad Goel},
	title     = {The Measure and Mismeasure of Fairness: {A} Critical Review of Fair
	Machine Learning},
	journal   = {CoRR},
	volume    = {abs/1808.00023},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.00023},
	eprinttype = {arXiv},
	eprint    = {1808.00023},
	timestamp = {Sun, 02 Sep 2018 15:01:57 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-00023.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{Khan2021-mb,
	title     = "Applications of Robust Regression Techniques: An Econometric
	Approach",
	author    = "Khan, Dost Muhammad and Yaqoob, Anum and Zubair, Seema and Khan,
	Muhammad Azam and Ahmad, Zubair and Alamri, Osama Abdulaziz",
	abstract  = "Consistent estimation techniques need to be implemented to
	obtain robust empirical outcomes which help policymakers
	formulating public policies. Therefore, we implement the least
	squares (LS) and the high breakdown robust least trimmed squares
	(LTS) regression techniques, while using econometric regression
	model based on a growth equation for the two countries, namely,
	India and Pakistan. We used secondary annual time series data
	which covers a long period of 41 years. The adequacy of the time
	series econometric model was checked through cointegration
	analysis and found that there is no spurious regression.
	Classical and robust procedures were employed for the estimation
	of the parameters. The empirical results reveal that the overall
	fit of the model improves in case of LTS technique, while the
	significance of the predictors changes significantly in cases of
	both countries due to the removal of outliers from the data.
	Thus, empirical findings exhibit that the results, obtained
	through LTS, are better than LS techniques.",
	journal   = "Mathematical Problems in Engineering",
	publisher = "Hindawi",
	volume    =  2021,
	pages     = "6525079",
	month     =  may,
	year      =  2021
}
@article { swissRLRM,
	author = "Andreas Muhlbauer and Peter Spichtinger and Ulrike Lohmann",
	title = "Application and Comparison of Robust Linear Regression Methods for Trend Estimation",
	journal = "Journal of Applied Meteorology and Climatology",
	year = "2009",
	publisher = "American Meteorological Society",
	address = "Boston MA, USA",
	volume = "48",
	number = "9",
	doi = "https://doi.org/10.1175/2009JAMC1851.1",
	pages=      "1961 - 1970",
	url = "https://journals.ametsoc.org/view/journals/apme/48/9/2009jamc1851.1.xml"
}
@misc{yu2014robust,
	title={Robust Linear Regression: A Review and Comparison}, 
	author={Chun Yu and Weixin Yao and Xue Bai},
	year={2014},
	eprint={1404.6274},
	archivePrefix={arXiv},
	primaryClass={stat.ME}
}
@Article{Laguel2021,
	author={Laguel, Yassine
	and Pillutla, Krishna
	and Malick, J{\'e}r{\^o}me
	and Harchaoui, Zaid},
	title={Superquantiles at Work: Machine Learning Applications and Efficient Subgradient Computation},
	journal={Set-Valued and Variational Analysis},
	year={2021},
	month={Dec},
	day={01},
	volume={29},
	number={4},
	pages={967-996},
	abstract={R. Tyrell Rockafellar and his collaborators introduced, in a series of works, new regression modeling methods based on the notion of superquantile (or conditional value-at-risk). These methods have been influential in economics, finance, management science, and operations research in general. Recently, they have been subject of a renewed interest in machine learning, to address issues of distributional robustness and fair allocation. In this paper, we review some of these new applications of the superquantile, with references to recent developments. These applications involve nonsmooth superquantile-based objective functions that admit explicit subgradient calculations. To make these superquantile-based functions amenable to the gradient-based algorithms popular in machine learning, we show how to smooth them by infimal convolution and detail numerical procedures to compute the gradients of the smooth approximations. We put the approach into perspective by comparing it to other smoothing techniques and by illustrating it on toy examples.},
	issn={1877-0541},
	doi={10.1007/s11228-021-00609-w},
	url={https://doi.org/10.1007/s11228-021-00609-w}
}

@inproceedings{bhatia2017,
	author = {Bhatia, Kush and Jain, Prateek and Kamalaruban, Parameswaran and Kar, Purushottam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Consistent Robust Regression},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},
	volume = {30},
	year = {2017}
}

@InProceedings{pmlr-v89-mukhoty19a,
	title = 	 {Globally-convergent Iteratively Reweighted Least Squares for Robust Regression Problems},
	author =       {Mukhoty, Bhaskar and Gopakumar, Govind and Jain, Prateek and Kar, Purushottam},
	booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
	pages = 	 {313--322},
	year = 	 {2019},
	editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
	volume = 	 {89},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {16--18 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v89/mukhoty19a/mukhoty19a.pdf},
	url = 	 {https://proceedings.mlr.press/v89/mukhoty19a.html},
	abstract = 	 {We provide the first global model recovery results for the IRLS (iteratively reweighted least squares) heuristic for robust regression problems. IRLS is known to offer excellent performance, despite bad initializations and data corruption, for several parameter estimation problems. Existing analyses of IRLS frequently require careful initialization, thus offering only local convergence guarantees. We remedy this by proposing augmentations to the basic IRLS routine that not only offer guaranteed global recovery, but in practice also outperform state-of-the-art algorithms for robust regression. Our routines are more immune to hyperparameter misspecification in basic regression tasks, as well as applied tasks such as linear-armed bandit problems. Our theoretical analyses rely on a novel extension of the notions of strong convexity and smoothness to weighted strong convexity and smoothness, and establishing that sub-Gaussian designs offer bounded weighted condition numbers. These notions may be useful in analyzing other algorithms as well.}
}
@ARTICLE{Kelley1997,
	title = {Sparse spatial autoregressions},
	author = {Pace, Kelley and Barry, Ronald},
	year = {1997},
	journal = {Statistics `I\& Probability Letters},
	volume = {33},
	number = {3},
	pages = {291-297},
	abstract = {Given local spatial error dependence, one can construct sparse spatial weight matrices. As an illustration of the power of such sparse structures, we computed a simultaneous autoregression using 20 640 observations in under 19 min despite needing to compute a 20 640 by 20 640 determinant 10 times.},
	keywords = {Spatial autoregression SAR Sparse matrices},
	url = {https://EconPapers.repec.org/RePEc:eee:stapro:v:33:y:1997:i:3:p:291-297}
}

@misc{Dua2019,
	author = {Dua, Dheeru and Graff, Casey"},
	year = {2017},
	title = {{UCI} Machine Learning Repository},
	url = {http://archive.ics.uci.edu/ml},
	institution = {University of California, Irvine, School of Information and Computer Sciences} 
}

@article{Diakonikolas2019RecentAI,
	title={Recent Advances in Algorithmic High-Dimensional Robust Statistics},
	author={Ilias Diakonikolas and Daniel M. Kane},
	journal={ArXiv},
	year={2019},
	volume={abs/1911.05911}
}

@book{Wackerly2008,
	title={Mathematical Statistics with Applications,7th Edition},
	author={Wackerly, D.D and Mendenhall, W. and Scheaffer, R.L.},
	year={2008},
	publisher={Thompson Learning, Inc.},
	address={USA}
}

@Article{Laguel2021,
	author={Laguel, Yassine
	and Pillutla, Krishna
	and Malick, J{\'e}r{\^o}me
	and Harchaoui, Zaid},
	title={Superquantiles at Work: Machine Learning Applications and Efficient Subgradient Computation},
	journal={Set-Valued and Variational Analysis},
	year={2021},
	month={Dec},
	day={01},
	volume={29},
	number={4},
	pages={967-996},
	abstract={R. Tyrell Rockafellar and his collaborators introduced, in a series of works, new regression modeling methods based on the notion of superquantile (or conditional value-at-risk). These methods have been influential in economics, finance, management science, and operations research in general. Recently, they have been subject of a renewed interest in machine learning, to address issues of distributional robustness and fair allocation. In this paper, we review some of these new applications of the superquantile, with references to recent developments. These applications involve nonsmooth superquantile-based objective functions that admit explicit subgradient calculations. To make these superquantile-based functions amenable to the gradient-based algorithms popular in machine learning, we show how to smooth them by infimal convolution and detail numerical procedures to compute the gradients of the smooth approximations. We put the approach into perspective by comparing it to other smoothing techniques and by illustrating it on toy examples.},
	issn={1877-0541},
	doi={10.1007/s11228-021-00609-w},
	url={https://doi.org/10.1007/s11228-021-00609-w}
}


